Baverez's evidence of degeneration is not as convincing as he believes.
For example, he cites the draining away of industrial jobs as the most glaring evidence of France's decline.
But deindustrialization is common to all advanced countries.
Indeed, France does actually better than most of its rich country rivals and, unlike Germany, the UK, or the US, has retained the same global market share in the industrial sector that it had in the early 1970's. 
Moreover, according to a better measure of comptetitiveness, foreign investment, France does very well.
According to a recent report by the Economic Analysis Council, direct investment in France grew to 60 billion euros in 2001, almost twice that of Germany.
French cost competitiveness also improved (by 20%) in the last three decades, with higher productivity offsetting rapid wage growth. 
But the core of France's long-run economic problems is precisely the fear of losing industrial jobs.
France remains a strong industrial power in traditional areas such as automobile and aerospace industries, but it is falling behind in innovative markets, where smaller companies spread new technologies.
France, for instance, spends only $30 billion annually on high tech R&amp;D, compared with $51 billion in Germany, $98 billion in Japan, and $265 billion in the US. 
In short, Baverez's analysis points in the wrong direction.
France has undergone a set of critically important reforms over the past two decades: economic and financial liberalization, elimination of price and foreign exchange controls, the end of credit restrictions, European liberalization, reduced inflation and trade deficits, the advent of the euro and the forced globalization of the country's firms.
The French malaise has nothing to do with any of them. 
France's problem is that it fails to recognize the new world of which it already is a part.
French state capitalism is dead, and France is finding it hard to come to terms with its passing.
Indeed, France must be the only country in which a Prime Minister, Lionel Jospin, feels obliged to apologize the day after for saying on TV that "the state is not omnipotent." 
There are many reasons for this French schyzophrenia, but a critical one has to do with the political game of the past 20 years.
Until 1981, France had a clear division between right and left.
Since then, both the Mitterand and Chirac administrations blunted their ideological edge in order to carry the difficult process of adjustment towards Europe and globalization. 
A round of "shock therapy" à la Mrs. Thatcher two decades ago might have forged a new political consciousness.
But the consensual dribbling out of reforms allowed the French to live in one world and think in another.
This is a key reason for the political cataclysm that exploded during the last presidential campaign, when the far-right leader Jean-Marie Le Pen reached the second round of the presidential election, ahead of the incumbent Prime Minister, Lionel Jospin. 
The rise of the extreme left, which gathered 10% of the vote during that presidential campaign, also attests to this political failure.
France needs a political revolution more than an economic one if it is to come to terms with reality, part of which is that "French decline" is a fear of the French people's own making. 
The Paranoid Style in Economics
NEW DELHI – Why do high-profile economic tussles turn so quickly to ad hominem attacks?
Perhaps the most well-known recent example has been the Nobel laureate Paul Krugman’s campaign against the economists Carmen Reinhart and Kenneth Rogoff, in which he moved quickly from criticism of an error in one of their papers to charges about their commitment to academic transparency.
For those who know these two superb international macroeconomists, as I do, it is evident that these allegations should promptly be dismissed.
But there is the larger question of why the paranoid style has become so prominent.
Part of the answer is that economics is an inexact science, with exceptions to almost every pattern of behavior that economists take for granted.
For example, economists predict that higher prices for a good will reduce demand for it.
But students of economics will no doubt remember an early encounter with “Giffen goods,” which violate the usual pattern.
When tortillas become more expensive, a poor Mexican worker may eat more of them, because she now has to cut back on more expensive food like meat.
Such “violations” occur elsewhere as well.
Customers often value a good more when its price goes up.
One reason may be its signaling value.
An expensive handcrafted mechanical watch may tell time no more accurately than a cheap quartz model; but, because few people can afford one, buying it signals that the owner is rich.
Similarly, investors flock to stocks that have appreciated, because they have “momentum.”
The point is that economic behavior is complex and can vary among individuals, over time, between goods, and across cultures.
Physicists do not need to know the behavior of every molecule to predict how a gas will behave under pressure.
Economists cannot be so sanguine.
Under some conditions, individual behavioral aberrations cancel one another out, making crowds more predictable than individuals.
But, under other conditions, individuals influence one another in such a way that the crowd becomes a herd, led by a few.
The difficulties for economic policymakers do not stop there.
Economic institutions can have different effects, depending on their quality.
In the run-up to the 2008 financial crisis, macroeconomists tended to assume away the financial sector in their models of advanced economies.
With no significant financial crisis since the Great Depression, it was convenient to take for granted that the financial plumbing worked in the background.
Models, thus simplified, suggested policies that seemed to work –&#160;that is, until the plumbing backed up.
And the plumbing malfunctioned because herd behavior – shaped by policies in ways that we are only now coming to understand – overwhelmed it.
So, why not let evidence, rather than theory, guide policy?
Unfortunately, it is hard to get clear-cut evidence of causality.
If high national debt is associated with slow economic growth, is it because excessive debt impedes growth, or because slow growth causes countries to accumulate more debt?
Many an econometrician’s career has been built on finding a clever way to establish the direction of causality.
Unfortunately, many of these methods cannot be applied to the most important questions facing economic policymakers.
So the evidence does not really tell us whether a heavily indebted country should pay down its debt or borrow and invest more.
Moreover, what seem like obvious, commonsense policy solutions all too often have unintended consequences, because a policy’s targets are not passive objects, as in physics, but active agents who react in unpredictable ways.
For example, price controls, rather than lowering prices, often cause scarcity and the emergence of a black market in which controlled commodities cost significantly more.
All of this implies that economic policymakers require an enormous dose of humility, openness to various alternatives (including the possibility that they might be wrong), and a willingness to experiment.
This does not mean that our economic knowledge cannot guide us, only that what works in theory – or worked in the past or elsewhere – should be prescribed with an appropriate degree of self-doubt.
But, for economists who actively engage the public, it is hard to influence hearts and minds by qualifying one’s analysis and hedging one’s prescriptions.
Better to assert one’s knowledge unequivocally, especially if past academic honors certify one’s claims of expertise.
This is not an entirely bad approach if it results in sharper public debate.
The dark side of such certitude, however, is the way it influences how these economists engage contrary opinions.
How do you convince your passionate followers if other, equally credentialed, economists take the opposite view?
All too often, the path to easy influence is to impugn the other side’s motives and methods, rather than recognizing and challenging an opposing argument’s points.
Instead of fostering public dialogue and educating the public, the public is often left in the dark.
And it discourages younger, less credentialed economists from entering the public discourse.
In their monumental research on centuries of public and sovereign debt, the normally very careful Reinhart and Rogoff made an error in one of their working papers.
The error is in neither their prize-winning 2009 book nor in a subsequent widely read paper responding to the academic debate about their work.
Reinhart and Rogoff’s research broadly shows that GDP growth is slower at high levels of public debt.
While there is a legitimate debate about whether this implies that high debt causes slow growth, Krugman turned to questioning their motives.
He accused Reinhart and Rogoff of deliberately keeping their data out of the public domain.
Reinhart and Rogoff, shocked by this charge – tantamount to an accusation of academic dishonesty – released a careful rebuttal, including online evidence that they had not been reticent about sharing their data.
In fairness, given Krugman’s strong and public positions, he has been subject to immense personal criticism by many on the right.
Perhaps the paranoid style in public debate, focusing on motives rather than substance, is a useful defensive tactic against rabid critics.
Unfortunately, it spills over into countering more reasoned differences of opinion as well.
Perhaps respectful debate in economics is possible only in academia.
The public discourse is poorer for this.
The Defeated Remain Defiant
HARARE: Before Zimbabwe's Presidential election earlier this month, I believed that if the election were not handled properly, there would be serious fallout in the country and throughout Southern Africa.
Despite the shadows of war and terrorism, I called for a fair and free election.
Zimbabwe's people did not get one.
Instead, they got terrorism.
In the two years that led up to the Presidential vote, the people of Zimbabwe were subjected to severe intimidation, harassment and fear, all of which was carried out as part of a broader program of state-sponsored terrorism by Robert Mugabe's Zimbabwe African National Union-Patriotic Front (ZANU-PF).
ZANU terrorism was conducted by rogue elements among the veterans of Zimbabwe's war of independence two decades ago, as well as by government militiamen and youths, all of whom were actively aided by the police.
This intimidation and violence meant that my party, the Movement for Democratic Change (MDC) could not reach the electorate in large parts of the countryside.
In addition, the government of President Mugabe, my opponent, effectively disenfranchised hundreds of thousands of urban voters, particularly in the capital city of Harare and in the city of Chitungwiza.
Because of an inadequate number of polling stations, I believe that in Harare and Chitungwiza alone more than 360,000 people stood in a queue to vote but never gained the opportunity to cast their ballot.
My concerns about the legitimacy of the election results are further raised by interference by Mugabe's militia, whose members prevented the MDC from placing polling agents in 52% of rural polling stations.
By the end of the voting, our party had no observers at 6 out of 10 rural stations.
This effectively meant that Mugabe's supporters could have their way at these stations.
We are compiling a comprehensive list of the polling stations where ZANU supporters had solitary control.
In our investigations so far, a pattern has emerged even at this early stage: these polling stations recorded the highest number of voters for Mugabe.
In light of all the evidence, I cannot accept the Presidential election results in which Robert Mugabe, the sitting president, was declared a winner this month with 56% of the 3 million votes tallied.
The official results do not reflect the true will of the people of Zimbabwe and are illegitimate in the eyes of the people.
Zimbabwe's people have been cheated of their right to freely and democratically elect a President of their choice.
The contest over the election results is a political issue that must be resolved politically.
Food and fuel are in short supply, jobs are vanishing, inflation is running at more than 100%.
The people of Zimbabwe deserve a celebration for their courage and determination.
We may yet get one.
As I write, I am saddened because Mugabe's regime remains intent on defying the people's will.
Whatever may happen, I as the people's loyal servant am with them all the way.
The government may want to arrest me.
Indeed, I was arrested for treason even before the election.
At worst, they may even wish to kill me.
But the government will never destroy the spirit of the people to reclaim their rights and power.
The power to achieve democratic change is in our hands.
We may have moments of fear in the days ahead, but we must never let despair overwhelm us.
The tide of political change is irreversible.
But we must be prepared to pay a high price for our freedom.
President Mugabe and his cronies are afraid of the people and we have heard they may do anything to kill the messenger.
If they do, I only ask that the people of Zimbabwe remain strong and carry on the work that we began together.
Among ordinary Zimbabweans walk heroes - heroes who waited hours and hours to vote, heroes who refused to be turned away.
These are the heroes of the new Zimbabwe whose voices must be heard around the world.
Together, we traveled a very harsh road together to achieve democratic change in my country.
Rarely in the history of humankind have a people faced such brutality while retaining such gracious exuberance.
I realize that the people of Zimbabwe are impatient.
I understand why.
But they must wait peacefully for the political process to unfold.
They will not let this election stand but neither will they succumb to Mugabe's provocative traps and resort to violence.
The people of Zimbabwe want constitutional change, legal change, legitimate change, and they are going to obtain it in spite of all the obstacles.
In this, we know we are not alone.
We know that throughout Africa, the will of the electorate has been thwarted.
It appears in the majority of African states, whenever you have elections, you have irregularities, fraud, cheating.
There's always a crisis of elections in Africa.
There is, sad to say, a lack of sincerity on the part of governments across this continent when it comes time to give people the right to choose.
But those who wish otherwise can take hope.
Zimbabwe's struggle is not over.
We have time to tally the lessons for Africa, and the world, of our experience.
My people are in their hour of greatest need.
I pledge not to abandon them.
The Delayed Promise of Health-Care IT
WASHINGTON, DC – Because information technology (IT) has so quickly transformed people’s daily lives, we tend to forget how much things have changed from the not-so-distant past.
Today, millions of people around the world regularly shop online; download entire movies, books, and other media onto wireless devices; bank at ATMs wherever they choose; and self-book entire trips and check themselves in at airports electronically.
But there is one sector of our lives where adoption of information technology has lagged conspicuously: health care.
Some parts of the world are doing better than others in this respect.
Researchers from the Commonwealth Fund recently reported that some high-income countries, including the United Kingdom, Australia, and New Zealand, have made great strides in encouraging the use of electronic medical records (EMR) among primary-care physicians.
Indeed, in those countries, the practice is now nearly universal.
Yet some other high-income countries, such as the United States and Canada, are not keeping up.
EMR usage in America, the home of Apple and Google, stands at only 69%.
The situation in the US is particularly glaring, given that health care accounts for a bigger share of GDP than manufacturing, retail, finance, or insurance.&#160;Moreover, most health IT systems in use in America today are designed primarily to facilitate efficient billing, rather than efficient care, putting the business interests of hospitals and clinics ahead of the needs of doctors and patients.
That is why many Americans can easily go online and check the health of their bank account, but cannot check the results of their most recent lab work.
Another difference between IT in US health care and in other industries is the former’s lack of interoperability.
In other words, a hospital’s IT system often cannot “talk” to others.
Even hospitals that are part of the same system sometimes struggle to share patient information.
As a result, today’s health IT systems act more like a “frequent flyer card” designed to enforce customer loyalty to a particular hospital, rather than an “ATM card” that enables you and your doctor to access your health information whenever and wherever needed.
Ordinarily, lack of interoperability is an irritating inconvenience.
In a medical emergency, it can impose life-threatening delays in care.
A third way that health IT in America differs from consumer IT is usability.
The design of most consumer Web sites is so obvious that one needs no instructions to use them.
Within minutes, a seven year old can teach herself to play a complex game on an iPad.
But a newly hired neurosurgeon with 27 years of education may have to read a thick user manual, attend tedious classes, and accept periodic tutoring from a “change champion” to master the various steps required to use his hospital’s IT system.
Not surprisingly, despite its theoretical benefits, health IT has few fans among health-care providers.
In fact, many complain that it slows them down.
Does this mean that health IT is a waste of time and money?
Absolutely not.
In 2005, colleagues of ours at the RAND Corporation projected that America could save more than $80 billion a year if health care could replicate the IT-driven productivity gains observed in other industries.
The fact that the US has not gotten there yet is not a problem of vision, but of less-than-ideal implementation.
Other industries, including banking and retail trade, struggled with IT until they got it right.
The gap between what IT promised and what it delivered in the early days was so stark that experts called it the “IT productivity paradox.”
Once these industries figured out how to make their IT systems more efficient, interoperable, and user-friendly, and then realigned their processes to leverage technology’s capabilities, productivity soared.
In America, as in much of the world, health care is late to the IT game, and is experiencing these growing pains only now.&#160;But health-care providers can shorten the process of transformation by learning from other industries.
The US government is trying to help.
In 2009, Congress passed the Health Information Technology for Economic and Clinical Health (HITECH) Act.
HITECH has undeniably accelerated IT adoption among health-care providers.
Yet the problems of usability and interoperability persist.
Globally, the health IT industry should not wait to be forced by government regulators into doing a better job.
Developers can boost the pace of adoption by creating more standardized systems that are easier to use, truly interoperable, and afford patients greater access to and control over their personal health data.
Health-care providers and hospital systems can dramatically boost the impact of health IT by reengineering traditional practices to take full advantage of its capabilities.
If America is any indicator, the sky is the limit when it comes to potential gains from health IT.
According to the Institute of Medicine, the US currently wastes more than $750 billion per year on unnecessary or inefficient health-care services, excessive administrative costs, high prices, medical fraud, and missed opportunities for prevention.
Properly applied, health IT can improve health care in all of these dimensions.
The payoff will be worth it.
Indeed, as with the adoption of IT elsewhere, we may soon wonder how health care could have been delivered any other way.
The Delusion of Debt Forgiveness
ROME: Governments from around the world will gather in Brussels under UN auspices on May 14th to discuss the plight of poor countries.
As they talk, the leaders of rich countries may strike a self-satisfied pose, recalling as they do the old proverb, which says that forgiveness is divine.
For little more than a year ago they agreed to forgive much of the debt of the world’s poorest countries.
Of course, forgiving the debts of the poor is undoubtedly noble.
Sadly, not everything that is noble is smart and effective.
Many of the debts of poor countries that have been forgiven were already considered as, de facto, lost and thus unrecoverable.
Interest and principal payments made by many poor countries were, as a result of this, much smaller than what was actually owed.
Ethiopia, for example, was paying a sum equal to 10% of its exports; payments of 70% would have been needed to cover the nominal value of its debt.
The Cologne Agreement on poor countries with the highest level of debt – the Highly Indebted Poor Countries (HIPC) initiative – that was agreed to last year declared that debts will be forgiven up to the point at which interest payments become “sustainable”: in other words, the agreement substitutes unrecoverable debt for a debt load that is nominally lower but “real.”
According to calculations by Britain’s Treasury, with the HIPC agreement Mozambique pays about the same proportion in debt service as it was paying before the accord was signed.
More than a shell game is at work here, however.
For the countries accorded debt forgiveness under the HIPC must renounce other forms of aid.
This restriction led Ghana to ask to opt out of the HIPC altogether.
Uncertainty about the numbers involved in the exercise in debt forgiveness is large, but reliable World Bank calculations show that, for the 9 countries in the most advanced stages of HIPC, interest expenditures should go down to 10% of the value of exports.
This amounts to a saving of about 1.4% of GDP – the actual number will perhaps be larger by a few decimal points here and there when the effects of bilateral debt forgiveness are added.
A respectable outcome, but not an enormous amount.
Independent of these numbers, however, the debt forgiveness debate perpetuates the legend that the West is responsible for the oppressive debt burdens of poor countries, and, more generally, for the scarcity of assistance to help them fight poverty.
The reality is the opposite.
Few developing countries are seriously engaged in fighting poverty.
In many of them, public expenditure favor the middle and upper middle classes; above all, it favors city dwellers.
Economies in most poor countries, indeed, are rigged against their own poor in a variety of ways:
• by artificially inflated public employment: with high salaries being paid to state workers relative to the average wage, state jobs often provide lifetime employment for children of the middle classes;
• because pensions are available only for a few executives in the formal sector and for state employees, with nothing provided to the rural poor;
• because free university education is a form of redistribution from poor to rich because the poor are taxed to pay for university, but rarely get to go;
• because expenditures for primary education often end up being captured by teachers’ unions in order to guarantee privileges for their members;
• through health expenditures that are often concentrated on hospitals in relatively prosperous and politically important urban areas;
• because free water and electricity are mostly a subsidy for the urban middle class and for rich farmers.
Overwhelming empirical evidence (provided in large part by the World Bank, a place keen on the idea of aid) proves that the development of the poorest countries is blocked, primarily, by government inefficiency and the corruption.
For development to take off, countries require moderately stable and efficient governments pursuing appropriate economic policies.
South Korea is the best example of what can be achieved in this way.
In 1960, it had a per capita income lower than Somalia; today it is a member of the OECD.
Empirical research amply demonstrates that development assistance is largely ineffective because it is given indiscriminately to governments who use it to enrich a tiny elite.
A recent document by a World Bank expert, William Easterly, shows that the same conclusion also holds for debt forgiveness initiatives.
Of course, the West is not blameless.
It erred in giving too much credit, not too little, to poor countries, and by handing out credit according to political needs rather than by rigorous criteria related to fighting poverty.
The ex-imperial powers favored their ex-colonies, regardless of whether they deserved assistance or how they used it.
During the Cold War, alliance loyalty rather than economic motivations, determined the flow of aid.
A corrupt but allied government was invariably preferred to an honest, non-allied one.
In addition, many of the bilateral credits now being canceled were in reality commercial credit guarantees which benefitted businesses in creditor countries more than poor countries.
In Italy, things were even worse: development assistance was largely disbursed in order to favor foreign friends of Italian politicians.
So debt forgiveness, even if morally commendable, risks sending two misleading messages: it tells the ruling classes of poor debtor countries that corruption pays; and it reinforces the Western public’s belief that the problems of poor countries are caused by Western policies.
If these two points are not confronted, debt forgiveness will end up laying the foundations for a new cycle of indebtedness, corruption, and abiding poverty.
The Demise of Parliaments
August is the traditional month when parliaments recess for the summer.
It offers a moment to examine why they are so enfeebled.
Across Europe, it is not only British Prime Minister Tony Blair who is accused of "presidentialism" nowadays.
German Chancellor Gerhard Schröder faces the same charge in the current German election campaign.
Prime Minister Silvio Berlusconi of Italy, indeed, does not even like being called Prime Minister.
Because he is technically President of the Council (of ministers), he insists on using the title of President.
France, of course, is a presidential democracy.
To many, "presidentialism" sounds like the American constitution; but those who suspect in today's trend another facet of the Americanization of Europe are wrong.
American presidents have powers which are severely restricted by Congress via the US constitution; they are but one in a triad of separate powers.
Europe's "presidential" prime ministers, on the contrary, are what a British Lord Chancellor once called, "elective dictators."
This means, above all, that they have ceased to take parliaments seriously.
Some Prime Ministers rarely attend their parliaments.
When they graciously consent to appear, they are treated with deference.
Parliaments are in some cases handmaidens of the executive rather than the source of sovereignty.
Prime Minister Berlusconi has persuaded the Italian Parliament to pass legislation which primarily benefits him and his business interests.
The mother of parliaments itself, the British parliament at Westminster, frequently cuts short debates by a procedural device known as the "guillotine," so that important legislation, like the current Asylum Bill, remains largely undebated in the House of Commons.
(The absurd consequence is that the unelected House of Lords becomes the real legislature of the country.)
All governments have acquired the habit of using secondary legislation by orders and regulations without parliamentary control.
The European Union sets a particularly bad example in all this.
Its legislature is the Council of Ministers which sometimes allows the elected Parliament to engage in a little "co-decision-making."
The Council of course meets in camera , that is without public scrutiny.
How could this happen?
What is the reason for the apparent demise of the central institution of democracy, parliament?
Why is it no longer the place in which the representatives of the people debate great issues and hold the executive to account?
Is democracy itself at risk?
One can think of a number of reasons for the evisceration of parliaments.
One is globalization.
Decisions have emigrated from the spaces for which parliaments are elected.
They take place in remote and often unknown places.
These can be the boardrooms of companies, or private international meetings of leaders, or just a course of events which escapes all controls.
The collapse of the "new economy" is an example, but in a curious way also the probable attack on Iraq will also be arrived at in such a way.
Even if national parliaments tried to come to grips with such developments, they would fail.
Another reason is the separation of the political game from the lives and concerns of most people.
Parties have become machines for allocating power rather than for gathering and representing the interests of citizens.
The party game has lost its representative quality.
As a result, leaders have developed a tendency to turn directly to the people without allowing much debate.
This can be done by opinion polls and the use of focus groups; it can also be achieved by referenda and plebiscites.
In both cases, parliaments become dispensable.
The danger of such developments is that they strengthen an already strong trend towards a new authoritarianism.
The political class becomes a kind of nomenklatura of leaders who prefer popularity to debate.
They find it awkward to give reasons for their policies and regard parliaments as no more than a reservoir for those with whom they are prepared to share power.
As reasoned debate falls by the wayside, citizens lose interest in politics.
They pursue their affairs and let those in power govern.
Along with parties and parliaments, elections have lost their charm.
Declining voter turnout tells the story.
The demise of parliaments is above all a decline in democratic debate and scrutiny.
It is not a development which defenders of the constitution of liberty can accept.
The time has come for a revolt of parliaments against both the arrogance of those in power and voter apathy.
In a sense, we need more rather than less presidentialism in Europe, at least if this is understood in the proper American way, in which legislative and executive powers are separately legitimated and equally strong.
The Demise of the Development Round
Hopes for a development round in world trade – opening up opportunities for developing countries to grow and reduce poverty – now seem dashed.
Though crocodile tears may be shed all around, the extent of disappointment needs to be calibrated: Pascal Lamy, the head of the World Trade Organization, had long worked to diminish expectations, so much so that it was clear that whatever emerged would bring, at most, limited benefits to poor countries.
The failure hardly comes as a surprise: The United States and the European Union had long ago reneged on the promises they made in 2001 at Doha to rectify the imbalances of the last round of trade negotiations – a round so unfair that the world’s poorest countries were actually made worse off.
Once again, America’s lack of commitment to multilateralism, its obstinacy, and its willingness to put political expediency above principles – and even its own national interests – has triumphed.
With elections looming in November, President George W. Bush could not “sacrifice” the 25,000 wealthy cotton farmers or the 10,000 prosperous rice farmers and their campaign contributions.
Seldom have so many had to give up so much to protect the interests of so few.
The talks bogged down over agriculture, where subsidies and trade restrictions remain so much higher than in manufacturing.
With 70% or so of people in developing countries depending directly or indirectly on agriculture, they are the losers under the current regime.
But the focus on agriculture diverted attention from a far broader agenda that could have been pursued in ways that would have benefited both North and the South.
For example, so-called “escalating tariffs,” which tax processed goods at a far higher rate than unprocessed products mean that manufacturing tariffs discourage developing countries from undertaking the higher value-added activities that create jobs and boost incomes.
Perhaps the most outrageous example is America’s $0.54 per gallon import tariff on ethanol, whereas there is no tariff on oil, and only a $0.5 per gallon tax on gasoline.
This contrasts with the $0.51 per gallon subsidy that US companies (a huge portion of which goes to a single firm) receive on ethanol.
Thus, foreign producers can’t compete unless their costs are $1.05 per gallon lower than those of American producers.
The huge subsidies have meant that the US has become the largest producer of ethanol in the world.
Yet, despite this huge advantage, some foreign firms can still make it in the American market.
Brazilian sugar-based ethanol costs far less to produce than American corn-based ethanol.
Brazil’s firms are far more efficient than America’s subsidized industry, which puts more energy into getting subsidies out of Congress than in improving efficiency.
Some studies suggest that it requires more energy to produce America’s ethanol than is contained in it.
If America eliminated these unfair trade barriers, it would buy more energy from Brazil and less from the Middle East.
Evidently, the Bush administration would rather help Middle East oil producers, whose interests so often seem at variance with those of the US, than Brazil.
Of course, the Administration never puts it that way; with an energy policy forged by the oil companies, Archer Daniels Midland and other ethanol producers are just playing along in a corrupt system of campaign-contributions-for-subsidies.
In the trade talks, America said that it would cut subsidies only if others reciprocated by opening their markets.
But, as one developing country minister put it, “Our farmers can compete with America’s farmers; we just can’t compete with America’s Treasury.” Developing countries cannot, and should not, open up their markets fully to America’s agricultural goods unless US subsidies are fully eliminated.
To compete on a level playing field would force these countries to subsidize their farmers, diverting scarce funds that are needed for education, health, and infrastructure.
In other areas of trade, the principle of countervailing duties has been recognized: when a country imposes a subsidy, others can impose a tax to offset the unfair advantage given to that country’s producers.
If markets are opened up, countries should be given the right to countervail American and European subsidies.
This would be a major step forward in trying to create a fair trade regime that promotes development.
At the onset of the development round, most developing countries worried not only that the EU and the US would renege on their promises (which they have in large part), but also that the resulting agreement would once again make them worse off.
As a result, much of the developing world is relieved that at least this risk has been avoided.
Still, there was a second risk: that the world would think that the agreement itself had accomplished the objectives of a development round set forth at Doha, with trade negotiators then turning once again to making the next round as unfair as previous rounds.
This concern, too, now seems to have been allayed.
There remains one further concern: America has rushed to sign a series of bilateral trade agreements that are even more one-sided and unfair to developing countries, which may prompt Europe and others to do likewise.
This divide-and-conquer strategy undermines the multilateral trade system, which is based on the principle of non-discrimination.
Countries that sign these agreements get preferential treatment over all others.
But developing countries have little to gain and much to lose by signing these agreements, which almost never deliver the promised benefits.
Indeed, the entire world is the loser if the multilateral trade system is weakened.
The rest of the world must not embrace America’s unilateral approach: the multilateral trade system is too precious to allow it to be destroyed by a US President who has repeatedly shown his contempt for global democracy and multilateralism.
The Democracy Paradox
PARIS – Elections stolen in Iran, disputed in Afghanistan, and caricatured in Gabon: recent ballots in these and many other countries do not so much mark the global advance of democracy as demonstrate the absence of the rule of law.
Of course, elections that lead to illiberal outcomes, and even to despotism, are not a new phenomenon.
Hitler, after all, came to power in Germany in 1933 through a free, fair, and competitive election.
Moreover, problematic elections constitute a specific challenge for the West, which is simultaneously the bearer of a universal democratic message and the culprit of an imperialist past that undermines that message’s persuasiveness and utility.
In a noted essay in 2004, for example, the Indian-born author Fareed Zakaria described the danger of what he called “illiberal democracy.”
For Zakaria, America had to support a moderate leader like General Pervez Musharraf in Pakistan, despite the fact that he had not come to power through an election.
By contrast, Zakaria argued, Venezuela’s populist president, Hugo Chávez, who was legitimately elected, should be opposed.
In our globalized world, the potential divorce between elections and democracy has assumed a new dimension.
With instantaneous communication and access to information, the less legitimate a regime, the greater will be the temptation for it to manipulate, if not fabricate, the results of elections.
The “trendy” way is to manufacture a significant but not too massive victory. Today’s despots view near-unanimous Soviet-style electoral “victories” as vulgar and old fashioned.
But another new aspect of this phenomenon are opposition forces that are willing to attempt to negate such machinations by the party in power.
Confronted with this dual process of illegitimacy, the West often finds itself condemned to sit between two chairs, and to face criticism whatever the outcome.
Those in power, as in Iran, accuse Western governments of supporting the opposition, and those in opposition accuse the West of supporting the government, as has happened to France in the case of Gabon.
So what lessons should we draw from the inevitably messy nature of electoral processes in countries where there is either no middle class or only a rudimentary one, and where a democratic culture is at best in its infancy?
The time has come for the West to reassess its policies in a fundamental way.
It cannot switch from “activism” at one moment to abstention the next.
A refusal to act, after all, is also a political choice.
Of course, the temptations of isolationism are great, and will increase in the months and years ahead.
But the West has neither the moral right nor a strategic possibility of withdrawing into an “ivory tower,” something which in most cases does not exist.
It is impossible to say to Afghanistan, for example, “You have deeply disappointed us, so, from now on, you must clean up your own mess.” In Afghanistan, Gabon, Iran, Pakistan, and elsewhere, fundamental Western interests – though very different depending on the case – are at stake.
In Afghanistan, the danger is that a terrorist haven could be reconstituted.
The risk in Iran is an ever more hostile regime armed with nuclear weapons.
In Gabon, the priority for France is to transcend neo-colonialism without losing its important links to the oil-rich African nation.
But, in pursuit of these difficult objectives, the West must get both its ambitions and its methods right.
Democracy is a legitimate objective, but it is a long-term one.
In the medium term, the absence of the rule of law constitutes the most serious problem for the countries in question.
French television, for example, recently aired a terrifying report on Haiti, where a local judge, without bothering to hide his actions, was protecting a narcotics dealer from the country’s own French-trained anti-drug force.
Corruption eats away at a society from within, destroying citizens’ trust in a future based on a shared sense of common good.
It is the West’s acceptance of corruption – either open or tacit – that makes it an accomplice to too many nefarious regimes, and makes its espousal of democratic principles appear either hypocritical or contradictory.
On the other hand, setting the rule-of-law standard too high will also misfire.
Singapore-style incorruptible one-party state bent on modernizing society is probably a far too ambitious goal for most non-democratic regimes.
The distance that separates the West from countries that rely on sham elections is not only geographic, religious, or cultural; it is chronological. Their “time” is not, has never been, or is no longer the same as that of the West.
How can they be understood without being judged, or helped without humiliating paternalism or, still worse, without an unacceptable “collateral damage,” as in Afghanistan?
The West’s status in tomorrow’s world will largely depend upon how it answers this question.
It cannot afford to ignore the issue any longer.
The Democrat’s Disease
BUCHAREST: “Paris,” Protestant King Henri of Navarre quipped before ascending the throne of Catholic France, “is worth a mass.”
Is EU membership worth the life of Romania’s democratic parties, perhaps its infant democracy?
That question will be on the lips of many of Romania’s reformers and democrats when going to the polls next week for general and presidential elections.
Baroness Emma Nicholson, the European Parliament’s special envoy to Romania, has warned that unless Romania’s next government speeds the pace of reform - particularly privatization of big state companies - hopes of joining the EU will collapse.
But our ruling center/right government is massively unpopular precisely because it tried to meet EU demands.
This unpopularity opened the way for a return to the presidency of Ion Iliescu, the Ceausescu-era apparatchik who ruled Romania during the first, wasted years of our postcommunist transition.
Iliescu’s first term were years that the locusts ate: reform lagged behind most Central European countries, corruption flourished, rabid nationalists barked, the government called in loyalist thugs from the mines to beat up people who protested.
Four years ago, on a high tide of hope, a center/right coalition and a new “democratic” president - Emil Constantinescu - took power, surprisingly defeating Iliescu and his party.
Constantinescu’s government promised to close the gap between Romania and other postcommunist countries bidding to join the EU.
It promised to clean up corruption and our banks.
Believing Jacques Chirac would deliver on his pledges of support, it promised NATO membership.
It promised that, by 2000, people would be better off.
None of this happened and Romanians feel betrayed.
They are now prepared to heed Iliescu’s siren song that the future can be found only by going back to the past.
Are they right?
Was Constantinescu’s presidency a total failure?
No.
After years of decline, the economy is growing; after numerous financial scandals, the banking system is safer.
Privatization advanced, nationalists were constrained, relations with our big Hungarian minority improved, the country stood with the West during the Kosovo war.
None of this, however, made any difference in people’s daily lives - indeed, the Kosovo war and sanctions on Serbia knocked 1-2% off Romania’s GDP last year.
Economic dissatisfaction, however, does not account for all Romania’s frustrations.
We also resent the Constantinescu coalition’s internal feuding and incompetence, what the Italians call “malgoverno.”
From day one, the coalition was riven by feuds.
Ministers acted like adversaries rather than partners.
Instead of blaming Iliescu for the shambles in which he left the country, coalition members blamed each other.
Such squabbling, it seems, is a postcommunist malady.
Poland, Hungary, even Russia: in all three, infighting among reformers allowed the political heirs to the communists to reclaim power.
Poland and Hungary’s postcommunists regained power after reform-minded governments freed prices, began privatization, and encouraged business.
Russia’s not-so-ex-communists never assumed power again, because reformers were succeeded by Viktor Chernomyrdin’s inert government.
This sequence is important because the postcommunist revival - in Poland and Hungary at least - came as growth reappeared.
Poland’s and Hungary’s postcommunists could see that reform worked and so, back in power, continued on the free-ish market road, taking credit for the economic growth nurtured by their rivals.
Even Chernomyrdin’s government did not nullify Russia’s early reforms.
But Iliescu’s first government dithered on reform; growth was a dream when Constantinescu became president.
Indeed, Romania’s reformers confronted conditions that the Polish, Hungarian, and Russian reform governments had faced when communism first collapsed.
Although much more could have been accomplished during Contstantinescu’s presidency, hard reforms were enacted but, as in Poland and Hungary half-a-decade ago, growth arrived too late to save its architects from electoral defeat.
So Iliescu and his PDSR party are cruising toward a crushing victory.
What remains unknown is whether they have learned the lesson other postcommunist parties learned - ie, to keep reform going once growth appears.
Or will they turn back the clock?
Adrian Nastase, the PDSR’s executive president and its candidate for prime minister, seems to lean toward reform.
One sign that he actually will institute reform will be demonstrated by whether or not the PDSR governs alone or in coalition with a centrist/liberal party or with the Hungarian minority party.
For to convince the world that they would continue reforming, Poland and Hungary’s postcommunists formed coalition governments despite winning overwhelming parliamentary majorities.
Nastase knows that if he surrenders to the popular mood, raising salaries dramatically and stalling privatization, the economy will be crippled and Romania’s chances of joining the EU disappear.
Whether Nastase can pursue a sensible program depends mightily on party unity, and here the PDSR is as divided as Constantinescu’s coalition.
Unreformed apparatchiks surround Iliescu; they will enjoy the prestige of the presidency in any power struggle over policy.
The current campaign shows that xenophobia and nostalgia for communism retain an alarmingly potent appeal.
The only hopeful aspect in Iliescu’s looming victory is that his opponents succeeded - just in time - in promoting growth and this may encourage the postcommunist to keep looking ahead.
If the PDSR turns backward, the malady of our democratic reformers may prove to be fatal, both for Romania’s economy and for our hopes of ever joining Europe.
The Democratic Hopes of Iraqis
The escalating violence in Iraq gives a bleak impression of that country’s prospects.
Sectarian conflict seems to be increasing on a daily basis, with militias massacring hundreds of Sunnis and Shiites solely on the basis of their religious identities.
Yet it would be a mistake to think that this bloodlust represents widespread sentiment among Iraqis.
While neither American nor Iraqi security officials have yet found a way to tame the militias, the Iraqi public is increasingly drawn toward a vision of a democratic, non-sectarian government for the country.
In 2004 and 2006, I was involved in conducting two nationwide public opinion surveys in Iraq.
Contrasting the findings of these surveys demonstrates that over the two years when sectarian violence has increased, Iraqis increasingly view their fate in a national, rather than communal, context.
Over this period, the number of Iraqis who said that it was “very important” for Iraq to have a democracy increased from 59% to 65%.
These same Iraqis saw a link between an effective democracy and the separation of religion and politics, as under a western system.
Overall, those who responded that they “strongly agree” that “Iraq would be a better place if religion and politics were separated” increased from 27% in 2004 to 41% in 2006.
Particularly significant were increases from 24% to 63% during this period among Sunnis and from 41% to 65% among Kurds.
Opinion on this question within the majority Shiite community remained stable, with 23% strongly agreeing in both 2004 and 2006.
Similarly, the survey found declining support for an Islamic state.
Between 2004 and 2006, the number of Iraqis who said it was “very good to have an Islamic government where religious leaders have absolute power” fell from 30% to 22%.
Declines occurred in all three leading ethnic communities: from 39% to 35% among Shiites, from 20% to 6% among Sunnis, and from 11% to 5% among Kurds.
There was some increase in the number of Shiites who thought that there should be a strong religious element in national laws, the majority still opposed this.
Nationalist sentiment is also increasing.
Asked whether they considered themselves “Iraqis, above all” or “Muslims, above all,” the 2006 survey found that 28% of Iraqis identified themselves as “Iraqis, above all,” up from 23% in 2004.
In the capital, Baghdad, the center of so much sectarian violence, the numbers were even more impressive, with the share of the population who saw themselves as “Iraqis, above all” doubling, from 30% to 60%.
By contrast, similar surveys in other Arab capitals find a decided tilt toward a Muslim identity. In Amman, Jordan, the most recent figure is 12% who put their national identity ahead of their Muslim identity.
The figure is 11% in Cairo, Egypt, and 17% in Riyadh, Saudi Arabia.
Curiously, in Tehran, Iran, the choice is markedly in favor of Iranian, rather than Muslim, identity.
Among residents of the Iranian capital, the share of “nationalists” soared from 38% in 2000 to 59% in 2005.
At the same time, Iraqi attitudes show a strong reaction to daily violence.
Between 2004 and 2006, the proportion who strongly agreed that life in Iraq is “unpredictable and dangerous” increased from 46% to 59%.
The change was felt in all communities, rising from 41% to 48% among Shiites, from 77% to 84% among Sunnis, and from 16% to 50% among Kurds.
At the same time, the surveys found little support among any of these three major groups for sectarian conflict.
The violence has had a major effect on Iraqi attitudes toward foreigners.
By 2006, distrust of Americans, British, and French had reached 90%, and attitudes toward Iraq’s neighbors were also tense.
More than half of Iraqis surveyed said that they would not welcome Turks, Jordanians, Iranians, or Kuwaitis as neighbors.
These feelings, it appears, are directly related to the violence gripping Iraq, and the role of foreigners in the country’s perilous security situation.
So it appears that Iraqis are showing greater attachment to their national identity and are supportive of a non-sectarian approach to government.
These are the basic traits of a modern political order.
Among Sunnis, the decline in support for an Islamic state is most dramatic, and may have significant ramifications for the ability of religious extremists to recruit among them.
Although Iraqis remain angry about the violence in their country, this anger has not undermined their sense of national identity.
At the same time, they appear to be holding onto important democratic values.
The key question, of course, remains whether these values can be translated into a peaceful reality.
Why India Slowed
NEW DELHI – For a country as poor as India, growth should be what Americans call a “no-brainer.”
It is largely a matter of providing public goods: decent governance, security of life and property, and basic infrastructure like roads, bridges, ports, and power plants, as well as access to education and basic health care.
Unlike many equally poor countries, India already has a strong entrepreneurial class, a reasonably large and well-educated middle class, and a number of world-class corporations that can be enlisted in the effort to provide these public goods.
Why, then, has India’s GDP growth slowed so much, from nearly 10% year on year in 2010-11 to 5% today?
Was annual growth of almost 8% in the decade from 2002 to 2012 an aberration?
I believe that it was not, and that two important factors have come into play in the last two years.
First, India probably was not fully prepared for its rapid growth in the years before the global financial crisis.
For example, new factories and mines require land.
But land is often held by small farmers or inhabited by tribal groups, who have neither clear and clean title nor the information and capability to deal on equal terms with a developer or corporate acquirer.
Not surprisingly, farmers and tribal groups often felt exploited as savvy buyers purchased their land for a pittance and resold it for a fortune.
And the compensation that poor farmers did receive did not go very far; having sold their primary means of earning income, they then faced a steep rise in the local cost of living, owing to development.
In short, strong growth tests economic institutions’ capacity to cope, and India’s were found lacking.
Its land titling was fragmented, the laws governing land acquisition were archaic, and the process of rezoning land for industrial use was non-transparent.
India is a vibrant democracy, and, as the economic system failed the poor and the weak, the political system tried to compensate.
Unlike in some other developing economies, where the rights of farmers or tribals have never stood in the way of development, in India politicians and NGOs took up their cause.
Land acquisition became progressively more difficult.
A similar story played out elsewhere.
For example, the government’s inability to allocate resources such as mining rights or wireless spectrum in a transparent way led the courts to intervene and demand change.
And, as the bureaucracy got hauled before the courts, it saw limited upside from taking decisions, despite the significant downside from not acting.
As the bureaucracy retreated from helping businesses navigate India’s plethora of rules, the required permissions and clearances were no longer granted.
In sum, because India’s existing economic institutions could not cope with strong growth, its political checks and balances started kicking in to prevent further damage, and growth slowed.
The second reason for India’s slowdown stems from the global financial crisis.
Many emerging markets that were growing strongly before the crisis responded by injecting substantial amounts of monetary and fiscal stimulus.
For a while, as industrial countries recovered in 2010, this seemed like the right medicine.
Emerging markets around the world enjoyed a spectacular recovery.
But, as industrial countries, beset by fiscal, sovereign-debt, and banking problems, slowed once again, the fix for emerging markets turned out to be only temporary.
To offset the collapse in demand from industrial countries, they had stimulated domestic demand.
But domestic demand did not call for the same goods, and the goods that were locally demanded were already in short supply before the crisis.
The net result was overheating – asset-price booms and inflation across the emerging world.
In India, matters were aggravated by the investment slowdown that began as political opposition to unbridled development emerged.
The resulting supply constraints exacerbated inflation.
So, even as growth slowed, the central bank raised interest rates in order to rebalance demand and the available supply, causing the economy to slow further.
To revive growth in the short run, India must improve supply, which means shifting from consumption to investment.
And it must do so by creating new, transparent institutions and processes, which would limit adverse political reaction.
Over the medium term, it must take an axe to the thicket of unwieldy regulations that make businesses so dependent on an agile and cooperative bureaucracy.
One example of a new institution is the Cabinet Committee on Investment, which has been created to facilitate the completion of large projects.
By bringing together the key ministers, the committee has coordinated and accelerated decision-making, and has already approved tens of billions of dollars in spending in its first few meetings.
In addition to more investment, India needs less consumption and higher savings.
The government has taken a first step by tightening its own budget and spending less, especially on distortionary subsidies.
Households also need stronger incentives to increase financial savings.
New fixed-income instruments, such as inflation-indexed bonds, will help.
So will lower inflation, which raises real returns on bank deposits.
Lower government spending, together with tight monetary policy, are contributing to greater price stability.
If all goes well, India’s economy should recover and return to its recent 8% average in the next couple of years.
Enormous new projects are in the works to sustain this growth.
For example, the planned Delhi-Mumbai Industrial Corridor, a project with Japanese collaboration entailing more than $90 billion in investment, will link Delhi to Mumbai’s ports, covering an overall length of 1,483 kilometers (921 miles) and passing through six states.
The project includes nine large industrial zones, high-speed freight lines, three ports, six airports, a six-lane expressway, and a 4,000-megawatt power plant.
We have already seen a significant boost to economic activity from India’s construction of its highway system.
The boost to jobs and growth from the Delhi-Mumbai Industrial Corridor, linking the country’s political and financial capitals, could be significantly greater.
To the extent that democratic responses to institutional incapacity will contribute to stronger and more sustainable growth, India’s economic clouds have a silver lining.
But if India’s politicians engage in point-scoring rather than institution-building, the current slowdown may portend stormy weather ahead.
The Democratization of Aid
The outpouring of aid in response to the Indian Ocean tsunami brought hope to a troubled world.
In the face of an immense tragedy, working class families around the world opened their wallets to the disaster’s victims.
Former US President Bill Clinton called this response a “democratization of development assistance,” in which individuals lend their help not only through their governments but also through their own efforts.
But, while more than 200,000 people perished in the tsunami disaster, an equivalent number of children die each month of malaria in Africa, a disaster I call a “silent tsunami.”
Africa’s silent tsunami of malaria, however, is actually largely avoidable and controllable.
Malaria can be prevented to a significant extent, and can be treated with nearly 100% success, through available low-cost technologies.
Yet malaria’s African victims, as well as those in other parts of the world, are typically too poor to have access to these life-saving technologies.
A global effort, similar to the response to the Asian tsunami, could change this disastrous situation, saving more than one million lives per year.
Herein lies the main message of the new report of the UN Millennium Project, which was delivered in mid-January to UN Secretary General Kofi Annan.
The Project, which I direct on behalf of the Secretary General, represents an effort by more than 250 scientists and development experts to identify practical means to achieve the Millennium Development Goals to cut extreme poverty, disease, and hunger by the year 2015.
Our new report, entitled Investing in Development: A Practical Plan to Achieve the Millennium Development Goals (available for download at www.unmillenniumproject.org ), shows that these goals can be achieved.
The key to meeting the Millennium Development Goals in poor countries is an increase in investment in people (health, education, nutrition, and family planning), the environment (water and sanitation, soils, forests, and biodiversity), and infrastructure (roads, power, and ports).
Poor countries cannot afford these investments on their own, so rich countries must help.
If more financial aid is combined with good governance in poor countries, then the Millennium Development Goals can be achieved on time.
In short, our new Report is a call to action.
Rich countries and poor countries need to join forces to cut poverty, disease, and hunger.
The reason that the Millennium Development Goals are feasible is that powerful existing technologies give us the tools to make rapid advances in the quality of life and economic productivity of the world’s poor.
Illness and deaths from malaria can be reduced sharply by using insecticide-treated bed nets to stop the mosquitoes that transmit malaria, and by effective medicines when the illness strikes.
The total cost of battling malaria in Africa would be around $2 to $3 billion per year.
With around one billion people living in high-income countries, it would thus cost just $2 to $3 per person per year in the developed world to fund an effort that could save more than one million children annually.
When child mortality is reduced, poor families choose to have fewer children, because they are more confident that their children will survive to adulthood.
Thus, paradoxically, saving children’s lives is part of the solution to rapid population growth in poor countries.
Malaria is an important example where specific investments can solve the problems of disease, hunger, and extreme poverty.
Our report makes dozens of such practical recommendations.
Investments in soil nutrients and water harvesting could help African farmers double or triple their food yields.
Anti-retroviral medicines can help save millions from death due to AIDS.
Rural roads, truck transport, and electricity could bring new economic opportunities to remote villages in Latin America, Africa, and Asia.
School meal programs using locally produced food could boost attendance by poor children, especially girls, and improve their ability to learn, while also providing an expanded market for local farmers.
These investments are an incredible bargain.
Rich countries have long promised to increase their aid levels to 0.7% of national income (from around only 0.25% today).
The promise of 0.7% means that the rich world would give developing countries a mere 70 cents out of each $100 of national income.
In recent weeks, many European countries have pledged to honor the 0.7% commitment, and five European countries (Denmark, Luxembourg, the Netherlands, Norway, and Sweden) already do so.
It’s up to the US and Japan to follow through on their promises as well.
Moreover, with the “democratization” of aid now underway, we can look forward to increased private efforts alongside official development assistance.
Of course, not all developing countries are sufficiently well governed to use an increase in aid in an honest, effective way.
The world should therefore start this bold effort by focusing on the poor countries that are relatively well governed and that are prepared to carry out needed investments in an efficient and fair manner.
Ghana, Senegal, Tanzania, Kenya, and Ethiopia are on that list.
It is urgent that we get started in these and similarly well governed poor countries this year.
The Democrats’ Line in the Sand
BERKELEY – Ever since the 1928 work of Frank Ramsey, economists have accepted the utilitarian argument that a good economy is one in which returns on investment are not too great a multiple – less than three – of the rate of per capita economic growth.
An economy in which profits from investment are high relative to the growth rate is an economy that is under-saving and under-investing.
This idea has also given rise to a very strong presumption that if an economy as a whole is under-saving and under-investing, the government ought to help to correct this problem by running surpluses, not make it worse by running deficits that drain the pool of private savings available to fund investment.
This is why most economists are deficit hawks.
Of course, governments need to run deficits in depressions in order to stimulate demand and stem rising unemployment.
Moreover, a lot of emergency government spending on current items is really best seen as national savings and investment.
Franklin Delano Roosevelt could have made no better investment for the future of America and the world than to wage total war against Adolf Hitler.
Likewise, Presidents George H.W. Bush and Bill Clinton ought to have recognized in the 1990’s that something like a Marshall Plan for Eastern Europe to help with the transition from communism would have been an excellent investment for the world’s future.
But the rule is that governments should run surpluses and not deficits, so various American presidents’ economic advisers have been advocates of aiming for budget surpluses except in times of slack demand and threatening depression.
This was certainly true of Eisenhower’s, Nixon’s, and Ford’s economic advisors, and of George H.W. Bush’s and Bill Clinton’s economic advisers.
It was true of Reagan’s economic advisers as well.
Some of Reagan’s advisers sincerely did not believe that the tax cuts of the early 1980’s would generate the large deficits that they did (Beryl Sprinkel and Lawrence Kudlow come to mind).
Others, like Martin Feldstein and Murray Weidenbaum, understood the consequences of the Reagan tax cuts and were bitter bureaucratic opponents, even if they did not speak out publicly.
In fact, since WWII, only George W. Bush’s economic advisers have broken with this consensus.
A few have done so because they are making careers as party-line Republicans, so their priority is to tell Republican politicians what they want to hear (Josh Bolton and Mitch Daniels come to mind here).
As for the rest, their reasons for supporting the Bush administration’s savings-draining policies remain mysterious.
It is not as though they were angling for lifetime White House cafeteria privileges, or that having said “yes” to George W. Bush will open any doors for them in the future.
But their failings do pose a dilemma for Democratic deficit-hawk economists trying to determine what good economic policies would be should Barack Obama become president.
Those of us who served in the Clinton administration and worked hard to put America’s finances in order and turn deficits into surpluses are keenly aware that, after eight years of the George W. Bush administration, things look worse than when we started back in 1993.
All of our work was undone by our successors in their quest to win the class war by making America’s income distribution more unequal.
A chain is only as strong as its weakest link, and it seems pointless to work to strengthen the Democratic links of the chain of fiscal advice when the Republican links are not just weak but absent.
Political advisers to future Democratic administrations may argue that the only way to tie the Republicans’ hands and keep them from launching another wealth-polarizing offensive is to widen the deficit enough that even they are scared of it.
They might be right.
The surplus-creating fiscal policies established by Robert Rubin and company in the Clinton administration would have been very good for America had the Clinton administration been followed by a normal successor.
But what is the right fiscal policy for a future Democratic administration to follow when there is no guarantee that any Republican successors will ever be “normal” again?
That’s a hard question, and I don’t know the answer.
There is, however, one fiscal principle that must be respected.
Fiscal deficits so large that they put the debt-to-GDP ratio on an explosive upward trend do not merely act as a drag on long-term economic growth; they also create the possibility that at any moment the economy might face an immediate macroeconomic and financial disaster.
A more hawkish fiscal stance may no longer be possible in future Democratic administrations, and might not be good policy if it were, given the likely complexion of successor administrations.
Stabilizing the debt-to-GDP ratio is thus the line in the sand that must not be crossed.
America’s Blinders
SINGAPORE – The time has come to think the unthinkable: the era of American dominance in international affairs may well be coming to an end.
As that moment approaches, the main question will be how well the United States is prepared for it.
Asia’s rise over the last few decades is more than a story of rapid economic growth.
It is the story of a region undergoing a renaissance in which people’s minds are re-opened and their outlook refreshed.
Asia’s movement toward resuming its former central role in the global economy has so much momentum that it is virtually unstoppable.
While the transformation may not always be seamless, there is no longer room to doubt that an Asian century is on the horizon, and that the world’s chemistry will change fundamentally.
Global leaders – whether policymakers or intellectuals – bear a responsibility to prepare their societies for impending global shifts.
But too many American leaders are shirking this responsibility.
Last year, at the World Economic Forum in Davos, two US senators, one member of the US House of Representatives, and a deputy national security adviser participated in a forum on the future of American power (I was the chair).
When asked what future they anticipated for American power, they predictably declared that the US would remain the world’s most powerful country.
When asked whether America was prepared to become the world’s second-largest economy, they were reticent.
Their reaction was understandable: even entertaining the possibility of the US becoming “number two” amounts to career suicide for an American politician.
Elected officials everywhere must adjust, to varying degrees, to fulfill the expectations of those who put them in office.
Intellectuals, on the other hand, have a special obligation to think the unthinkable and speak the unspeakable.
They are supposed to consider all possibilities, even disagreeable ones, and prepare the population for prospective developments.
Honest discussion of unpopular ideas is a key feature of an open society.
But, in the US, many intellectuals are not fulfilling this obligation.
Richard Haass, the president of the Council on Foreign Relations, suggested recently that the US “could already be in the second decade of another American century.”
Likewise, Clyde Prestowitz, the president of the Economic Strategy Institute, has said that “this century may well wind up being another American century.”
To be sure, such predictions may well prove accurate; if they do, the rest of the world will benefit.
A strong and dynamic US economy, reinvigorated by cheap shale gas and accelerating innovation, would rejuvenate the global economy as a whole.
But Americans are more than ready for this outcome; no preparation is needed.
If the world’s center of gravity shifts to Asia, however, Americans will be woefully unprepared.
Many Americans remain shockingly unaware of how much the rest of the world, especially Asia, has progressed.
Americans need to be told a simple, mathematical truth.
With 3% of the world’s population, the US can no longer dominate the rest of the world, because Asians, with 60% of the world’s population, are no longer underperforming.
But the belief that America is the only virtuous country, the sole beacon of light in a dark and unstable world, continues to shape many Americans’ worldview.
American intellectuals’ failure to challenge these ideas – and to help the US population shed complacent attitudes based on ignorance – perpetuates a culture of coddling the public.
But, while Americans tend to receive only good news, Asia’s rise is not really bad news.
The US should recognize that Asian countries are seeking not to dominate the West, but to emulate it.
They seek to build strong and dynamic middle classes and to achieve the kind of peace, stability, and prosperity that the West has long enjoyed.
This deep social and intellectual transformation underway in Asia promises to catapult it from economic power to global leadership.
China, which remains a closed society in many ways, has an open mind, whereas the US is an open society with a closed mind.
With Asia’s middle class set to skyrocket from roughly 500 million people today to 1.75 billion by 2020, the US will not be able to avoid the global economy’s new realities for much longer.
The world is poised to undergo one of the most dramatic power shifts in human history.
In order to be prepared for the transformation, Americans must abandon ingrained ideas and old assumptions, and liberate unthinkable thoughts.
That is the challenge facing American public intellectuals today.
The Denial of Caste
Deconstruction as an intellectual device has played havoc in universities across the West.
Now it is casting its spell over developing countries, making a powerful play to rewrite India's past, as Deepak Lal suggests.
India's ancient and medieval history is notoriously malleable.
The country's Hindu nationalists unleashed the latest furor over the nature of India's past.
They reject the widely accepted view, based on early sacred texts, that ancient believers did not ban the slaughter of cattle, and that such a ban probably became part of the Hindu moral code only around the fifth and sixth centuries AD, when the later Puranas were written.
The trouble with reaching definitive conclusions about this or any other contested aspect of India's distant past is that, unlike in neighboring China, there is little in the way of an objective historical record to rely upon.
Some archaeological evidence exists.
But the main sources for ancient India are orally transmitted literary accounts dating to the Rigveda (around 1500-1300 BC) and the subjective records of foreign travelers.
The social history of ancient India, as one scholar admits, "appears to be a string of conjectures and speculations."
So it is hardly surprising that India's ancient past can be manipulated to fit alternative ideological preconceptions.
Nowhere is this more evident than in discussions about the origins and nature of India's caste system.
In the early 1980's, the predominant view on the Left was that the ancient Hindu caste system was a variant of European feudalism.
The late Ashok Rudra, himself a man of the Left but an empiricist, provided the most cogent critique of this view.
He pointed out a crucial difference between the conceptions of mutual social ties in the Hindu and European systems.
Where individuals were the central actors in the characteristic rituals of dominance and homage in European feudalism, in Hindu society the relationships were always defined in terms of caste groups.
Rudra assumed that caste existed in ancient India because, despite the unreliable historical record, the caste system is so manifest even today.
But recent historians on the Left in India now question this assumption.
A school of revisionist historians and anthropologists argue that caste is an invention of the colonial British Raj.
They claim that the early British scholars and administrators who documented Indian customs and translated the early sacred texts were bamboozled by the Brahmins--the first Indians to learn English and thus the only available intermediaries--into believing that the Hindu social order was caste-based.
This imagined order became reality when British census takers forced Indians to categorize themselves by caste.
Before this, these historians argue, Hindus were allegedly no less individualistic than Europeans.
Even hitherto sensible anthropologists and social historians seem to have succumbed to this travesty of scholarship.
Much of the supposed evidence that this so-called "Subaltern" school of historians provides takes the form of contemporary anthropological studies of regions like southern India or parts of medieval central India.
But, as Susan Bayley, a "Subaltern" convert, puts it, "the initial premise is that even in parts of the Hindu heartland of Gangetic upper India, the institutions and beliefs which are often described as the elements of traditional caste were only taking shape as recently as the early 18 th century."
I can personally disprove this premise.
While visiting Kerala in the late 1960's, I--a Hindu from the Gangetic heartland--went to the Sri Padmanabhaswamy temple in Trivandrum, where the usual "pandas" accosted me.
One asked where I came from and asked about my jati and gotra .
When I answered, he rattled off the names of at least six generations of my ancestors, while asking me to fill in the details about my cousins, the family into which I had married, and the names of our children.
He did not ask for money; he was only interested in updating his records.
Unless he had imagined my ancestors, he had just traced my caste-based past to well before the 17 th century.
Aside from such examples, an economic rationale can also be provided for the origins of the Indian caste system as for European feudalism.
The great Eurasian civilizations were all dependent on agriculture and needed to create institutional means of tying labor, which was then scarce, to the land, which was abundant.
Serfdom, indenture, slavery, and the caste system all served this end.
How did ancient Indian princes tie labor to the land if neither feudalism nor caste existed?
Even if we are forced, faute de mieux , to speculate about ancient Indian social life, we must remain true to the common facts of economic and military life among European and Asian civilizations, which imposed similar institutional responses.
These caveats, however, will no doubt leave the revisionists unmoved, for they are part of the "deconstructionist" movement that has deeply influenced the humanities in American and British universities.
Deconstructionists do not believe that any settled "facts" exist.
If they are right, there is no difference between the intellectual discipline of history, founded by the Greeks 2400 years ago to record the past truthfully, and the myths that every culture tells to affirm its self-worth.
Without history, we will be exposed to the full force of the tales of our imagined pasts, which only mirror our current hates and loves.
The Depopulation of Europe
By common consent, the recent EU summit at Stockholm was a dull affair.
EU leaders made some progress on creating a single financial market throughout the Union, and on establishing a standard European patent.
Other proposals for economic liberalisation, were deadlocked.
The idea of a single European air traffic control area was blocked because of a dispute between Britain and Spain over Gibraltar; proposals for liberalisation of gas and electricity supply were resisted by France, which promotes the public service advantages of public sector utilities.
There was good news, however, in that the Stockholm summit witnessed the first signs that the EU may at last be creeping, crab-wise, toward an idea hitherto taboo: that the European Union may require an immigration policy.
Until recently, member governments held two ideas on immigration: they all took the view that ordinary immigration must be restricted or prevented; they all agreed that asylum-seekers be vilified as “bogus” economic migrants and deterred through policies of harassment, intimidation and - in some cases - detention.
In addition, some member governments, like those of Germany and Austria, have demonstrated serious anxiety that EU enlargement will incite a flood of job-seekers from Central and Eastern Europe, and made clear that they will demand a long delay in granting free movement of labour to any new member state.
Eighteen months ago, at the Tampere summit, EU leaders said that they recognised “the need for more efficient management of migration flows.”
What they really meant, however, was stronger measures to tackle illegal immigration.
Stockholm came one step closer to admitting Europe’s real population problem, when it spoke of “the demographic challenge of an ageing population of which people of working age constitute an ever-smaller part.”
Indeed, the summit communique said that “an in-depth discussion of immigration, migration and asylum” would take place at the EU summit at the end of this year.
But EU leaders have yet to admit what everybody knows: that Europe’s population is not just ageing, it is starting to decline, and is likely to decline steeply in coming decades.
At the Stockholm summit the leaders pretended that the solution to Europe’s ageing problem will be found in better job mobility, more flexibility, and further economic reform.
The plain fact, however, is that Europe’s population decline can only be stemmed by substantial inward immigration.
Last month (February 2001) the United Nations published its latest forecasts for world population over the next 50 years.
The broad picture is that the population of the world’s less developed regions is expected to rise from 4.9 billion today to 8.2 billion in 2050, whereas the population of the developed regions will, in aggregate, remain roughly stable at 1.2 billion.
Within the developed world, however, Europe’s population is expected to decline markedly.
In some European countries, like France and Ireland, the population will actually rise. But in others there will be steep falls.
The population of Germany will decline from 82m to just under 71m, that of Italy from 57.5m to 43m, and that of Spain from nearly 40m to just over 31m. “all in all, the population of the EU 15 will decline from 376m to 339.3m, a fall of 37m or 10%.
Even sharper declines are expected in the candidate countries of Central and Eastern Europe, from a total of 105m to 85m, a fall of 20m, or about 20%.
The implications are far-reaching.
First, on a medium-term time-scale, the EU needs substantial immigrants.
Second, the fear of a tidal wave of immigrants from Eastern Europe is largely a populist fantasy.
Third, Eastern Europe will itself need substantial immigration.
Two strategic footnotes need to be added to this scenario.
First, Turkey’s population is likely to rise hugely in the next 50 years, from 66m to nearly 100m.
Now, Turkey has in principle been accepted as a potential candidate for EU membership.
But if UN projections are correct, Turkey’s population in 2050 would be far larger than any other member state, including Germany, and would represent nearly 20% of the population of the enlarged EU.
The increase in Turkey’s population might almost balance out the decline in the population of the present EU 15; but it is unlikely that today’s existing member states will regard this satisfactory.
Moreover, the strategic balance to Europe’s east will also be transformed, because the populations of Russia and the Ukraine are expected to decline even more steeply than those of the EU.
Russia’s population, currently 145.5m, is expected to fall to 104.5m; Ukraine’s population is projected to fall from nearly 50m to 30m, a startling drop of 40%.
This regional pattern, of population decline in Europe, Russia and Ukraine, needs to be set against the larger picture of a 3 billion increase globally, mainly in the developing world.
It is difficult to avoid the implication that Europe’s need for immigrants is likely to combine with massive, and probably irresistible, migratory pressure from developing countries to the developed world.
What is now required is that EU leaders stop pretending that Europe’s demographic ageing can be solved by more internal labor mobility.
The EU will, instead, need a large, managed immigration policy.
This is, undoubtedly, a difficult demand to make of politicians, particularly given the legacy of xenophobic politics in so many member states.
But it must be done, and soon.
The Devaluation Delusion Revisited
"The dollar is our currency and your problem."
So America's Treasury Secretary quipped before President Nixon pulled the plug on the Bretton Woods system three decades ago.
What John Connolly's bluntness reflected was America's ability--and willingness--to export its economic problems by driving down the dollar's value while scapegoating countries opposed to that strategy.
President George W. Bush seems hell-bent on repeating Nixon's misbegotten policy.
Like the Nixon/Connolly team, the Bush administration is responding to America's massive budget and trade deficits by letting the dollar fall--and hard --while it also tries to distract attention from its responsibility by pointing an accusing finger at China as the cause for both US joblessness and deflationary pressures.
This strategy, however, is likely to prove as ineffective now as it was for Nixon, who succeeded only in ushering in an era of stagnation.
Bush's policy is doomed to fail because, as in the 1970's, America's economic problems are homegrown.
They are not imported and cannot be fixed merely by changing the dollar's value.
The world's reliance on the US as the one source of growth in global demand buttresses America's currency power play, but at the cost of aggravating vast global imbalances.
Since 2000, America's excess productive capacity has outstripped the Euro area and Japan combined, its economy growing far more slowly than its 3.5% to 4% annual potential, with US unemployment rising.
This pitiful performance incites angry cries that American jobs are disappearing abroad, and that low-cost exports may result in deflation.
Sadly, America is not alone in viewing devaluation as a panacea for domestic problems.
Too timid to undertake serious reforms at home, Japanese authorities fight to keep the yen's value as low as possible against the dollar and rival Asian currencies.
Similarly, the European Union created the euro to give its members increased currency stability.
When the euro was down, wounded pride forced EU governments to try talking it up.
Now that the euro is up, those same governments are trying to talk it down.
The euro's dizzying trade-weighted ascent over the past year is, indeed, exacerbating near-term pressures within the EU.
But the Union's finance ministers would be better off pushing the internal reforms Europe needs, rather than following the Bush example and pressing the European Central Bank to force a strong currency down to earth.
Yet America's currency strategy is the most reckless--another mighty error in an economic policy so wayward that it is hard to know where to begin listing the mistakes.
Perhaps the best place to start is with the speedy deterioration in America's federal budget in the past two years.
From a surplus of 1.4% of GDP in 2000, the Bush administration has delivered a deficit of 4.6% of GDP this year.
US budget deficits may have spared America and the world economy even worse performances over the past two years, but fixing what ails the US is now far more difficult.
Tightening fiscal policy would go a long way towards boosting confidence in the US.
It might also help push the rest of the world into moving away from its reliance on America as the sole engine of growth.
Growth elsewhere might help America export more.
But this won't provide the quick fix that the Bush administration yearns for, because a declining dollar need not bring about an equivalent rise in the price of US imports.
This is partly because the final price of an imported good reflects numerous costs such as distribution and marketing, which are unaffected by the exchange rate.
Indeed, many countries that export to the US--especially Japan and China--price their goods in dollars.
Because these countries are keen to maintain their share in the world's biggest market, they often absorb the effect of a drop in the dollar by cutting their profits rather than raise prices.
As a recent study by J.P. Morgan's Chief Economist John Lipsky shows, the link between exchange rates and trade is weakening.
What makes US strategy reckless is that the Bush administration is attacking China at the very moment that America's dependence on Chinese purchases of US government bonds is growing.
Without these purchases, the US might face a rise in domestic interest rates that could threaten both its economic recovery and the global economy.
Moreover, if China decided to sell its dollar holdings, the bond market would discount US government securities, raising US long-term interest rates and canceling much (perhaps all) of whatever stimulus has been provided by the dollar's depreciation.
Luckily, China's government knows that America's advice to let the renminbi float right now is dangerous--both to China and for the world economy.
Policies intended to benefit one country's economy at the expense of another, such as competitive devaluations, were widespread during the Great Depression of the 1930's.
For most of the postwar era, governments appeared to have learned the lesson of that time.
So it is bizarre that, today, leaders in the world's strongest economies are dabbling in such dangerous nonsense again.
To fix its domestic problems, America must tighten its budget, not drive down the dollar's value.
But with presidential elections looming, no US government would cut spending or raise taxes.
So forget about the economically illiterate Bush administration daring to do either.
The silver lining here is that this failure may revive the realization that no country--not even mighty America--can devalue its way out of trouble.
The Emerging Economies’ Eurozone Crisis
WASHINGTON, DC – Most of today’s economic institutions, from money to banking, evolved over many years – the unintended consequences of decisions by millions of individuals.
By contrast, the eurozone stands out for being a deliberate creation.
It is arguably the world’s second-largest, deliberately-planned economic structure, after Communism.
The eurozone is a remarkable experiment, a genuine vanguard of global progress.
As 2012 comes to a close, it is in trouble, and every effort must be made to nurture and strengthen it.
By the second half of 2011, it was evident that emerging economies, which had weathered the financial crisis that began in 2008 moderately well, were taking on water as the eurozone crisis deepened.
Growth slowed sharply in Brazil, India, China, and other countries.
Central banks acted as lenders of last resort, thereby averting a major crisis.
In December 2011 and February 2012, the European Central Bank announced the long-term refinancing operation (LTRO), whereby European banks were lent around €1 trillion ($1.3 trillion) in two tranches.
Then, in July, came ECB President Mario Draghi’s famous assurance to do “whatever it takes” to save the euro.
The United States Federal Reserve injected liquidity, as did other advanced countries’ central banks.
There was a collective sigh of relief, financial markets stabilized, and industrial-production rebounded.
The question on everyone’s mind now is whether this post-storm calm will last, allowing the global economy to pick up.
Nowhere does this question loom larger than in developing and emerging economies, which are outside the main theater of the crisis, but are more precariously positioned than the advanced countries.
Many had only recently begun to grow rapidly, and, with vast reservoirs of poor people, economic growth has a moral urgency that it does not have in rich countries.
So, will the global economy stage a sustained recovery?
Examining the past as carefully as I can, and aware of the risks of augury, my answer has to be no.
Until 2015, the outlook is gloomy for Europe and, by extension, for the emerging and developing economies.
The injection of liquidity that occurred over the last year was the right policy.
But it only bought time; it did not solve the problem.
And time is running out.
Unfortunately, most people have an instinctive propensity to look away from approaching problems until they are very close.
America’s “fiscal cliff,” for example, was long in coming, but we are scrambling to avoid it only now.
So we should take early stock of the fact that there is another problem coming our way, which may be called (to give it the resonance of a coming storm) Edward –&#160;the “European Debt Wall and Repayment Deadline.”
The LTRO money that banks received on such easy terms, we must recall, took the form of three-year loans, which implies a wall of debt repayment in December 2014 and February 2015.
If Europe succeeds in making major fiscal and banking reforms and gets its economy in order, Edward will lose steam.
If not, the crisis will persist, and Europe will be rocked as Edward makes landfall by the end of 2014.
Where does that leave developing countries?
The US and Europe are the world’s two largest economic powerhouses.
Their slowdown will have an adverse impact on all emerging economies.
Moreover, the US and Europe have already used large doses of fiscal stimulus, which shares an uncanny similarity to antibiotics.
Administered over a short duration, it can be a powerful antidote; but, used repeatedly over too long a period, the side effects can outstrip the benefits.
Consider the case of India.
Since 2009, India has been expanding its deficit as a deliberate measure to counter its economic slowdown.
Because fiscal expansion followed several years of restraint, it was very effective in spurring demand and output growth.
But now the scope for further expansion is limited.
Unlike advanced countries, most emerging economies are exhibiting inflationary pressures, which could be exacerbated by another round of stimulus spending.
So the short-run situation remains precarious.
Nevertheless, for emerging economies, the medium- to long-term prospects are bright.
Countries that are saving a substantial amount, investing in human capital, and providing a modicum of good governance should resume their previous rapid growth.
India, for example, is saving and investing well over 30% of its GDP, devoting a significant share of these resources to infrastructure.
Its entrepreneurial capacity is expanding.
In several recent years, India’s outward direct investment in Britain has exceeded inward direct investment from Britain.
So, once the crisis is over, annual growth should rebound to its earlier rate of more than 8%.
Investors seem to be taking this view to heart.
They have been tightfisted when it comes to short-term equity investments.
But, when it comes to long-term direct investment, they committed a record-high $43.8 billion to India in 2011-2012.
Beyond the current crisis, the prospect appears to be similar in other major emerging economies, including Brazil, China, and Indonesia.
Easing short-term jitters and paving the way for further developing-country growth will require a clear and credible program for returning high-income economies, especially those in Europe, to a sustainable fiscal path.
It will be a bumpy road ahead, requiring careful navigation and bold policy implementation.
The Diabetes Watch
WELLINGTON – The world is currently in the grip of a diabetes epidemic.
A recent major study by Majid Ezzati and colleagues from Imperial College London and Harvard University found that the number of adults with type 2 diabetes increased from an estimated 153 million in 1980 to 347 million in 2008.
The number could be 370 million today.
Every region of the world is affected, although the epidemic is growing most rapidly in Oceania and least rapidly in East Asia.
Globally, the type 2 diabetes epidemic has been growing in lockstep with rising obesity levels.
This is not surprising – an increase in body fat and a decrease in physical activity are the direct causes of type 2 (as opposed to type 1) diabetes.
In fact, much of the health effects of obesity and physical inactivity are mediated through diabetes.
These health effects are serious.
Diabetes already is the major cause of kidney failure, blindness, and lower-limb amputation in many countries, and a major cause of heart attacks and strokes.
Despite this, surveillance of diabetes remains relatively undeveloped throughout the world, even in high-income countries.
Public-health surveillance is “the ongoing systematic collection, analysis, interpretation, and dissemination of health data for the purpose of preventing and controlling disease” – in short, information for action.
There is nothing in this definition that restricts surveillance to communicable diseases, yet in practice this has generally been the case.
The reasons are not hard to find.
Communicable disease outbreaks occur over days to weeks (or at the most, months); the danger is “clear and present”; and prevention and control generally requires intervention by the state – the quarantine of victims, tracing and immunization of contacts, or elimination of environmental sources of the infectious agent.
The situation regarding chronic diseases like diabetes is very different.
The epidemic happens silently over years or decades; the danger is either not recognized or not considered avoidable; and action is often seen as the responsibility of the individual (lifestyle modification) or health-care system (pharmaceutical prescription), rather than the state.
Yet effective chronic disease surveillance can save lives.
If disease trends are monitored, along with patients’ responses to treatment and the population’s exposure to risk factors, the success or failure of policies designed to prevent or control chronic diseases can be evaluated, resource allocation can be rationally prioritized, and the public can be kept fully informed of the risks that they face.
Recognizing this, in December 2005 the New York City Board of Health mandated the laboratory reporting of test results for glycosylated haemoglobin (HbA1c) – a biomarker for diabetes and a key indicator of blood glucose control – thereby creating the world’s first population-based diabetes registry.
Mandatory laboratory reporting of HbA1c results (along with basic demographic data) for a defined population (New York City residents) allowed New York’s Department of Health to monitor trends in diabetes prevalence, assess testing coverage, and examine health-care use and glycemic control of residents living with diabetes.
Beyond these population-based surveillance functions, the registry was able to support patient care by ensuring that individual health-care providers and patients were made aware of elevated or rising HbA1c levels. Both the patient-support function and the surveillance function required use of a unique patient identifier, so that letters could be mailed to patients and tests from the same patient could be linked over time.
In 2009, Thomas Frieden and colleagues from the New York City Board of Health reviewed the registry’s first four years of operation and concluded that it was performing well.
Getting all laboratories to report regularly and completely, however, proved challenging and not all health-care providers and patients proved willing to participate.
The Board of Health’s initiative has been widely praised as exemplifying the application of classical communicable-disease surveillance-and-control tools to a chronic disease.
Other commentators, however, have criticized the registry for potentially compromising patient confidentiality and privacy, and even for disrupting the relationship between patients and their doctors.
While these criticisms may or may not be justified, it is probably true to say that the New York City diabetes registry, though highly innovative, is at best an interim solution.
Rather than relying on laboratory reporting of a single biomarker, an ideal chronic-disease surveillance system would extract all necessary data directly from the patient record.
Any diagnosis of diabetes, or subsequent monitoring of disease progression, requires a medical consultation and hence an entry into the patient record – and so into the practice’s patient-management information system.
Logically, the surveillance system should operate by extracting the entire subset of data required for surveillance purposes from each health-care provider’s patient management information system (“front-end capture”).
This data would then be securely transferred (electronically) to a suitable data warehouse.
After appropriate cleaning (checking for missing data, correcting coding errors), and anonymizing, the data would be available for access and querying.
Given appropriate statistical analysis and careful interpretation, useful reports could be generated for surveillance purposes and, if desired, for patient-care support as well (using encrypted unique patient identifiers to preserve confidentiality of personal information).
In view of the rising burden of diabetes and other chronic diseases throughout the world, urgent attention must be devoted to strengthening surveillance systems for noncommunicable diseases at all levels – from local practices to global institutions.
The Diarrhea Pioneers
PALO ALTO – On top of the devastation caused in Haiti by the January earthquake, Hurricane Tomas this month, and the subsequent dislocations, exposure, and malnutrition, the country is now experiencing an accelerating cholera outbreak. At least 8,000 people are currently in hospital, and the death toll is near 600 from this waterborne bacterial disease.
And, given the widespread unavailability of clean water, basic sanitation, and medical facilities, those numbers are sure to increase.
Even in the absence of natural disasters, diarrhea is the number-two infectious killer of children under the age of five in developing countries (surpassed only by respiratory diseases), accounting for roughly two million deaths a year.
However, thanks to a simple but ingenious innovation by an emerging bioscience company, those numbers could become a relic of the past, like mortality from smallpox and bubonic plague.
Since the 1960’s, the standard of care for childhood diarrhea in the developing world has been the World Health Organization’s formulation of rehydration solution, a glucose-based, high-sodium liquid that is administered orally.
This low-tech product was revolutionary.
It saved countless lives and reduced the need for costly (and often unavailable) hospital stays and intravenous rehydration.
However, this product did nothing to lessen the severity or duration of the condition, which over time leads to malnutrition, anemia, and other chronic health risks.
Other approaches to treatments and preventative measures – including changes in public-health policy, improvement of water treatment, and development of vaccines – have not yielded significant, cost-effective results.
The solution (literally and figuratively) may be an ingenious, affordable innovation from Ventria Bioscience that combines high- and low-tech components to deliver what could be a veritable Holy Grail: two proteins produced inexpensively in rice that radically improve the effectiveness of rehydration solutions.
It has been known for decades that breast-fed children get sick with diarrhea and other infections less often than those fed with formula.
Research in Peru has shown that fortifying oral rehydration solution with two of the primary protective proteins in breast milk, lactoferrin and lysozyme, lessens the duration of diarrhea and reduces the rate of recurrence.&#160; The availability of such an oral rehydration solution to people in the developing world would be a near-miraculous advance.
Ventria joined with researchers at the University of California, Davis, and at a leading children’s hospital and a nutrition institute in Lima, Peru, to test the effects of adding lactoferrin and lysozyme to a rice-based oral rehydration solution. The new brew provides more nutrition than glucose-based oral rehydration solution and tastes better to kids, so they are more likely to drink it.
The researchers found that when lactoferrin and lysozyme are added to solution, the children’s illness is cut from more than five days to 3.7.
This improvement is thought to be caused by the antimicrobial effect of lactoferrin and lysozyme, long known to be the primary protective proteins in breast milk.
Moreover, over the twelve-month follow-up period, the children who had received the rice-based solution had less than half the recurrence rate of diarrhea (8% versus 18% in the control sample).
This effect is probably caused by the lactoferrin, which promotes repair of the diarrhea-damaged cells of the intestinal mucosa.
What makes this approach to managing diarrhea feasible is Ventria’s invention of a genetic-engineering method that uses rice to produce lactoferrin and lysozyme.
This process, dubbed “biopharming,” is an inexpensive and ingenious way to synthesize the large quantities of the proteins that will be necessary.
In effect, the rice plants’ inputs are carbon dioxide, water, soil, and the sun’s energy, and its output is a rice kernel containing large amounts of the proteins.
The kernel is processed to extract and purify the proteins, which are then used to formulate the improved rehydration solution.
Rice is self-pollinating, so out-crossing – interbreeding with other rice varieties – is virtually impossible.
However, to ameliorate even such hypothetical concerns, Ventria has chosen to grow its rice in Kansas, where there are no other rice growers.
The proteins used to supplement the oral rehydration solution have the same structure and functional properties as those in natural breast milk, and the process is analogous to that used routinely for the production of proteins from other organisms, such as bacteria and yeast.
The proteins were judged by a regulatory panel of experts to be “Generally Recognized As Safe” (GRAS) for inclusion in food products like oral rehydration solution.
The typical approach prior to commercialization would be a notification to the United States Food &amp; Drug Administration (FDA) that the rice-produced proteins had been determined to be as safe as their natural counterparts.
Then, the regulators’ review would take no more than six months.
But, while the first GRAS notification was submitted to FDA in 2004 and was followed by further notifications, the FDA took no action; eventually, Ventria withdrew the notifications in March of this year.
Although the company could market the product without the FDA’s approval, the financial risks would be prohibitive.
Researchers at the University of California, Davis, have faced similar regulatory obstacles in attempting to produce lysozyme in the milk of genetically engineered goats.
Their solution was to move the project to Brazil, where it is progressing.
Biopharming has brought us to the verge of a safe, affordable solution to one of the developing world’s most pressing health problems.
But, thanks to bureaucratic obstructionism, it will come too late for the many who will be stricken with diarrheal diseases in Haiti and other poor countries in the coming weeks and months.
Regulators and public-health authorities should be beating down doors and demanding life-saving products like lactoferrin and lysozyme.
Instead, all we hear from them is a deafening silence – and, in Haiti and elsewhere, the silence of the grave.
The Diet Debacle
SAN FRANCISCO – Two seemingly benign nutritional maxims are at the root of all dietary evil: A calorie is a calorie, and You are what you eat.&#160;Both ideas are now so entrenched in public consciousness that they have become virtually unassailable.
As a result, the food industry, aided and abetted by ostensibly well-meaning scientists and politicians, has afflicted humankind with the plague of chronic metabolic disease, which threatens to bankrupt health care worldwide.
The United States currently spends $147 billion on obesity-related health care annually.
Previously, one could have argued that these were affluent countries’ diseases, but the United Nations announced last year that chronic metabolic disease (including diabetes, heart disease, cancer, and dementia) is a bigger threat to the developing world than is infectious disease, including HIV.
These two nutritional maxims give credence to the food industry’s self-serving corollaries: If a calorie is a calorie, then any food can be part of a balanced diet; and, if we are what we eat, then everyone chooses what they eat.
Again, both are misleading.
If one’s weight really is a matter of personal responsibility, how can we explain toddler obesity?
Indeed, the US has an obesity epidemic in six-month-olds.
They don’t diet or exercise.
Conversely, up to 40% of normal-weight people have chronic metabolic disease.
Something else is going on.
Consider the following diets: Atkins (all fat and no carbohydrates); traditional Japanese (all carbohydrates and little fat); and Ornish (even less fat and carbohydrates with lots of fiber).
All three help to maintain, and in some cases even improve, metabolic health, because the liver has to deal with only one energy source at a time.
That is how human bodies are designed to metabolize food.
Our hunter ancestors ate fat, which was transported to the liver and broken down by the lipolytic pathway to deliver fatty acids to the mitochondria (the subcellular structures that burn food to create energy).
On the occasion of a big kill, any excess dietary fatty acids were packaged into low-density lipoproteins and transported out of the liver to be stored in peripheral fat tissue.
As a result, our forebears’ livers stayed healthy.
Meanwhile, our gatherer ancestors ate carbohydrates (polymers of glucose), which was also transported to the liver, via the glycolytic pathway, and broken down for energy.
Any excess glucose stimulated the pancreas to release insulin, which transported glucose into peripheral fat tissue, and which also caused the liver to store glucose as glycogen (liver starch).
So their livers also stayed healthy.
And nature did its part by supplying all naturally occurring foodstuffs with either fat or carbohydrate as the energy source, not both.
Even fatty fruits – coconut, olives, avocados – are low in carbohydrate.
Our metabolisms started to malfunction when humans began consuming fat and carbohydrates at the same meal.
The liver mitochondria could not keep up with the energy onslaught, and had no choice but to employ a little-used escape valve called “de novo lipogenesis” (new fat-making) to turn excess energy substrate into liver fat.
Liver fat mucks up the workings of the liver.
It is the root cause of the phenomenon known as “insulin resistance” and the primary process that drives chronic metabolic disease.
In other words, neither fat nor carbohydrates are problematic – until they are combined.
The food industry does precisely that, mixing more of both into the Western diet for palatability and shelf life, thereby intensifying insulin resistance and chronic metabolic disease.
But there is one exception to this formulation: sugar.
Sucrose and high-fructose corn syrup are comprised of one molecule of glucose (not especially sweet) and one molecule of fructose (very sweet).
While glucose is metabolized by the glycolytic pathway, fructose is metabolized by the lipolytic pathway, and is not insulin-regulated.
Thus, when sugar is ingested in excess, the liver mitochondria are so overwhelmed that they have no choice but to build liver fat.
Today, 33% of Americans have a fatty liver, which causes chronic metabolic disease.
Prior to 1900, Americans consumed less than 30 grams of sugar per day, or about 6% of total calories.
In 1977, it was 75 grams/day, and in 1994, up to 110 grams/day.
Currently, adolescents average 150 grams/day (roughly 30% of total calories) – a five-fold increase in one century, and a two-fold increase in a generation.
In the past 50 years, consumption of sugar has also doubled worldwide.
Worse yet, other than the ephemeral pleasure that it provides, there is not a single biochemical process that requires dietary fructose; it is a vestigial nutrient, left over from the evolutionary differentiation between plants and animals.
It is therefore clear that a calorie is not a calorie.
Fats, carbohydrates, fructose, and glucose are all metabolized differently in the body.
Furthermore, you are what you do with what you eat.Combining fat and carbohydrate places high demands on the metabolic process. And adding sugar is particularly&#160;egregious.
Indeed, while food companies would have you believe that sugar can be part of a balanced diet, the bottom line is that they have created an unbalanced one.
Of the 600,000 food items available in the US, 80% are laced with added sugar.
People cannot be held responsible for what they put in their mouths when their choices have been co-opted.
And this brings us back to those obese toddlers.
The fructose content of a soft drink is 5.3%.
Of course, many parents might refuse to give soft drinks to their children, but the fructose content of soy formula is 5.1%, and 6% for juice.
We have a long way to go to debunk dangerous nutritional dogmas.
Until we do, we will make little headway in reversing an imminent medical and economic disaster.
The Digital Divide
LONDON: A global division is arising between the world’s computer haves and have-nots.
Call it the digital divide.
Seven months ago at the Kyushu-Okinawa Summit the world’s leading industrial countries established a Digital Opportunity Taskforce (dotforce) to share information and communication technologies with poor countries.
Are computer technologies, however, really so easily transferred?
Moreover, will governments in poor and postcommunist countries seize upon the supposedly magical powers of computers as an excuse not to pursue coherent growth strategies?
It is said that, unless developing and postcommunist countries catch up with the computer “revolution”, their economies will fall ever farther behind.
True, integration in the world economy offers the best hope for growth.
But global integration cannot be achieved by IT alone.
Indeed, the word from Microsoft’s Bill Gates is that poor countries need sound development strategies, not a great leap into cyberspace.
Poverty, underdevelopment, and mal-development result from macroeconomic and industrial policies, skewed income distribution, and flawed market infrastructures.
Of these, only the latter may be affected positively by IT.
Undoubtedly, the digital divide excludes much of the world’s population by age, income and residence from today’s computer revolution.
To benefit from Information and Communication Technologies (ICTs) an economy requires, in addition to sophisticated telecommunications infrastructure, fundamental advances in basic literacy and secondary technical education.
These are the pre-conditions for successful technology transfers.
A liberal regulatory regime helps, too.
So, clearly, the dotforce initiative is no stand-alone development tool. Governments must also act.
Unfortunately, government actions usually obstruct more than help.
Some developing and postcommunist countries censor the Internet by excess taxation and/or by limiting access.
So, as international agencies invest in computers for developing countries they must identify the obstacles erected by governments to the diffusion of ICTs, as well as the costs for overcoming these obstacles.
Moreover, technology transfer is not free.
The technical expertise required for IT start-ups will only be transferred by investors if they are compensated.
Still, even if governments do everything right, the benefits of ICTs for low-income countries are limited and difficult to predict.
Right now, IT exports offer the most promise.
High Tech provides 28% of East Asian and Pacific regional exports.
The average level of high tech exports for all of South Asia is 4% with India topping 10%.
India’s software exports, indeed, exceeded $4 billion in 2000, about 9% of India’s total exports.
In Latin America and the Caribbean, high tech exports amount to 12% of manufactured exports.
Fueled by microchip exports (38% of the total), Costa Rica’s economy grew 8.3% in 1999, the highest in Latin America.
Domestic demand in developing and postcommunist countries lags behind because people are usually too poor to buy IT goods and services.
Constraining demand even more is the fact that businesses in poor countries only need IT when they have educated workforces, which most poor country firms neither have nor need.
To change all this, spending on education must change.
In high-income countries public expenditure on education reached 5.4% of GDP in 1997, in middle income countries it was 4.8%, but only 3.3% in low income countries, and only 2.2% in Mali.
India, which spends 3.2% of GDP on education, ranks lower than the average in Sub-Saharan Africa.
Public spending, moreover, on education is often regressive.
In Nepal, the richest quintile receives four times as much public education spending as the poorest quintile.
Skills, including literacy, as well as infrastructure, will also hold the IT revolution back.
The illiteracy rate is 68% in Central African Republic, 57% in India, 16% in South Africa.
In Africa, there are 14 million telephone lines, a figure roughly comparable to that of any of the world’s major cities.
The Internet is of no benefit if you can’t get online.
To assure positive outcomes from ICT investment, policymakers should focus on integrated development, telecommunications infrastructure, and education as the foundation for gradual technology transfer.
Privacy laws, taxation and openness are as important for the development of the Internet as for every other aspect of development.
Institutional and political failures will limit the capacity of developing countries to benefit from investments in technology in the same measure as developed countries, even when they actually acquire computers in optimal numbers.
Because of a lack of empirical evidence, it is too early to say that these obstacles are binding constraints.
But it is clear that IT will deliver benefits only if sound development strategies are pursued.
The failure of other ‘magic bullet’ cures for development should make everyone pause before placing unbounded faith in what the Internet can deliver.
The Digital Provide: What Information Technology Can Do For the World\u0027s Poorest
The Digital War on Poverty
NEW YORK – The digital divide is beginning to close.
The flow of digital information – through mobile phones, text messaging, and the Internet – is now reaching the world’s masses, even in the poorest countries, bringing with it a revolution in economics, politics, and society. 
Extreme poverty is almost synonymous with extreme isolation, especially rural isolation.
But mobile phones and wireless Internet end isolation, and will therefore prove to be the most transformative technology of economic development of our time.
The digital divide is ending not through a burst of civic responsibility, but mainly through market forces.
Mobile phone technology is so powerful, and costs so little per unit of data transmission, that it has proved possible to sell mobile phone access to the poor.
There are now more than 3.3 billion subscribers in the world, roughly one for every two people on the planet. 
Moreover, market penetration in poor countries is rising sharply.
India has around 300 million subscribers, with subscriptions rising by a stunning eight million or more per month.
Brazil now has more than 130 million subscribers, and Indonesia has roughly 120 million.
In Africa, which contains the world’s poorest countries, the market is soaring, with more than 280 million subscribers.
Mobile phones are now ubiquitous in villages as well as cities.
If an individual does not have a cell phone, they almost surely know someone who does.
Probably a significant majority of Africans have at least emergency access to a cell phone, either their own, a neighbor’s, or one at a commercial kiosk.
Even more remarkable is the continuing “convergence” of digital information: wireless systems increasingly link mobile phones with the Internet, personal computers, and information services of all kinds.
The array of benefits is stunning.
The rural poor in more and more of the world now have access to wireless banking and payments systems, such as Kenya’s famous M-PESA system, which allows money transfers through the phone.
The information carried on the new networks spans public health, medical care, education, banking, commerce, and entertainment, in addition to communications among family and friends.
India, home to world-leading software engineers, high-tech companies, and a vast and densely populated rural economy of some 700 million poor people in need of connectivity of all kinds, has naturally been a pioneer of digital-led economic development.
Government and business have increasingly teamed up in public-private partnerships to provide crucial services on the digital network.
In the Indian states of Andhra Pradesh and Gujarat, for example, emergency ambulance services are now within reach of tens of millions of people, supported by cell phones, sophisticated computer systems, and increased public investments in rural health.
Several large-scale telemedicine systems are now providing primary health and even cardiac care to rural populations.
Moreover, India’s new rural employment guarantee scheme, just two years old, is not only employing millions of the poorest through public financing, but also is bringing tens of millions of them into the formal banking system, building on India’s digital networks.
On the fully commercial side, the mobile revolution is creating a logistics revolution in farm-to-retail marketing.
Farmers and food retailers can connect directly through mobile phones and distribution hubs, enabling farmers to sell their crops at higher “farm-gate” prices and without delay, while buyers can move those crops to markets with minimum spoilage and lower prices for final consumers.
The strengthening of the value chain not only raises farmers’ incomes, but also empowers crop diversification and farm upgrading more generally.
Similarly, world-leading software firms are bringing information technology jobs, including business process outsourcing, right into the villages through digital networks.
Education will be similarly transformed.
Throughout the world, schools at all levels will go global, joining together in worldwide digital education networks.
Children in the United States will learn about Africa, China, and India not only from books and videos, but also through direct links across classrooms in different parts of the world.
Students will share ideas through live chats, shared curricula, joint projects, and videos, photos, and text sent over the digital network.
Universities, too, will have global classes, with students joining lectures, discussion groups, and research teams from a dozen or more universities at a time.
This past year, my own university – Columbia University in New York City – teamed up with universities in Ecuador, Nigeria, the United Kingdom, France, Ethiopia, Malaysia, India, Canada, Singapore, and China in a “Global Classroom” that simultaneously connected hundreds of students on more than a dozen campuses in an exciting course on global sustainable development. 
In my book The End of Poverty , I wrote that extreme poverty can be ended by the year 2025.
A rash predication, perhaps, given global violence, climate change, and threats to food, energy, and water supplies.
But digital information technologies, if deployed cooperatively and globally, will be our most important new tools, because they will enable us to join together globally in markets, social networks, and cooperative efforts to solve our common problems.
The Dilemma of Curiosity and Its Use
Albert Einstein once said, “I have no special gift, but I am passionately curious.” Certainly, Einstein was being tremendously modest.
But, just as certainly, curiosity is a powerful driving force in scientific discovery.
Indeed, along with talent and interest, as well as mathematical or other quantitative abilities, curiosity is a necessary characteristic of any successful scientist.
Curiosity betrays emotional passion.
It is a state of being involuntarily gripped by something that is difficult to ward off and for which, since one cannot act otherwise, one is accountable only in a limited sense.
We all come into the world curious, equipped with the psychological drive to explore the world and to expand the terrain that we think we master.
It is no coincidence that a well-known book on developmental psychology bears the title The Scientist in the Crib, a work that traces the parallels between small children’s behavior and the processes and research strategies that are usual in science.
But the urge for knowledge that drives inborn curiosity to transcend given horizons does not remain uncurbed.
Parents can tell many a tale about how, with the beginning of school, their children’s playful approach suddenly changes, as they must now focus on objects dictated by the curriculum.
Likewise, however desirable its ability to produce the unexpected and unforeseeable, science today cannot claim that it is not accountable to society.
Curiosity is insatiable and, in research, it is inextricably tied to the unforeseeability of results.
Research is an endless process, with a destination that no one can predict precisely.
The more that unexpected results, brought forth by research in the laboratory, are a precondition for further innovations, the more pressure there is to bring the production of knowledge under control, to direct research in specific directions, and to tame scientific curiosity.
But curiosity must not be limited too severely, lest science’s ability to produce new knowledge be lost.
This dilemma is at the center of many policy debates surrounding scientific research.
To be sure, not everything that arouses scientific curiosity is controversial; in fact, most scientific research is not.
Still, the dilemma is obvious in pioneering fields like biomedicine, nanotechnology, and neurosciences.
Research in these areas sometimes meets with vehement rejection, for example, on religious grounds with respect to stem-cell research, or owing to fear with respect to the possibility of altering human identity.
Curiosity implies a certain immoderation, a certain necessary excess.
That is precisely what makes it a passion: it is amoral and follows its own laws, which is why society insists on taming it in various ways.
Private investment in research directs curiosity onto paths where new scientific breakthroughs promise high economic potential.
Politicians expect research to function as a motor of economic growth.
Ethics commissions want to establish limits on research, even if these require frequent re-negotiation.
The demand for more democratic input, finally, also seeks to influence research priorities.
These considerations must be borne in all efforts to support basic research.
In Europe, the establishment of the European Research Council (ERC) is entering a decisive phase, with crucial implications concerning the role we are prepared to concede to scientific curiosity.
For the first time, support for basic research is being made possible on the EU level.
Individual teams are to enter a pan-European competition to determine the best of the best, opening a free space for scientific curiosity and enabling the unforeseeable outcomes that are characteristic of cutting-edge research.
The dilemma – and it is a decisive one – is that today we cherish the passionate curiosity of an Albert Einstein. But we still want to control the unforeseeable consequences to which curiosity leads.
The dilemma must be overcome by allowing curiosity to be protected and supported, while trying to capture those of its fruits that will benefit society.
How we accomplish this must be continuously negotiated in the public sphere.
Irreducible contradictions will remain, and therein lie the ambivalence that characterizes modern societies’ stance toward science.
The Dilemma of Multiculturalism
Many people have suddenly become very hesitant about using the term “multicultural society.”
Or they hesitate to use it approvingly, as a desirable ideal that social reality should at least approximate.
July’s terrorist attacks in London demonstrated both the strength and the weakness of the concept.
London is certainly a multicultural metropolis.
An indiscriminate attack such as a bomb in the Underground will necessarily hit people of many cultural backgrounds and beliefs.
Sitting, or more likely standing, in the “tube” (as London’s Underground is affectionately known), one never ceases to be amazed at the ease with which Jewish mothers and Muslim men, West Indian youngsters and South Asian businessmen, and many others endure the same stressful conditions and try to lighten its impact by being civil to one another.
The terror attacks demonstrated not only how particular people helped each other, but also how the whole city, with all the ingredients of its human mixture, displayed a common spirit of resilience.
This is the positive side of a multicultural society.
Careful observers have always noted that it is strictly confined to the public sphere, to life in those parts of the city that are shared by all.
It does not extend in quite the same way to people’s homes, let alone to their ways of life in the private sphere.
This is one reason why London has experienced the other, darker side of the multicultural society: the veneer of multiculturalism is thin.
It does not take much to turn people of one group against those of others with whom they had apparently lived in peace.
We know this because it lies at the core of the murderous environment that gripped the Balkans in the 1990’s.
For decades (and in some cases much longer), Serbs and Croats, – indeed, Orthodox, Catholic, and Muslim “Yugoslavs” – had lived together as neighbors.
Few thought it possible that they would turn against each other in a bloodletting of such brutal enormity that it is very unlikely that Bosnia-Herzegovina can ever become a successful multicultural society.
Yet it happened, and in a different way it is happening now in Britain.
It is important to recognize that we are not talking about the return of age-old hostilities.
Ethnic and cultural conflicts today, often in the form of terrorism, are not the eruption of a supposedly extinct volcano.
They are, on the contrary, a specifically modern phenomenon.
For the terrorists themselves, such conflicts are one consequence of the unsettling effects of modernization.
Beneath the veneer of integration into a multicultural environment, many people – especially young men with an immigrant background – are lost in the world of contradictions around them.
Their seamless, all-embracing world of tradition is gone, but they are not yet confident citizens of the modern, individualistic world.
The question is not primarily one of employment, or even poverty, but of marginalization and alienation, of the lack of a sense of belonging.
It is in such circumstances that the key feature of terrorism comes into play: the preaching of hate by often self-appointed leaders.
They are not necessarily religious leaders; in the Balkans and elsewhere, they are nationalists who preach the superiority of one nationality over others.
But to call these hate-mongers “preachers” is fitting nonetheless, because they invariably appeal to higher values to sanctify criminal acts.
The mobilization of criminal energies by such preachers of hate is itself a modern phenomenon.
It is a far cry even from such doubtful claims as the self-determination of peoples defined as ethnic communities.
Hate preachers use highly modern methods to enhance their personal power and to create havoc around them.
But countering them does not involve warfare, or even a rhetorically looser “war on terror.”
Of course, part of the answer is to identify the relatively small number of people who are prepared to use their lives to destroy the lives of others without distinction or purpose.
But the more important issue is to identify the preachers of hate and stop their murderous incitement.
This is why it is so important to capture and prosecute Radovan Karadzic, who spurred on the homicidal rage of so many Bosnian Serbs.
And this is why militant Islamist preachers must be stopped.
Beyond this carefully targeted – and, in principle, limited – task, there remains the need to strengthen the sphere of common values and cooperation in societies that will, after all, remain multicultural.
This will be difficult, and it must not be approached naively.
Differences will not – and need not – go away; but ensuring that all citizens can rely on each other requires us to find a way to extend and bolster the civic trust that we see in the public sphere.
The Diplomacy of the Blind
PARIS – Why do revolutions so often take professional diplomats by surprise?
Is there something in their DNA that makes them prefer the status quo so much that, more often than not, they are taken aback by rapid changes, neither foreseeing them nor knowing how to respond once they begin?
What is happening today in the Arab world is a revolution that may turn out to be for the Middle East the equivalent of what the French Revolution was for Europe in 1789: a profound and radical change that alters completely the situation that prevailed before.
How many Bastilles will ultimately fall in the region, and at what pace, no one can say.
The only recent analogy is the collapse of the Soviet bloc, followed by the demise of the Soviet Union itself, in 1989-1991.
Who saw that sudden and rapid transformation coming?
As the German Democratic Republic was about to disappear, some top French diplomats in Germany were still assuring their government in Paris that the Soviet Union would never accept German reunification, so there was nothing to worry about: life would go on nearly as usual.
The specter of a united Germany was not to become a reality soon.
We saw the same conservative instinct at work with the first reactions to the events in Tunisia, and then in Egypt. “President Ben Ali is in control of the situation,” some said.
Or “President Mubarak has our complete confidence.”
The United States managed to get it right, albeit very slowly, whereas many European countries erred on the side of the status quo for a much longer time, if not systematically, as they refused to see that the region could be evolving in a direction contrary to what they deemed to be in their strategic interest.
Historical and geographic proximity, together with energy dependency and fear of massive immigration, paralyzed European diplomats.
But there is something more fundamental underlying diplomats’ natural diffidence.
They are very often right in their readings of a given situation – the US diplomatic cables released by WikiLeaks, for example, include a slew of masterful and penetrating analyses.
But it is as if, owing to an excess of prudence, they cannot bring themselves to pursue their own arguments to their logical conclusions.
Revolutionary ruptures upset diplomats’ familiar habits, both in terms of their personal contacts and, more importantly, in terms of their thinking.
A fast-forward thrust into the unknown can be exhilarating, but it is also deeply frightening.
In the name of “realism,” diplomats and foreign-policy strategists are naturally conservative.
Indeed, it is no accident that Henry Kissinger’s masterpiece, A World Restored, was devoted to the study of the recreation of the world order by the Vienna Congress after the rupture of the French Revolution, followed by the Napoleonic adventures.
Is it more difficult to predict, and adjust to, the coming of a fundamental change, than to defend the present order, under the motto of “the devil you know is always preferable to the devil you don’t know!”
But, beyond these mental habits lie more structural reasons for the conservatism of foreign policymakers and diplomats.
By emphasizing the relations between states and governments over contacts with the opposition or civil societies (when they exist in an identifiable form), traditional diplomacy has created for itself a handicap that is difficult to overcome.
By requiring their diplomats to limit their contacts with “alternative” sources of information in a country, in order to avoid antagonizing despotic regimes, governments irremediably limit diplomats’ ability to see change coming, even when it is so close that nothing can be done.
When regimes lose legitimacy in the eyes of their citizens, it is not reasonable to derive one’s information mainly from that regime’s servants and sycophants.
In such cases, diplomats will too often merely report the regime’s reassuring yet biased analysis.
Diplomats, instead, should be judged by their ability to enter into a dialogue with all social actors: government representatives and business leaders, of course, but also representatives of civil society (even if it exists only in embryonic form).
With proper training and incentives, diplomats would be better equipped to anticipate change.
Of course, not all Western foreign ministries are the same; some do understand the need to nurture relationships with people outside the government – if not in opposition to it.
But one thing is clear: the more traditional foreign ministries tend to be, the more difficult it is for them and their diplomats to grasp change.
Needless to say, the ability to comprehend change has become indispensable at a time when the world is experiencing tectonic geopolitical shifts.
The new Middle East that is emerging before us is probably both “post-Western,” given the rise of new powers, and “post – Islamist,” with the revolt led by young, technologically savvy people with no ties to political Islam whatsoever.
By being late in perceiving change that they do not want to see coming, Western diplomats run the risk of losing on both levels: the regime and the people.
Diplomats require openness and imagination in order to carry out their responsibilities.
They should not abdicate these qualities when they are needed most.
The Dirt on Nuclear Power
SINGAPORE – Japan’s nuclear crisis is a nightmare, but it is not an anomaly.
In fact, it is only the latest in a long line of nuclear accidents involving meltdowns, explosions, fires, and loss of coolant – accidents that have occurred during both normal operation and emergency conditions, such as droughts and earthquakes.
Nuclear safety demands clarity about terms.
The Nuclear Regulatory Commission in the United States generally separates unplanned nuclear “events” into two classes, “incidents” and “accidents.”
Incidents are unforeseen events and technical failures that occur during normal plant operation and result in no off-site releases of radiation or severe damage to equipment.
Accidents refer to either off-site releases of radiation or severe damage to plant equipment.
The International Nuclear and Radiological Event Scale uses a seven-level ranking scheme to rate the significance of nuclear and radiological events: levels 1-3 are “incidents,” and 4-7 are “accidents,” with a “Level 7 Major Accident” consisting of “a major release of radioactive material with widespread health and environmental effects requiring implementation of planned and extended countermeasures.”
Under these classifications, the number of nuclear accidents, even including the meltdowns at Fukushima Daiichi and Fukushima Daini, is low.
But if one redefines an accident to include incidents that either resulted in the loss of human life or more than $50,000 in property damage, a very different picture emerges.
At least 99 nuclear accidents meeting this definition, totaling more than $20.5 billion in damages, occurred worldwide from 1952 to 2009 – or more than one incident and $330 million in damage every year, on average, for the past three decades.
And, of course, this average does not include the Fukushima catastrophe.
Indeed, when compared to other energy sources, nuclear power ranks higher than oil, coal, and natural gas systems in terms of fatalities, second only to hydroelectric dams.
There have been 57 accidents since the Chernobyl disaster in 1986.
While only a few involved fatalities, those that did collectively killed more people than have died in commercial US airline accidents since 1982.
Another index of nuclear-power accidents – this one including costs beyond death and property damage, such as injured or irradiated workers and malfunctions that did not result in shutdowns or leaks – documented 956 incidents from 1942 to 2007.
And yet another documented more than 30,000 mishaps at US nuclear-power plants alone, many with the potential to have caused serious meltdowns, between the 1979 accident at Three Mile Island in Pennsylvania and 2009.
Mistakes are not limited to reactor sites.
Accidents at the Savannah River reprocessing plant released ten times as much radio­iodine as the accident at Three Mile Island, and a fire at the Gulf United facility in New York in 1972 scattered an undisclosed amount of plutonium, forcing the plant to shut down permanently.
At the Mayak Industrial Reprocessing Complex in Russia’s southern Urals, a storage tank holding nitrate acetate salts exploded in 1957, releasing a massive amount of radioactive material over 20,000 square kilometers, forcing the evacuation of 272,000 people.
In September 1994, an explosion at Indonesia’s Serpong research reactor was triggered by the ignition of methane gas that had seeped from a storage room and exploded when a worker lit a cigarette.
Accidents have also occurred when nuclear reactors are shut down for refueling or to move spent nuclear fuel into storage.
In 1999, operators loading spent fuel into dry-storage at the Trojan Reactor in Oregon found that the protective zinc-carbon coating had started to produce hydrogen, which caused a small explosion.
Unfortunately, on-­site accidents at nuclear reactors and fuel facilities are not the only cause of concern.
The August 2003 blackout in the northeastern US revealed that more than a dozen nuclear reactors in the US and Canada were not properly maintaining backup diesel generators.
In Ontario during the blackout, reactors de­signed to unlink from the grid automatically and remain in standby mode instead went into full shutdown, with only two of twelve reac­tors behaving as expected.
As environmental lawyers Richard Webster and Julie LeMense argued in 2008, “the nuclear industry…is like the financial industry was prior to the crisis” that erupted that year. “[T]here are many risks that are not being properly managed or regulated.”
This state of affairs is worrying, to say the least, given the severity of harm that a single serious accident can cause.
The meltdown of a 500-megawatt reactor located 30 miles from a city would cause the immediate death of an estimated 45,000 people, injure roughly another 70,000, and cause $17 billion in property damage.
A successful attack or accident at the Indian Point power plant near New York City, apparently part of Al Qaeda’s original plan for September 11, 2001, would have resulted in 43,700 immediate fatalities and 518,000 cancer deaths, with cleanup costs reaching $2 trillion.
To put a serious accident in context, according to data from my forthcoming book Contesting the Future of Nuclear Power, if 10 million people were exposed to radiation from a complete nuclear meltdown (the containment structures fail completely, exposing the inner reactor core to air), about 100,000 would die from acute radiation sickness within six weeks.
About 50,000 would experience acute breathlessness, and 240,000 would develop acute hypothyroidism.
About 350,000 males would be temporarily sterile, 100,000 women would stop menstruating, and 100,000 children would be born with cognitive deficiencies.
There would be thousands of spontaneous abortions and more than 300,000 later cancers.
Advocates of nuclear energy have made considerable political headway around the world in recent years, touting it as a safe, clean, and reliable alternative to fossil fuels.
But the historical record clearly shows otherwise.
Perhaps the unfolding tragedy in Japan will finally be enough to stop the nuclear renaissance from materializing.
The Disappearance of Europe
BRUSSELS – What will it mean to be European 25 years from now?
Unlike the United States, whose history as a “melting pot” has given Americans a truly multi-ethnic character, native Europeans are becoming an endangered species.
Europe badly needs immigrants, yet is not culturally prepared to welcome them. The coming decades will therefore see substantially greater social change in Europe than elsewhere, although the nature of that change is far from clear.
At first glance, much of Europe’s current debate is about political and economic integration – about how far its nation states should go in pooling resources and sovereign powers in the European Union. But beneath the surface, the real tensions are about immigration and fears that national “cultures” are threatened by the influx of non-natives, both white and non-white.
Immigration in Europe today is running at a higher rate than in the US, with almost two million people arriving officially every year, together with an unknown number of illegal immigrants.
The most conservative estimate, by Eurostat, the EU’s statistical agency, puts the total number of newcomers to Europe between now and 2050 at 40 million. Inevitably, that sort of influx will ensure that Europe’s already vociferous right-wing extremist politicians win even greater support.
The specter of rising racial tensions is worrying enough. But it is just one aspect of Europe’s urgent need to import people from Africa and Asia.
Europeans will also see the dismantling of their welfare states and social security systems; the cherished “European model” of pensions, healthcare, and unemployment benefits risks being replaced by the despised and widely feared “American model.”
This is not, needless to say, because Europeans crave the rigors of America’s less cosseted social conditions, but because it’s the only way that European governments will be able to stay afloat financially.
The root cause of all these developments is Europe’s population shrinkage.
The “demographic time bomb” that economic analysts have been discussing for a decade or more is now exploding.
The result is widespread labor shortages in many EU countries and an alarming reduction in the proportion of working-age people whose taxes pay the pensions and medical costs of those who have retired.
Many countries have themselves aggravated the problem, either by encouraging early retirement or – as with France – through job-creation nostrums like the 35-hour working week.
About one-third of male workers in Europe quit their jobs by their early fifties.
That, together with two generations in which birth rates across Europe have dropped well below the two-children-per-couple replacement rate, and what the European Commission describes as “spectacular” increases in longevity, means that by 2050, instead of four workers supporting each retiree, there will be only two.
In short, European policymakers are in an impossible position.
The political mindset in most EU countries remains firmly focused on unemployment as the chief ill to be cured, whereas the real threat is the worsening shortage of people to fill job vacancies.
The European Commission has warned that this will put a lower ceiling on GDP growth rates.
According to Klaus Regling, the Commission’s Director-General for Economic and Financial Affairs, Europe’s working population has so shrunk that from 2010 onward maximum annual economic growth in western Europe will drop to 1.8%, from an average of 2.3% in recent years, and to just 1.3% a year from 2030.
Economic stagnation on this scale has alarming implications, because it means less and less tax revenues to fund all the reform projects and infrastructural investments Europe badly needs to regain its productivity and high-tech competitive edge.
And if things look bad for Western Europe, they’re worse for the EU’s formerly communist newcomers, whose demographic trends imply that average potential growth will nosedive from today’s healthy 4.3% per year to just 0.9% after 2030.
Much of Europe already suffers from under-performing labor markets in which job seekers and would-be employers either can’t or won’t find one another.
Stubbornly high youth unemployment, along with Europe’s dwindling numbers of school-leavers, is already canceling out the positive effects of immigration. Here in Brussels, where the largely North African immigrant population comprises a quarter of the city’s inhabitants, hotels and restaurants recently resorted to an emergency on-line recruitment service to counter their worsening staff shortages.
The manpower crisis is even more acute in sectors that demand greater skills and qualifications.
Like the US, Europe’s manpower-related difficulties are accentuated by the rise of India and China.
How Europeans, and to a lesser extent Americans, will maintain their high standards of living is anyone’s guess.
But Europe’s problem is greater, for its politicians are at a loss to cope with the high-voltage issues of race, religion, and ethnicity in societies that seem determined to remain anchored in the past.
The Disappearing Sky
Some months ago, an American astronaut accidentally let a tool escape into orbit, eliciting concern about its hazardous potential as a hurtling object that could destroy an expensive satellite or even threaten lives aloft.
Shortly afterwards, China blew up one of its satellites, immediately doubling the type of fine orbiting debris that is dangerous because it is hard to track.
Once again the world became aware of the strange situation emerging in our skies.
The sky is a unique domain, and one that is inadequately regulated.
With the advent of global pollutions and technologies, remedying this is becoming an increasingly urgent problem.
In most cases, the laws for skies mirror those governing the world’s oceans.
Oceans belong to everyone except those near landmasses, which are managed in a similar manner to the country's land-bound borders.
As a result, the sky is usually conceptualized in terms of traffic.
Airliners and fighter planes operate in “controlled” air close to the ground, while nationality is supposed to matter less the higher you go.
Fragile treaties that cover this are enforced mostly by the fact that few nations can afford to place assets that high.
But lately, more complex problems are beginning to arise from humanity’s sharing the atmosphere.
Carbon and fluorocarbons affect everyone’s children.
When Chernobyl exploded, it was not Ukraine alone that inherited generations of radioactive effects.
Soon nations will colonize the moon, giving rise to the same unsatisfactory and tentative situation we have in Antarctica, where nations essentially take without legally owning.
A more enlightened approach to shared resources is needed, one less dependent on neo-colonial control.
Some suggest that sky governance follow the precedent set by the electromagnetic spectrum.
The “airwaves” are used for a variety of communications, including government use and public access like radio.
The range of usable territory – the spectrum – is administered by governments as though it were real estate, and is broken up according to wavelength, with an amount apportioned for cell phones, other bits for military pilots, and so on.
This situation would be disastrous if it could not be closely managed, because people would broadcast on top of one another.
Soon, we will see the spectrum become even more active, with the merger of cell phone infrastructure and the relatively unregulated Internet.
This will likely be followed by more sophisticated means of communications based on access to the air.
To some extent, everyone will benefit from this, because governments will be less able to censor information.
But would it be a better model of atmospheric administration?
Perhaps not.
The problem is that at least some electromagnetic waves are dangerous.
Consider this: at any given moment, the average citizen in the developed world has billions of messages passing through his or her brain.
It can be shown that cells are able to detect these messages, but the extent to which they affect the body is unknown.
However, it is known that bees are dying in the northern hemisphere.
This is a major concern because so much food depends on bees for pollination.
The primary causes of this recent epidemic are germs and mites, but these have always been with us.
So why are they affecting bees now? 
A German study suggests that the proliferation of cell phone towers is weakening bees’ immune systems (the study correlates towers and signal strength to bee deaths).
The jury is still out, but it may be that there is no safe level of exposure to many common radiations; the more we are exposed, the more damage we do.
We would see the result of this in indirect effects, such as growing rates of asthma and hyperactivity in children.
So perhaps the problem with existing regulatory models lies in the assumption that the entire atmosphere is available for unconstrained use.
We have an intuitive understanding of the importance of limits when the loss of our sky is articulated in poetic terms.
As light pollution covers more of the planet, we are losing one of our oldest connections to nature: the ancient ability to gaze at the stars.
If dying bees do not inspire formal guidelines about how the sky should be shared, let us hope that empty space will.
The sky must belong to the people.
Abuse of it harms everyone, and profits from its use should benefit all as well, implying the need to establish worldwide democratic rights over what is an unarguably universal resource.
The Discreet Terror of Fidel Castro
This spring marks the third anniversary of the wave of repression in which Fidel Castro’s regime arrested and handed down long sentences to 75 leading Cuban dissidents.
Soon afterwards, many friends and I formed the International Committee for Democracy in Cuba.
The bravery of those who found their social conscience, overcame fear, and stood up to communist dictatorship remains fresh in my memory.
It reminds me of the jingle of keys that rang out on Prague’s Wenceslas Square -- and later around the rest of what was then Czechoslovakia -- in the autumn of 1989.
This is why I rang keys during the conference calling for democracy in Cuba that our committee held in Prague three years ago.
I wanted to draw the international community’s attention to the human-rights situation in Cuba, to support that country’s opposition, and to encourage all pro-democratic forces.
The European Union then introduced diplomatic sanctions, albeit mostly symbolic, against Castro’s regime.
Soon after, however, a contrary position came to the fore.
The EU opened a dialogue with the Cuban regime, sanctions were conditionally suspended, and it was even made clear to dissidents that they were not welcome at the embassies of several democratic countries.
Cowardly compromise and political alibis – as so often in history – defeated a principled position.
In return, the Cuban regime made a sham gesture by releasing a small number of the prisoners of conscience – mostly those who were tortured and seriously ill – who the regime most feared would die in its notorious prisons.
Those of us who live in Europe’s new post-communist democracies experienced similar political deals when we lived behind the former Iron Curtain.
We are also extremely familiar with the argument that European policies have not led to any mass arrests in Cuba.
But democracy has shown weakness and the Cuban regime has in turn adapted its tactics.
Respected organizations like Reporters without Borders and Amnesty International have collected ample evidence of violence and intimidation against freethinking Cubans, who can expect a different kind of ring than that from jangling keys.
Their cases often do not end in courts but in hospitals.
Groups of “fighters for the revolution” – in reality, the Cuban secret police – brutally attack their political opponents and accuse them of absurd crimes in an effort to intimidate them or to force them to emigrate.
On the island, such planned harassments are called “actos de repudio” – “acts of rejection.”
Political violence that creates the impression of mere street crime is never easy to prove, unlike jail terms of several years, and therefore it does not receive due attention from the world.
However, thousands of former political prisoners in central and eastern Europe can attest to the fact that a kick from a secret policeman on the street hurts just as much as a kick from a warden behind prison gates.
The powerlessness of the victim of state-organized street fights and threats against his family is experienced in the same way as the powerlessness of somebody harassed during a state security investigation.
Many European politicians who have sought to see the situation on the ground have been barred in recent years.
Some Europeans apparently regard Cuba as a faraway country whose fate they need take no interest in, because they have problems of their own.
But what Cubans are enduring today is part of our own European history.
Who better than Europeans, who brought communism to life, exported it to the world, and then paid dearly for it over many decades, know better about the torments inflicted upon the Cuban people?
Humanity will pay the price for communism until such a time as we learn to stand up to it with all political responsibility and decisiveness.
We have many opportunities to do so in Europe and Cuba.
And it is no surprise that the new member countries of the EU have brought to Europe fresh historical experience, and with it far less understanding for and tolerance of concession and compromise.
Representatives of the EU’s member states will meet in Brussels in mid-June to review a common policy towards Cuba.
European diplomats should weigh up the consequences of accommodating Castro’s regime.
They should show that they will neither ignore his practices nor neglect the suffering of Cuban prisoners of conscience.
We must never forget the seemingly anonymous victims of Castro’s “acts of rejection.”
The Diseases of Global Warming
Today few scientists doubt that Earth's atmosphere is warming.
Most also agree that the rate of heating is accelerating and that the consequences could become increasingly disruptive.
Even schoolchildren can recite some projected outcomes: oceans will warm and glaciers will melt, causing sea levels to rise and salt water to inundate low-lying coastal areas. Regions suitable for farming will shift.
But less familiar effects of global warming--namely, serious human medical disorders--are no less worrisome.
Many are already upon us.
Most directly, global warming is projected to double the number of deaths related to heat waves by 2020.
Prolonged heat can increase smog and the dispersal of allergens, causing respiratory symptoms.
Global warming also boosts the frequency and intensity of floods and droughts.
Such disasters not only cause death by drowning or starvation, but also damage crops and make them vulnerable to infection and infestations by pests and choking weeds, thereby contributing to food shortages and malnutrition.
They displace entire populations, leading to overcrowding and associated diseases, such as tuberculosis.
Developing countries--where resources to prevent and treat infectious diseases are scarce--are most vulnerable to other infectious diseases associated with climate change as well.
But advanced nations, too, can fall victim to surprise attack--as happened last year when the first outbreak of West Nile virus in North America killed seven New Yorkers.
International commerce and travel enable infectious diseases to strike continents away from their sources.
Of course, not all the human health consequences of global warming may be bad.
Very high temperatures in hot regions may reduce snail populations, which have a role in transmitting schistosomiasis, a parasitic disease.
High winds--caused by parching of the earth's surface--may disperse pollution.
Warmer winters in normally chilly areas may reduce cold-related heart attacks and respiratory ailments.
Overall, however, the undesirable effects of more variable and extreme weather are likely to overshadow any benefits.
Diseases relayed by mosquitoes--malaria, dengue fever, yellow fever, and several kinds of encephalitis--are eliciting particularly grave concern as the world warms.
These disorders are projected to become increasingly prevalent because cold weather limits mosquitoes to seasons and regions with certain minimum temperatures.
Extreme heat also limits mosquito survival.
But within their survivable range of temperatures, mosquitoes proliferate faster and bite more as the air becomes warmer.
Greater heat also speeds the rate at which pathogens inside them reproduce and mature.
The immature malaria parasite takes 26 days to develop fully at 68 degrees F, but only 13 days at 77 degrees F. The  Anopheles  mosquitoes that spread malaria live only several weeks, so warmer temperatures enable more parasites to mature in time for the mosquitoes to transfer the infection.
As whole areas heat up, mosquitoes enter formerly forbidden territories, bringing illness with them, while causing more disease for longer periods in the areas they already inhabit.
Malaria has already returned to the Korean peninsula, and parts of the US, southern Europe and the former Soviet Union have experienced small outbreaks.
Some models project that by the end of this century, the zone of potential malaria transmission will contain about 60% of the world's population, up from 45% now.
Similarly, Dengue (or "breakbone") fever--a severe flu-like viral illness that can cause fatal internal bleeding--has broadened its range in the Americas over the past 10 years, reaching down to Buenos Aires by the end of the 1990's.
(It has also found its way to northern Australia.)
Today it afflicts an estimated 50-100 million people in the tropics and subtropics.
These outbreaks, of course, cannot be traced conclusively to global warming.
Other factors--declines in mosquito-control and other public health programs, or rising drug and pesticide resistance--could be involved.
But the case for a climatic cause becomes stronger when outbreaks coincide with other projected consequences of global warming.
Such is the case in the world's highlands.
In the 19th century, European colonists in Africa settled in the cooler mountains to escape the dangerous swamp air (" mal aria ") in the lowlands.
Today many of those havens are compromised.
As anticipated, warmth is climbing up many mountains.
Since 1970, the elevation at which temperatures are always below freezing has ascended almost 500 feet in the tropics.
Insect-borne infections are being reported at high elevations in South and Central America, Asia, and east and central Africa.
More droughts and floods due to global warming will also probably fuel outbreaks of water-borne diseases.
Paradoxically, droughts can favor water-borne diseases--including cholera, a cause of severe diarrhea--by wiping out supplies of safe drinking water, concentrating contaminants, and preventing good hygiene.
Lack of clean water also limits safe rehydration of diarrhea or fever sufferers.
Floods, meanwhile, wash sewage and fertilizer into water supplies, triggering expansive blooms of harmful algae that are either directly toxic to humans, or contaminate the fish and shellfish that humans consume.
The human health toll taken by global warming will depend to a large extent on us.
Effective surveillance of climate conditions and of the emergence or resurgence of infectious diseases (or their carriers) should be a global priority, as should providing preventive measures and treatments to at-risk populations.
But we must also limit human activities that contribute to atmospheric heating, or that exacerbate its effects.
Little doubt remains that burning fossil fuels contributes to global warming by spewing carbon dioxide and other heat-absorbing, or "greenhouse," gases into the air.
Analysis of tree rings identifies fossil fuels as the source of the 30% increase in greenhouse gases over pre-industrial levels.
Cleaner energy sources must be adopted, while forests and wetlands must be preserved and restored to absorb carbon dioxide, and to absorb floodwaters and filter contaminants before they reach water supplies.
None of this will come cheap.
But humanity will pay a far dearer price for inaction.
The Diseases of Globalization
Globalization is under strain as never before.
Everywhere its stresses rumble.
Most of sub-Saharan Africa, South America, the Middle East, and Central Asia are mired in stagnation or economic decline.
North America, Western Europe, and Japan are bogged down in slow growth and risk renewed recession.
War now beckons in Iraq.
For advocates of open markets and free trade this experience poses major challenges.
Why is globalization so at risk?
Why are its benefits seemingly concentrated in a few locations?
Can a more balanced globalization be achieved?
No easy answers to these questions exist.
Open markets are necessary for economic growth, but they are hardly sufficient.
Some regions of the world have done extremely well from globalization - notably East Asia and China in recent years.
Yet some regions have done miserably, especially sub-Saharan Africa.
The US Government pretends that most problems in poor countries are of their own making.
Africa's slow growth, say American leaders, is caused by Africa's poor governance.
Life, however, is more complicated than the Bush Administration believes.
Consider Africa's best governed countries - Ghana, Tanzania, Malawi, Gambia.
Each experienced falling living standards over the past two decades while many countries in Asia that rank lower on international comparisons of governance - Pakistan, Bangladesh, Myanmar and Sri Lanka - experienced better economic growth.
The truth is that economic performance is determined not only by governance standards, but by geopolitics, geography, and economic structure.
Countries with large populations, and hence large internal markets, tend to grow more rapidly than countries with small populations.
(Like all such economic tendencies, there are counterexamples).
Coastal countries tend to outperform landlocked countries.
Countries with high levels of malaria tend to endure slower growth than countries with lower levels of malaria.
Developing countries that neighbor rich markets, such as Mexico, tend to outperform countries far away from major markets.
These differences matter.
If rich countries don't pay heed to such structural issues, we will find that the gaps between the world's winners and losers will continue to widen.
If rich countries blame unlucky countries - claiming that they are somehow culturally or politically unfit to benefit from globalization - we will create not only deeper pockets of poverty but also deepening unrest.
This, in turn, will result in increasing levels of violence, backlash, and yes, terrorism.
So it's time for a more serious approach to globalization than rich countries, especially America, offer.
It should begin with the most urgent task - meeting the basic needs of the world's poorest peoples.
In some cases their suffering can be alleviated mainly through better governance within their countries.
But in others, an honest look at the evidence will reveal that the basic causes are disease, climatic instability, poor soils, distances from markets, and so forth.
An honest appraisal would further show that the poorest countries are unable to raise sufficient funds to solve such problems on their own.
Rather than rich countries giving more lectures about poor governance, real solutions will require that rich countries give sufficient financial assistance to overcome the deeper barriers.
One illustration makes the case vividly. Controlling disease requires a health system that can deliver life-saving medications and basic preventive services such as bed-nets to fight malaria and vitamins to fight nutritional deficiencies.
Such a system costs, at the least, around $40 per person per year.
That's a tiny amount of money for rich countries, which routinely spend over $2000 per person per year for health, but it is a sum out of reach for poor countries like Malawi, with annual incomes of $200 per person.
A functioning health system would cost more than its entire government revenue!
Even if Malawi is well governed, its people will die of disease in large numbers unless Malawi receives adequate assistance.
Successful globalization requires that we think more like doctors and less like preachers.
Rather than castigating the poor for their "sins," we should make careful diagnoses, as a good doctor would, for each country and region - to understand the fundamental factors that retard economic growth and development.
In some regions, such as the Andes and Central Asia, the problem is primarily geographical isolation.
Here the task is to build roads, air links, and internet connectivity to help these distant regions create productive ties with the world.
Rich countries must help finance these projects.
In sub-Saharan Africa, the basic challenges are disease control, soil fertility, and expanded educational opportunities.
Once again, greater foreign assistance will be needed.
In still other regions, the main problems may be water scarcity, discrimination against women or other groups, or one of a variety of specific problems.
It is past time to take globalization's complexities seriously.
The one-size-fits-all ideology of the Washington Consensus is finished.
As we stand at the brink of war, it is urgent that the hard job of making globalization work for all gets started.
This can be done, if we remove the ideological blinkers of the rich, and mobilize a partnership between rich and poor.
Our common future depends on it.
The Dismal Economist’s Joyless Triumph
NEW YORK – I have long been forecasting that it was only a matter of time before America’s housing bubble – which began in the early days of this decade, supported by a flood of liquidity and lax regulation – would pop.
The longer the bubble expanded, the larger the explosion and the greater (and more global) the resulting downturn would be.ampnbsp; 
Economists are good at identifying underlying forces, but they are not so good at timing.
The dynamics are, however, much as anticipated.
America is still on a downward trajectory for 2009 – with grave consequences for the world as a whole.
For example, as their tax revenues plummet, state and local governments are in the process of cutting back their expenditures.
American exports are about to decline.
Consumer spending is plummeting, as expected.
There has been an enormous decrease in (perceived) wealth, in the trillions, with the decline in house and stock prices.
Besides, most Americans were living beyond their means, using their houses, with their bloated values, as collateral.
That game is up.ampnbsp; 
America would be facing these problems even if it were not simultaneously facing a financial crisis.
America’s economy had been supercharged by excessive leveraging; now comes the painful process of deleveraging.
Excessive leveraging, combined with bad lending and risky derivatives, has caused credit markets to freeze.
After all, when banks don’t know their own balance sheets, they aren’t about to trust others’.ampnbsp; 
The Bush administration didn’t see the problems coming, denied that they were problems when they came, then minimized their significance, and, finally, panicked.
Guided by one of the architects of the problem, Hank Paulson, who had advocated for deregulation and allowing banks to take on even more leveraging, it was no surprise that the administration veered from one policy to another – each strategy supported with absolute conviction, until minutes before it was abandoned for another.
Even if confidence really were all that mattered, the economy would have sunk.
Moreover, what little action has been taken has been aimed at shoring up the financial system.
But the financial crisis is only one of several crises facing the country: the underlying macroeconomic problem has been made worse by the sinking fortunes of the bottom half of the population.
Those who would spend don’t have the money, and those with the money aren’t spending.
America, and the world, is also facing a major structural problem, not unlike that at the beginning of the last century, when productivity increases in agriculture meant that a rapidly declining share of the population could find work there.
Nowadays, increases in manufacturing productivity are even more impressive than they were for agriculture a century ago; but that means the adjustments that must be made are all the greater.
Not long ago, there was discussion of the dangers of a disorderly unwinding of the global economy’s massive imbalances.
What we are seeing today is part of that unwinding.
But there are equally fundamental changes in the global balances of economic power: the hoards of liquid money to help bail out the world lie in Asia and the Middle East, not in West.
But global institutions do not reflect these new realities.
Globalization has meant that we are increasingly interdependent.
One cannot have a deep and long downturn in the world’s largest economy without global ramifications.
I had long argued that the notion of decoupling was a myth; the evidence now corroborates that view.
This is especially so because America has exported not just its recession, but its failed deregulatory philosophy and toxic mortgages, so financial institutions in Europe and elsewhere are also confronting many of the same problems.ampnbsp; 
Many in the developing world have benefited greatly from the last boom, through financial flows, exports, and high commodity prices.
Now, all of that is being reversed.
Indeed, it is the ultimate irony that money is now flowing from poor and well-managed economies to the US, the source of the global problems.
The point of reciting these challenges facing the world is to suggest that, even if Obama and other world leaders do everything right, the US and the global economy are in for a difficult period.
The question is not only how long the recession will last, but what the economy will look like when it emerges.
Will it return to robust growth, or will we have an anemic recovery, à la Japan in the 1990’s?
Right now, I cast my vote for the latter, especially since the huge debt legacy is likely to dampen enthusiasm for the big stimulus that is required.
Without a sufficiently large stimulus (in excess of 2% of GDP), we will have a vicious negative spiral: a weak economy will mean more bankruptcies, which will push stock prices down and interest rates up, undermine consumer confidence, and weaken banks.
Consumption and investment will be cut back further.
Many Wall Street financiers, having received their gobs of cash, are returning to their fiscal religion of low deficits.
It is remarkable how, having proven their incompetence, they are still revered in some quarters.
What matters more than deficits is what we do with money; borrowing to finance high-productivity investments in education, technology, or infrastructure strengthens a nation’s balance sheet.
The financiers, however, will argue for caution: let’s see how the economy does, and if it needs more money, we can give it.
But a firm that is forced into bankruptcy is not un-bankrupted when a course is reversed.
The damage is long-lasting.ampnbsp; 
If Obama follows his instinct, pays attention to Main Street rather than Wall Street, and acts boldly, then there is a prospect that the economy will start to emerge from the downturn by late 2009.
If not, the short-term prospects for America, and the world, are bleak.
The Dissident and the Mahatma
NEW DELHI – With the Nobel Peace Prize presented this month in the absence of this year’s laureate, the imprisoned Chinese dissident Liu Xiaobo, it might be wise to think of a man who never won the prize: Mahatma Gandhi.
Despite that omission, there is no doubting Gandhiji’s worldwide significance – including for Liu.
The Mahatma’s image now features in advertising campaigns for everything from Apple computers to Mont Blanc pens.
When Richard Attenborough's film Gandhi swept the Oscars in 1983, posters for the film proclaimed that “Gandhi’s triumph changed the world forever.” But did it?
The case for Gandhi-led global change rests principally on the American civil-rights leader Martin Luther King, Jr., who attended a lecture on Gandhi, bought a half-dozen books about the Mahatma, and adopted satyagraha as both precept and method.
In leading the struggle to break down segregation in the southern United States, King used non-violence more effectively than anyone else outside India. “Hate begets hate.
Violence begets violence,” he memorably declared. “We must meet the forces of hate with soul force.”
King later avowed that “the Gandhian method of non-violent resistance...became the guiding light of our movement.
Christ furnished the spirit and motivation, and Gandhi furnished the method.” Last month, Barack Obama told India’s Parliament that were it not for Gandhi, he would not be standing there as America’s president.
So, yes, Gandhism helped to change America forever.
But it is difficult to find many other instances of its success.
India’s independence marked the dawn of the era of decolonization, but many nations threw off the yoke of empire only after bloody and violent struggles.
Other peoples have since fallen under the boots of invading armies, been dispossessed of their lands, or terrorized into fleeing their homes.
Non-violence has offered no solutions to them.
Indeed, non-violence could only work against opponents vulnerable to a loss of moral authority – that is, governments responsive to domestic and international public opinion, and thus capable of being shamed into conceding defeat.
In Gandhi’s own time, non-violence could have done nothing to prevent the slaughter of Jews in Hitler’s path.
The power of Gandhian non-violence rests in being able to say, “To show you that you are wrong, I punish myself.” But that has little effect on those who are not interested in whether they are wrong and are already seeking to punish you.
Your willingness to undergo punishment merely expedites their victory.
No wonder Nelson Mandela, who told me that Gandhi had “always been a great source of inspiration,” explicitly disavowed non-violence as ineffective in South Africans’ struggle against apartheid.
Indeed, Gandhi can sound frighteningly unrealistic: “The willing sacrifice of the innocent is the most powerful answer to insolent tyranny that has yet been conceived by God or man.  Disobedience, to be ‘civil,’ must be sincere, respectful, restrained, never defiant, and it must have no ill will or hatred behind it.
Neither should there be excitement in civil disobedience, which is a preparation for mute suffering.”
For many around the world who suffer under oppressive regimes, such a credo would sound like a prescription for sainthood – or for impotence.
Mute suffering is fine as a moral principle, but only Gandhi could use it to bring about meaningful change.
The sad truth is that the staying power of organized violence is almost always greater than that of non-violence.
Gandhi believed in “weaning an opponent from error by patience, sympathy, and self-suffering.” But, while such an approach may have won Burma’s Aung San Suu Kyi the Nobel that eluded the Mahatma himself, the violence of the Burmese state proved far stronger in preventing change than her suffering has in fomenting it.
And, as the Mumbai terror attacks of November 2008 demonstrated, India today faces the threat of cross-border terrorism to which the Mahatma’s only answer – a fast in protest – would have left its perpetrators unmoved.
In his internationalism, the Mahatma expressed ideals that few can reject.
But the decades since his death have confirmed that there is no escaping the conflicting sovereignties of states.
Some 20 million lives have been lost in wars and insurrections since Gandhi died.
In a dismaying number of countries, including his own, India, governments spend more for military purposes than for education and health care combined.
The current stockpile of nuclear weapons represents more than a million times the explosive power of the bomb whose destruction of Hiroshima so grieved him.
Outside India, as within it, Gandhian techniques have been perverted by terrorists and bomb-throwers who declare hunger strikes when punished for their crimes.
Gandhism without moral authority is like Marxism without a proletariat.
Yet too few who have tried his methods worldwide have had his personal integrity or moral stature.
None of these misguided or cynical efforts has diluted Gandhi’s greatness, or the extraordinary resonance of his life and his message.
While the world was disintegrating into fascism, violence, and war, the Mahatma taught the virtues of truth, non-violence, and peace.
He destroyed the credibility of colonialism by opposing principle to force.
And he set and attained personal standards of conviction and courage that few will ever match.
He was that rare kind of leader who was not confined by his followers’ inadequacies.
Yet Gandhi’s truth was essentially his own.
He formulated its unique content and determined its application in a specific historical context.
Inevitably, few in today’s world can measure up to his greatness or aspire to his credo.
The originality of Gandhi’s thought and the example of his life still inspire people around the world today – as Liu Xiaobo would readily admit.
But his triumph did not “change the world forever.”
I wonder if the Mahatma, surveying today’s world, would feel that he had triumphed at all.
The Distance Between the First and Third Rome
MOSCOW/ROME: For a decade, Pope John Paul II has been flying in circles around Mother Russia: one day he visits the Baltics or his homeland of Poland; the next, Orthodox Romania and Georgia.
In June, 2001 Pope John Paul II will visit Ukraine and Armenia, both parts of the former Soviet Union and both still watched over warily by Russia’s Orthodox Church.
Karol Wojtyla, the first Slavic Pope in history, has long dreamed of visiting Moscow; indeed he may see such a visit as putting the finishing touch on his long, turbulent pontificate.
But, a decade after communism’s collapse, it is Russia’s churchmen, not its politicians, who are blocking the way.
From Khrushchev onward, Moscow’s rulers eyed the Vatican suspiciously but not without interest.
Kremlin leaders instinctively understood the benefits of normalising relations with the Holy See for Soviet propaganda and foreign policy, and meetings between the Pope and Andrei Gromyko and Nikolai Podgorny did take place.
Not until 1989, however, did Mikhail Gorbachev dare establish official relations with the Vatican, inviting Pope John Paul II to visit the Soviet Union.
Boris Yeltsin repeated the invitation in 1991, and Vladimir Putin did the same on his visit to Rome soon after his inauguration as Russia’s president.
No papal visit to Moscow has taken place, however, because Russia’s Orthodox Church remains opposed.
The Russian Orthodox attitude is voiced repeatedly by Patriarch Alexy II and his closest aides, including the Patriarchy’s “foreign minister,” Metropolitan Kyrill.
Before the announcement of the Papal visit to Ukraine, the Italian newspaper "Corriera della Sera" quoted Patriarch Alexy II as admitting that a papal visit was possible, but only if “persecution” of Russian Orthodoxy by Catholics in western Ukraine and proselytising by Catholic clergy on the "canonic territory" of the Russian Orthodox church came to an end.
Although seemingly hardline, these two conditions are a step forward for the Patriarch from his notorious statement of a few years ago, when he said that a true Orthodox Christian must not even pick mushrooms in the same forest as a Catholic.
The Soviet Union, the Communist Party of the USSR, and the KGB may have collapsed, but the Russian Orthodox Church still defends the sacred borders of the former Russian empire.
Moreover, because almost half of the parishes overseen by the Moscow Patriarchate are abroad – in Ukraine, Belarus, Moldova, Central Asia and other regions – Patriarch Alexy II lays increasing stress on the global character of his church.
Indeed, the Russian Church’s archbishops presented their Patriarch a golden icon, one side of which depicted the Last Supper and the other a map of his vast canonical territory, to mark the tenth anniversary of his reign.
Clearly, all the bows and plaintive “mea culpas” made by the Pope over the past year to mark the 2000th anniversary of the Birth of Christ, have not satisfied the Patriarch.
So, sadly, the Pontiff’s dream of stepping on Russian soil, a soil lavishly showered with the blood of martyrs, before what the Pope has called this terrible "wolverine" century comes to an end, may never come true.
Opportunities for a meeting between the two church leaders still exist, of course.
For example, in 2001 both the Catholic and Orthodox Easter will fall on the same day – 15 April – an extremely rare occurrence.
What better pretext could there be for an ecumenical meeting?
But will the Patriarch seize the moment?
The wounds between the two churches would heal if left undisturbed.
I will never forget how two old men – the Pope and 90-year old Academician Dmitry Likhachev – leafed through a rare book together with tears in their eyes.
Likhachev was showing the Pope an album about the Solovetsy camp where Likhachev had been imprisoned for “religious propaganda”.
The album contained photographs of the windows of the old monk’s cells to which the Soviets had affixed bars; for years these cells imprisoned Catholic clergymen, such as those arrested after State Prosecutor Nikolay Krylenko proclaimed the Catholic Church “an enemy of the people” in 1917.
No one in authority in Russia, however, has ever repented for crimes committed against the country’s own people.
The hierarchy of the Russian Orthodox Church, indeed, has not admitted the sins of its collaboration with the Soviet state.
Such a “mea culpa” from Russia might also help begin to heal the wounds of the Catholic/Orthodox division.
Rapprochement between Christians is today a moral and political imperative.
The Pope made the first step in a meeting last spring with Patriarch Shenuda III of the Coptic Orthodox Church during the Pope’s visit to the Holy land.
John Paul II hinted, daringly, at the possibility of revising the thousand-year old principle of the Bishop of Rome being first of the apostles and Christ’s vicar on earth.
Since 1054, this idea has been a primary obstacle to Christian unity.
According to the Kremlin press service, President Putin speaks to Patriarch Alexy II often. Perhaps he should explain to him that failure to pursue rapprochement with the Vatican contradicts Russia’s state interests.
For the Kremlin and the Vatican see eye to eye on many problems, including the Middle East. As Russia’s struggles to find a role and a voice in world diplomacy, having the Holy See as an ally would be beneficial.
“The tears of this century laid the ground for a new spring of the human spirit," the Pope declared in his last visit to the UN.
Russia, as everyone knows, has suffered as much as, if not more than, any country.
Sadly, as the title from one of Russia’s most beloved movies of the communist era tells us, it still seems that “Moscow (or at least its Patriarch) does not believe in tears.”
The Disuniting Kingdom?
Three hundred years after the first Scottish Parliament voluntarily voted itself out of existence in 1707, the Scottish National Party has won a plurality in the devolved Scottish parliament that is one of Tony Blair’s great legacies.
Does an SNP-led government herald the break-up of the United Kingdom?
More broadly, does nationalism, that product of nineteenth-century politics, still have a role to play in Europe?
The answer to the first question is almost certainly no.
Nationalist polled only 31.9% of the votes cast with parties supporting the union polling 59.6%.
Proof positive that proportional representation can produce strange outcomes.
Back in 1957, the motive for “laying the foundations of an ever closer union of the peoples of Europe” was to make war between European nations obsolete, and, in doing so, to bring internal stability to all European nations.
For 50 years, the European Union was not much tested by this mission because nationalist impulses were crushed between the two great Cold War alliances.
With those constraints gone, nationalism in both its Bismarckian state-making and ethnic state-breaking guises has gotten a second wind.
When people nowadays speak of nationalism, sinister images from another era come to mind.
But nationalism is, of course, not inevitably violent: it flares into conflict only in places with a flammable legacy.
The break-up of the Soviet Union and its satellite empire shows that the way to address such a legacy is not to force unhappy peoples to live together in one country.
It is to recognize that in some places divorce is inevitable, and to ensure that it is as amicable as possible.
The world could not have prevented Yugoslavia’s spiral into civil war, but it might have made it less cruel by helping to negotiate terms of separation earlier.
Some suggest that the SNP’s accession to power in Scotland does herald the rebirth of a nation, many others regard that claim as rhetoric which ignores the tremendous advances made in the quality of life, opportunity and living standards.
If the claim were true it is a rebirth that is coming about in an extraordinary way.
Except for some odd and hapless individuals, there have been no underground armies, separatist terrorists, campaigns of civil disobedience aimed at unseating governments, or even any mass demonstrations.
The establishment of a Scottish Parliament vested with powers devolved from the United Kingdom Parliament at Westminster has been perhaps the first revolution of the modern era that was conducted by committees of lawyers, clergymen, and accountants rather than cells of bearded radicals.
Moreover, it was achieved without a shot being fired.
So it is not surprising that it has also been a revolution that – unlike that which divided Czechoslovakia 14 years ago – falls well short of achieving full statehood for Scotland.
The Parliament at Westminster, to which Scots continue to elect MP’s, still controls defense and foreign affairs, macroeconomic policy, taxation, and social security.
The Scottish Parliament, however, is able to legislate over health services, education, local government, housing, criminal and civil justice, and economic development.
It can also raise or lower the basic rate of income tax – though by no more than 3% – and levy charges, such as road tolls.
This semi-independence partly reflects the absence of linguistic motives in Scottish nationalism, unlike the Quebecois of Canada or Flemish nationalism in Belgium.
Gaelic is spoken by only about 80,000 of Scotland’s 5.1 million inhabitants.
Nor does religion play a discernible role; while Roman Catholics used to fear independence as being liable to result in Protestant hegemony, today Catholics and Protestants are roughly equal in their support of Scotland’s institutions.
Moreover, unlike East European or Balkan nationalism, the Scottish variety has little to do with ethnicity or religion.
Thus, in Scotland today there are none of the conditions that fomented rebellion in Ireland and led to Irish independence in 1922, the last great rupture in the political union of the British Isles.
Scottish nationalists do look longingly at Ireland, but for its recent phenomenal economic growth.
For most Scots, however, the Irish experience is not an appealing model – perhaps because it is associated with terrorism.
They also know that the huge financial assistance which Ireland received form the European Union will never be repeated
What motivates Scottish nationalism is the strong attachment to Scotland’s civic institutions.
In this respect, Scotland differs from Wales, which was forcibly incorporated into England more than 400 years before the Scots signed the voluntary Act of Union in 1707.
Distinctive Welsh institutions, apart from those concerned with the Welsh language, are difficult to identify.
By contrast, Scotland’s institutional landscape – schools and universities with their own curriculum and exam structures, a legal system with its own codes and rules, a church independent of the state, and a distinctive system of local government – were left untouched by the union.
Today’s Scottish parliament helps to dissipate Scottish discontents because it is revitalizing those institutions.
But it is also propelling British politics into a new and unfamiliar decentralized political system.
Westminster’s writ no longer runs north of the border, at least as far as things like education and health are concerned.
Equally, it is now far harder for Scots to blame a distant government in London for their problems, although the SNP will now try to make blaming Westminster for all ills even more of an art form.
So, far from being a harbinger of the break-up of Britain, devolution has brought fresh vitality to national life outside London.
The new confidence in Edinburgh, which is experiencing an economic boom, is self-evident.
What is arising is a different Scotland, and a different Britain, and this is Scotland’s lesson for Europe.
Britain’s highly centralized political culture has been irreversibly changed.
It is being replaced by a more diverse sort of politics, in which different regional and national identities gain encouragement and expression.
By cooperating with the rest of the United Kingdom, rather than clashing, Scotland is giving new meaning to the phrase “ever closer union.”
The Bilateral Threat to Free Trade
ISTANBUL – The Doha Round of global trade talks appears to have died this year, almost without a whimper.
While a small portion of the project may be saved, the essential reality is that this is a unique failure in the history of multilateral trade negotiations, which have transformed the global economy since World War II.
Many of the seven previous rounds of negotiations – including the Uruguay Round, which resulted in the establishment of the World Trade Organization (WTO) in 1995 as the successor to the General Agreement on Trade and Tariffs (GATT) – took years to complete, but none died of neglect or disinterest.
Today’s indifference is particularly, though not exclusively, evident in the United States.
President Barack Obama was silent on the issue in his re-election campaign, and breathed scarcely a word about it in his first campaign, too.
One wonders whether what is at stake is even fully understood in some capitals.
Successful multilateral trade negotiations have significantly shaped the world in which we live and have dramatically enhanced the lives of millions of people.
Between 1960 and 1990, only one person in five lived in an economically open society; today, nine in ten do.
The rule-based trading system developed by the GATT and the WTO has been embraced by virtually the entire global community.
It has provided an effective road map for the former planned and import-substituting economies, facilitating their integration into the global market.
Initially, “globalization” was a dirty word to some.
But, even among its opponents, its value for poorer countries came to be recognized, as it helped to lift more than a billion people in Asia out of abject poverty.
While much more needs to be done for Africa and parts of Latin America, the Doha Round was intended to assist in providing market access (and therefore opportunity) to many more in the developing world.
The essence of the multilateral system consists in two principles: non-discrimination and national treatment.
The former is described in the trade negotiators’ lexicon as the ���most favored nation” principle, which essentially seeks to ensure that trade benefits provided to one country are provided to all.
The latter requires member states to provide the same treatment to trading partners within national borders as that provided to nationals.
The non-discrimination principle ensured that global trade did not become a “spaghetti bowl” of preferential bilateral trade agreements.
Moreover, a multilateral framework for trade negotiations gave weaker states far more balanced conditions than they would face were they forced to negotiate bilaterally with the likes of China, the United States, or the European Union.
In fact, what we have seen in recent years is an increasing rush to bilateral agreements by the major trading countries and blocs.
This has apparently consumed virtually all of their attention.
The WTO has been marginalized, and even what has already been achieved in the incomplete Doha Round appears unlikely to be delivered in a final agreement in the foreseeable future.
The damage to the credibility of the WTO – once lauded as the greatest advance in global governance since the inspired institution-building of the immediate postwar period – may yet prove lasting.
Worse, it could have a serious impact not merely on trade, but on political relationships more generally.
One of the WTO’s great achievements has been the adjudication system that it provides – the so-called Dispute Settlement Mechanism.
This independent body has been a resounding success, giving the world an effective quasi-judicial system to resolve disputes between trading partners.
But its continued success depends ultimately on the credibility of the WTO itself; it will inevitably suffer collateral damage from a failure of multilateral negotiations.
Indeed, the current rush to bilateral trade agreements has been accompanied by a rise in protectionism.
For example, there have been 424 new measures of this kind in the EU since 2008.
Furthermore, the EU’s non-discriminatory tariffs are fully applicable to only nine trading partners.
Everyone else has “exceptional” treatment.
Next, no doubt, we will have the prospect of a bilateral free-trade agreement between the EU and the US.
An EU-Japan treaty is already in the wind, as is a “Trans-Pacific Partnership” to liberalize trade among the US and major Asian and Latin American economies.
If either ever comes to pass, which I doubt, a huge share of world trade would be conducted within a discriminatory framework.
Some recognize the risks.
In May 2011, Jagdish Bhagwati of Columbia University and I co-chaired a High Level Group convened by the prime ministers of the United Kingdom, Germany, Turkey, and Indonesia to attempt to move the multilateral process ahead.
Our sponsors welcomed our recommendations, but that and similar efforts have gained little traction, leaving all countries rushing headlong toward a world full of uncertainty and risk.
It is not too late to reverse the apparently inexorable tide of bilateralism.
But the only way to do so is by proceeding with WTO negotiations.
Even if the Doha Round cannot be concluded, there may be other routes, such as implementing what has already been agreed.
Another alternative might be to advance multilateral negotiations among willing countries in specific areas, such as services, with other WTO members joining later.
But if we are to move forward rather than revert to earlier, more dangerous times, the US, in particular, must reassert a constructive role in multilateralism.
The US must lead again, as it did in the past.
And now it must do so with China at its side.
The Doha Round’s Premature Obituary
NEW YORK – The Doha Round, the first multilateral trade negotiation conducted under the auspices of the World Trade Organization, is at a critical stage.
Now in their 10th year, with much negotiated, the talks need a final political nudge, lest Doha – and hence the WTO – disappear from the world’s radar screen.
Indeed, the danger is already real: when I was in Geneva a year ago and staying at the upscale Mandarin Oriental, I asked the concierge how far away the WTO was.
He looked at me and asked: “Is the World Trade Organization a travel agency?”
The threat of irrelevance is understood by leading statesmen, who have committed themselves to putting their shoulders to the wheel.
British Prime Minister David Cameron, German Chancellor Angela Merkel, and Indonesian President Susilo Bambang Yudhoyono have unequivocally endorsed the recommendation of the High-level Expert Group on Trade, which Peter Sutherland and I co-chair, that we ought to abandon the Doha Round if it is not concluded by the end of this year.
Our idea was that, just as the prospect of an imminent hanging concentrates the mind, the deadline and prospective death of the Doha Round would galvanize the world’s statesmen behind completing the last mile of the marathon.
(The analogy is all the more appropriate, given that WTO Director-General Pascal Lamy, who has brilliantly kept the process going, is a marathon runner.)
But even as these efforts are gathering momentum, The Financial Times, which used to be a staunch supporter of multilateral free trade, dropped a cluster bomb on Doha, even congratulating itself that, in 2008 (when a ministerial meeting failed to reach closure), it “argued that leaders should admit the negotiations were dead.” But if skeptics forget Mark Twain’s famous response to a mistaken obituary – “The reports of my death are greatly exaggerated” – were the negotiators who have continued to work since then akin to Gogol’s “dead souls?”
The Financial Times recommends working out a Plan B here and now, which would sabotage the political efforts to conclude the Doha Round.
Despite rhetorical bromides about requiring “ministers to unlearn ingrained habits and focus on substance, not rhetoric,” and about “business associations engaging with the granular detail of what companies want,” this proposed Plan B would strengthen the bilateral and regional trade initiatives that have diverted energy and attention from Doha and the WTO.
The irony here is that the proliferation of such preferential trade agreements (PTAs) is usually justified by pointing to the lack of progress in concluding the Doha talks.
Never have cause and effect been so dramatically reversed in arguments over trade policy.
It has become increasingly obvious that such PTAs are what I call “termites in the trading system.”
Indeed, evidence is mounting that they foster harmful trade diversion by increasing discrimination against non-members through differential use of anti-dumping actions.
Thus, recent work by the economists Tom Prusa and Robert Teh has produced convincing evidence that anti-dumping filings decrease by 33-55% within a PTA, whereas such filings increase against non-members by 10-30%.
More importantly, PTAs are used by hegemonic powers to foist on weaker trading partners demands unrelated to trade but desired by domestic lobbies, at times in a markedly asymmetric way.
Thus, Peru saw its labor legislation virtually rewritten by United States Congressmen indebted to American unions before the US-Peru PTA was concluded.
Similarly, Claude Barfield has documented how Colombia has been intimidated into making it a crime (with prison terms of up to five years) to engage in acts that “undermine the right to organize and bargain collectively.” Colombia must also pass a law dictating prison terms for anyone who “offers a collective pact to non-union workers that is superior to terms for union workers.”
Will the US administration start filing criminal charges against the governor of Wisconsin and the many other Republican leaders who are doing precisely what the Colombian government is being bullied into not doing?
Such overreach is typical of what goes on in hegemon-led PTAs, unlike at the WTO, where stronger countries like India (which asked the European Union to take all non-trade-related measures out of the proposed PTA) and Brazil cannot be so intimidated.
The danger is that overreach will lead civil society and voters in democratic developing countries to react against self-serving displays of hegemonic might by jettisoning free trade itself, on the assumption that such openness is little more than neo-colonialism.
The Dollar and the Dragon
CAMBRIDGE – For several years, American officials have pressed China to revalue its currency.
They complain that the undervalued renminbi represents unfair competition, destroying American jobs, and contributing to the United States’ trade deficit.
How, then, should US officials respond?
Just before the recent G-20 meeting in Toronto, China announced a formula that would allow modest renminbi appreciation, but some American Congressmen remain unconvinced, and threaten to increase tariffs on Chinese goods.
America absorbs Chinese imports, pays China in dollars, and China holds dollars, amassing $2.5 trillion in foreign-exchange reserves, much of it held in US Treasury securities.
To some observers, this represents a fundamental shift in the global balance of power, because China could bring the US to its knees by threatening to sell its dollars.
But, if China were to bring the US to it knees, it might bring itself to its ankles in the process.
China would not only reduce the value of its reserves as the dollar’s value fell, but it would also jeopardize America’s continued willingness to import cheap Chinese goods, which would mean job losses and instability in China.
Judging whether economic interdependence produces power requires looking at the balance of asymmetries, not just at one side of the equation.
In this case, interdependence has created a “balance of financial terror” analogous to the Cold War, when the US and the Soviet Union never used their potential to destroy each other in a nuclear exchange.
In February 2010, angered over American arms sales to Taiwan, a group of senior military officers called for the Chinese government to sell off US government bonds in retaliation.
Their proposal went unheeded.
Instead, Yi Gang, China’s director of state administration of foreign exchange, explained that “Chinese investments in US Treasuries are market investment behavior, and we don’t wish to politicize them.” Otherwise, the pain would be mutual.
Nevertheless, this balance does not guarantee stability.
There is always the danger of actions with unintended consequences, especially as both countries can be expected to maneuver to change the framework and reduce their vulnerabilities.
For example, after the 2008 financial crisis, while the US pressed China to let its currency appreciate, officials at China’s central bank began arguing that America needed to increase its savings, reduce its deficits, and move toward supplementing the dollar’s role as a reserve currency with IMF-issued special drawing rights.
But China’s bark was louder than its bite.
China’s increased financial power may have increased its ability to resist American entreaties, but despite dire predictions, its creditor role has not been sufficient to compel the US to change its policies.
While China has taken minor measures to slow the increase in its dollar-denominated holdings, it has been unwilling to risk a fully convertible currency for domestic political reasons.
Thus, the renminbi is unlikely to challenge the dollar’s role as the largest component of world reserves (more than 60%) in the next decade.
Yet, as China gradually increases domestic consumption rather than relying on exports as its engine of economic growth, its leaders may begin to feel less dependent than they now are on access to the US market as a source of job creation, which is crucial for internal political stability.
In that case, maintaining a weak renminbi would protect the trade balance from a flood of imports.
Asymmetries in currency markets are a particularly important aspect of economic power, since they underlie global trade and financial markets.
By limiting the convertibility of its currency, China is avoiding currency markets’ ability to discipline domestic economic decisions.
Compare, for example, the discipline that international banks and the IMF were able to impose on Indonesia and South Korea in 1998, with the relative freedom of the US – bestowed by denomination of American debt in dollars – to increase government spending in response to the 2008 financial crisis.
Indeed, rather than weakening, the dollar has appreciated as investors regard the underlying strength of the US as a safe haven.
Obviously, a country whose currency represents a significant proportion of world reserves can gain international power from that position, thanks to easier terms for economic adjustment and the ability to influence other countries.
As French President Charles de Gaulle once complained, “since the dollar is the reference currency everywhere, it can cause others to suffer the effects of its poor management.
This is not acceptable. This cannot last.”
But it did.
America’s military and economic strength reinforces confidence in the dollar as a safe haven.
As a Canadian analyst put it, “the combined effect of an advanced capital market and a strong military machine to defend that market, and other safety measures, such as a strong tradition of property rights protection and a reputation for honoring dues, has made it possible to attract capital with great ease.”
The G-20 is focusing on the need to “re-balance” financial flows, altering the old pattern of US deficits matching Chinese surpluses.
This would require politically difficult shifts in consumption and investment, with America increasing its savings and China increasing domestic consumption.
Such changes do not occur quickly.
Neither side is in a hurry to break the symmetry of interdependent vulnerability, but both continue to jockey to shape the structure and institutional framework of their market relationship.
For the sake of the global economy, let us hope that neither side miscalculates.
The Dollar Hits an Oil Slick
CAMBRIDGE – The rapid rise in the price of oil and the sharp depreciation of the dollar are two of the most noteworthy developments of the past year.
The price of oil has increased by 85% over the past 12 months, from $65 a barrel to $120.
During the same period, the dollar fell by 15% relative to the euro and 12% against the yen.
To many observers, the combination of a falling dollar and a rise in oil prices appears to be more than a coincidence.
But what is the link between the two?
Would the price of oil have increased less if oil were priced in euros instead of dollars?
Did the dollar’s fall cause the price of oil to rise?
And how did the rise in the price of oil affect the dollar’s movement?
Because the oil market is global, with its price in different places virtually identical, the price reflects both total world demand for oil and total supply by all of the oil-producing countries.
The primary demand for oil is as a transport fuel, with lesser amounts used for heating, energy, and as inputs for petrochemical industries like plastics.
The increasing demand for oil from all countries, but particularly from rapidly growing emerging-market countries like China and India, has therefore been, and will continue to be, an important force pushing up the global price. 
The thinking behind the question of whether oil would cost less today if it were priced in euros seems to be that, since the dollar has fallen relative to the euro, this would have contained the rise in the price of oil.
In reality, the currency in which oil is priced would have no significant or sustained effect on the price of oil when translated into dollars, euros, yen, or any other currency. 
Here is why. The market is now in equilibrium with the price of oil at $120.
That translates into 75 euros at the current exchange rate of around $1.60 per euro.
If it were agreed that oil would instead be priced in euros, the quoted market-equilibrating price would still be 75 euros and therefore $120.
Any lower price in euros would cause excess global demand for oil, while a price above 75 euros would not create enough demand to absorb all of the oil that producers wanted to sell at that price.
Of course, the rate of increase of the price of oil in euros during the past year was lower than the rate of increase in dollars.
The euro price of oil in May 2007 was 48 euros, or 56% below its current price.
But that would be true even if oil had been priced in euros.
The coincidence of the dollar decline and the rise in the oil price suggests to many observers that the dollar’s decline caused the rise in the price of oil.
That is only true to the extent that we think about the price of oil in dollars, since the dollar has fallen relative to other major currencies.
But if the dollar-euro exchange rate had remained at the same level that it was last May, the dollar price of oil would have increased less.
The key point here is that the euro price of oil would be the same as it is today.
And the dollar price of oil would have gone up 56%.
The only effect of the dollar’s decline is to change the price in dollars relative to the price in euros and other currencies.
The high and rising price of oil does, however, contribute to the decline of the dollar, because the increasing cost of oil imports widens the United States’ trade deficit.
In 2007, the US spent $331 billion on oil imports, which was 47% of the US trade deficit of $708 billion dollars.
If the price of oil had remained at $65 a barrel, the cost of the same volume of imports would have been only $179 billion, and the trade deficit would have been one-fifth lower.
The dollar is declining because only a more competitive dollar can shrink the US trade deficit to a sustainable level.
Thus, as rising global demand pushes oil prices higher in the years ahead, it will become more difficult to shrink America’s trade deficit, inducing more rapid dollar depreciation.
The Dollar’s Last Days?
CAMBRIDGE – Zhou Xiaochuan, the governor of the People’s Bank of China, recently suggested that replacing the dollar with the International Monetary Fund’s Special Drawing Rights as the dominant reserve currency would bring greater stability to the global financial system.
The idea of reforming the system by introducing a supranational reserve currency is also, it appears, supported by Russia and other emerging markets.
And a United Nations advisory committee chaired by the Nobel laureate Joseph Stiglitz has argued for a new global reserve currency, possibly one based on the SDR.
Transforming the dollar standard into an SDR-based system would be a major break with a policy that has lasted more than 60 years.
The SDR was introduced 40 years ago to supplement what was then seen as an inadequate level of global reserves, and was subsequently enshrined in the IMF’s amended Articles of Agreement as the future principal reserve asset.
But the world soon became awash in dollars.
So, instead of becoming the principal reserve asset of the global system, the proportion of SDRs in global reserves shrank to a tiny fraction, rendering the SDR the monetary equivalent of Esperanto.
Although the euro, created in 1999, turned out to be a more serious competitor to the dollar, its share in total international reserves has probably remained below 30%, compared to 65% for the dollar (these shares are in part estimates, as China, the world’s largest holder of reserves, does not report the currency composition of its holdings).
There are two ways in which the dollar’s role in the international monetary system can be reduced.
One possibility is a gradual, market-determined erosion of the dollar as a reserve currency in favor of the euro.
But, while the euro’s international role – especially its use in financial markets – has increased since its inception, it is hard to envisage it overtaking the dollar as the dominant reserve currency in the foreseeable future.
Such an outcome is probably only possible if two conditions are met: first, the United Kingdom joins the euro area, and, second, the United States makes serious, confidence-sapping mistakes.
The latter condition may already have been partially met, but US policies to stabilize its financial system should help avoid a major dollar slide.
Moreover, the European Central Bank has repeatedly stated that it neither encourages nor discourages the euro’s international role.
With the dollar’s hegemony unlikely to be seriously undermined by market forces, at least in the short and medium-term, the only way to bring about a major reduction in its role as a reserve currency is by international agreement.
The Chinese proposal falls into this category.
One way to make the SDR the major reserve currency relatively soon would be to create and allocate a massive amount of new SDRs to the IMF’s members.
While the G-20 leaders have decided to support an SDR allocation of $250 billion, this will increase the share of SDRs in total international reserves to no more than 4%.
In order to make the SDR the principal reserve asset via the allocation route, close to $3 trillion in SDRs would need to be created, an unrealistic proposition.
But there is a more realistic way for the SDR’s importance to grow.
Back in 1980, the IMF came close to adopting a so-called SDR Substitution Account.
The idea was to permit countries whose official dollar holdings were larger than they were comfortable with to convert dollars into SDRs.
Conversion would occur outside the market, and thus would not put downward pressure on the dollar.
Member countries would receive an asset that was more stable than the dollar, as it was based on a basket of currencies, thereby providing better protection against losses.
The plan fell apart when some major IMF shareholders could not accept the burden-sharing arrangements that would be necessary in case of losses due to exchange-rate movements.
The US also lost interest in the scheme as the dollar strengthened.
What are the chances of adopting a scheme of this kind today?
Is the US prepared to go along with a reform of the international monetary system that reduces the dollar’s role?
Until recently, I would have considered this unlikely.
But the changed international climate, and the possibility of a bout of severe dollar weakness, could convince the US to go along with a conversion scheme that would alleviate excessive pressure on the dollar.
And, apart from possible political considerations, large holders of dollars would find a substitution account attractive as a form of protection against strong fluctuations in the dollar’s value.
What about possible losses suffered by the Substitution Account? This can be dealt with by setting aside part of the IMF’s large gold stock.
Even if an SDR Substitution Account is established, it is unlikely that the dollar’s share in international reserves would fall to an insignificant level.
The dollar will remain important for many countries as a vehicle for intervention in foreign-exchange markets, as well as for invoicing and for denominating internationally traded securities.
But one can envisage a system in which international reserves are held each in roughly equal shares of dollars, euros (assuming a further gradual increase in its share), and SDRs.
While there are currently other priorities, it would be useful for the IMF to study anew an SDR substitution account and similar schemes.
If it does not, the debate will take place elsewhere.
The Dollar’s Long Tail
NEW DELHI – The ongoing economic crisis and the persistent deficits of the United States have increasingly called into question the dollar’s role as the world’s anchor currency.
Recent moves to internationalize China’s renminbi have led to anticipation of a looming shift in the global monetary system.
Many prominent economists, including the members of a United Nations panel headed by the Nobel laureate economist Joseph Stiglitz, are recommending a “Global Reserve System” to replace the dollar’s hegemony.
But the long history of global anchor currencies suggests that the dollar’s days on top may be far from over.
In ancient times, India ran a large trade surplus with the Roman empire.
As Pliny wrote in the first century: “Not a year passed in which India did not take 50 million sesterces away from Rome.”&nbsp; That trade imbalance implied a continuous drain on gold and silver coin, causing shortages of these metals in Rome.
In modern terms, the Romans faced a monetary squeeze.
Rome responded by reducing the gold/silver content (the ancient equivalent of monetization), which led to sustained inflation in the empire.
But the frequent discovery of Roman coins in India suggests that Roman coinage continued to be accepted internationally long after it must have been obvious that its gold or silver content had fallen.
In the sixteenth century, Spain emerged as a superpower following its conquest of South America.
Between 1501 and 1600, 17 million kilograms of pure silver and 181,000 kilograms of pure gold flowed from the Americas to Spain, which spent the money on wars in the Netherlands and elsewhere.
This increase in liquidity caused both an economic boom and inflation across Europe.
Despite this wealth, Spain became increasingly indebted, eventually defaulting three times – in 1607, 1627, and 1649 –&nbsp;and heading into sharp geopolitical decline.
Yet Spanish silver coins (known as “pieces of eight” or Spanish dollars) remained the main currency used in world trade right up to the American Revolutionary War.
In fact, Spanish coin remained legal tender in the US until 1857 – long after Spain itself had ceased to be a major power.
By the middle of the nineteenth century, the world was functioning on a bi-metal system based on gold and silver.
But, following the British example, most major countries had shifted to a gold standard by the 1870’s.
The Bank of England stood ready to convert a pound sterling into an ounce of (11/12 fine) gold on demand. That system was disrupted by World War I, but Britain went back to a gold peg in 1925.
As the Great Depression took root, however, the Bank of England was forced to choose between providing liquidity to banks and honoring the gold peg.
It opted for the former in 1931.
Yet pound sterling continued to be a major world currency until well after World War II.
Even as late as 1950 – more than a half-century after the US had replaced Britain as the world’s largest industrial power – 55% of foreign-exchange reserves were held in sterling, and many countries continued to peg their currencies to it.
Three things should be clear from this history.
First, a global monetary system based on precious metals does not resolve the fundamental imbalances of a global economic system.
Second, precious metals do not resolve the problem of inflation.
And, finally, the anchor currency and the underlying eco-system of world trade often outlive the geopolitical decline of the anchor country by decades.
A new economic order was established after World War II, with the US as the anchor country.
The Bretton Woods system linked the US dollar to gold at $35 per ounce, with other currencies linked to the dollar (though occasionally allowed to make adjustments).
The flaw in the system was that it underpinned global economic expansion for only as long as the US was willing to provide dollars by running up deficits – the same deficits that would eventually undermine America’s ability to maintain the $35/ounce gold price.
In 1961, the US responded by creating the London Gold Pool, which obliged other countries to reimburse the US for half of its gold losses.
But this arrangement quickly bred discontent, with France leaving the Gold Pool in 1967.
The Bretton Woods system collapsed four years later.
Or did it?
Despite the problems of the 1970’s, the dollar remained the world’s dominant currency, with successive generations of Asian countries pegging their exchange rates to it.
As with the Bretton Woods system, a peripheral economy (for example, China) could grow very rapidly even as the anchor economy (the US) enjoyed cheap financing.
China’s relative rise did not diminish the dollar’s role – and may even have enhanced it.
Indeed, like the Japanese during their period of rapid growth, the Chinese have, until recently, resisted internationalization of the renminbi.
So, are we entering a post-dollar world?
Despite all of the pain caused by the Great Recession, there is no sign that the world is forsaking the greenback.
Investors remain willing to finance the US at rock-bottom interest rates, and the nominal trade-weighted index of the dollar has not collapsed.
Even if China replaces the US as the world’s largest economy within a decade, an anchor currency can be more resilient than the economic and geopolitical dominance of its country of origin.
That is why the dollar will most likely remain the dominant global currency long after the US has been surpassed.
The Dollar Wars Return
Faced by uncertain re-election prospects, and worried about job losses, US President George W. Bush has begun to blame other countries, sending his Treasury Secretary to demand that they raise their exchange rates in order to make foreign goods more expensive for American consumers.
That was John Snow's mission on his recent trip to China, but his goal was nothing new.
Two of Snow's predecessors, John Connally and James Baker, followed a similar quest for politically desirable exchange rates.
What Snow asked of China is an odd demand: we do not normally go into stores and ask the shopkeeper to raise his prices.
But the emergence of powerful new currencies always provokes fresh attempts at the use of exchange rates for political purposes.
For twenty years after WWII, there were just two major international currencies, as there had been through the interwar period: the British pound and the American dollar.
By the end of the 1960's, the pound had been so weakened that international markets lost interest.
Instead, two new major currencies emerged, as the strength of the Japanese and (West) German economies produced big trade surpluses.
American producers and the US government complained that American workers were being priced out of jobs because the Yen and D-mark were being held at artificially low levels.
The US government under President Richard Nixon pressed the Germans and Japanese to revalue their currencies.
The Germans did but the Japanese did not, infuriating the US Treasury Secretary, who tried his best to bully Japan.
John Connally complained that Japan had a "controlled economy" and did not play by the rules.
All sides engaged in mudslinging about who was doing the most damage to the world economy.
The Americans accused the Europeans and especially the Japanese of growing too slowly, while the Europeans and the Japanese argued that the US was exporting inflation to the rest of the world and abusing the international monetary system in order to sustain its military adventurism (at that time in Vietnam).
Indeed, US monetary expansion had itself become a weapon in the war over exchange rates.
High inflation in the US highlighted the exchange-rate problem by showing the instability of the dollar standard.
Indeed, Connally called the dollar "our currency but your problem."
In the end, the US forced the Japanese to revalue their currency by destroying the international monetary order in August 1971.
The Americans first unilaterally suspended the gold convertibility of the dollar--the backbone of the fixed but adjustable exchange-rate regime created by the post-WWII Bretton Woods conference--and then teamed up with the Europeans to force on Japan a 16.9% revaluation of the Yen against the dollar.
(Japan's Finance Minister had threatened to commit hara-kiri if the revaluation rate was set at 17%.)
But the end of the fixed exchange-rate regime for industrial countries did not end activist management of currencies.
The scenario of the early 1970's was replayed with floating rates in the mid-1980's, with the meetings of the major countries' finance ministers first forcing down the Yen and the D-mark against the dollar, and then trying in 1987 to hold rates stable.
Both of these historical episodes ended in chaos.
The breakdown of Bretton Woods in 1971, far from stopping inflation, unleashed a synchronized monetary surge.
A decade later, efforts to keep the Yen in line with American views produced the Japanese bubble economy, for which many Japanese blamed America after it burst.
Today, the US dollar is also looking at a challenge from two new or newly important currencies, China's Renmimbi and the Euro.
As in the 1970's and the 1980's, the rising Asian currency is seen as a much greater threat because that is where the biggest trade imbalances lie.
Moreover, the US still has a powerful weapon because other countries hold so many dollars: the current estimate is that 46% of US Treasury bonds are held overseas.
This invites both sides to play a game of brinkmanship.
America's leverage consists in the rest of the world's massive financial stake in the fate of the dollar.
But the rest of the world can threaten a dollar sell-off.
China's government today might find some useful lessons in the apparently remote dramas of the 1970's.
The most immediate and most obvious is that it is difficult to hold out long against the US, especially when the American government is under pressure from major domestic interest groups whose strong lobbying power might be used to lead a push for new trade protectionism.
But the world--and the US in particular--might also learn some lessons.
Previous episodes in the politically driven creation of new exchange-rate systems have been highly chaotic, and resulted in increased levels of international tension.
At an emotional level, debating exchange rates means blaming foreigners for whatever goes wrong in the domestic economy.
This approach is both unhealthy and immature, yet not necessarily very damaging.
What is worse is when the economic policy response becomes the equivalent of blaming foreigners: imposing new forms of trade barriers.
Doing so may feel good in the short term, but it merely destroys wealth and undermines job creation everywhere.
The Domestic Logic of Iran’s Foreign Plots
WASHINGTON, DC – Though Saudi Ambassador Adel al-Jubeir is alive and well in Washington, the plot to assassinate him may have succeeded – if its aim was not to kill al-Jubeir, but rather Iranian President Mahmoud Ahmadinejad’s foreign policy.
The history of the Islamic Republic is filled with cases of factions exploiting foreign policy to gain power against their domestic rivals.
It is common for competing groups to sacrifice national interests – such as Iran’s international credibility – to achieve their own goals.
During the Iran-Iraq war, Ronald Reagan’s national security adviser, Robert McFarlane, paid a clandestine visit to Iran with the approval of the country’s highest authorities, to pursue a deal that would have been to Iran’s advantage.
But anti-American elements in the government leaked the news to an Arab newspaper, killing the deal and landing both the Reagan administration and the Iranian government in huge trouble.
Mir Hossein Mousavi, the former prime minister who now leads the opposition, spelled out the problem more than 20 years ago.
On September 5, 1988, he resigned in protest against then-president Ali Khamenei’s interference in his duties.
In his resignation letter, Mousavi complained that the “government’s authority on foreign policy was taken away.”
He charged that
“[military and intelligence] operations outside the country are taking place without the government’s knowledge or orders....Only after an airplane is hijacked are we made aware of it.
Only after a machine gun opens fire in one of Lebanon’s streets and its noise echoes everywhere do we find out.
Only after [Saudi police] find explosive material in Iranian pilgrims’ baggage am I informed.”
Iranian intelligence has assassinated more than 400 Iranian dissidents outside of Iran, including the last prime minister under the Shah, Shapour Bakhtiar, and four Kurds in Berlin’s Mikonos restaurant in 1992 (a German judge later officially named Iran’s Supreme Leader as one of the senior officials involved in the terrorist attack).
Similarly, the Quds Force of Iran’s Revolutionary Guard has carried out hundreds of operations in Iraq, Afghanistan, Lebanon, and elsewhere.
By contrast, the plot targeting al-Jubeir seems to have been highly unprofessional, almost unconcerned with success.
So unlikely are the details that only a power struggle within Iran could justify it.
If so, the plot’s target likely was not al-Jubeir himself, but rather those elements in the regime that seek a diplomatic opening to the US –&#160;namely, Ahmadinejad and his circle.
When news about the assassination plot first appeared, the Supreme Leader, Ali Khamenei, was on his way to the western province of Kermanshah.
In his first speech after the story broke, he made no reference to it, most likely expecting the president to make a statement.
After two days of Ahmadinejad’s meaningful silence, Khamenei felt forced to reject the US allegations publicly.
Only after five days did Ahmadinejad issue a statement denying the accusations – and not as strongly as did Khamenei.
It is ironic to see Iran’s radical Islamist president portray himself as open to resuming US-Iran relations and suffer for it at the hands of a supposedly pragmatic Supreme Leader.
But Ahmadinejad is no exception to the historical rule in Iran: every faction that loses out to its domestic rivals looks beyond the country for an alliance with outside powers.
In the Islamic Republic, those interested in opening up to the West are typically those with little authority.
Ahmadinejad and his faction face a two-edged problem: Khamenei has boxed them in politically, while rampant economic corruption and failed efforts at reform have disillusioned many of the president’s supporters.
As a result, Ahmadinejad, in a further ironic twist, is seeking to glorify Iran’s pre-Islamic history in order to stir nationalist sentiment in his favor and accomplish what he is sure that Khamenei does not want: greater openness to the West, particularly to the US.
He has repeatedly expressed his willingness to meet with US President Barack Obama – even sending him a congratulatory letter on his inauguration – and has traveled to New York to be interviewed by the American media much more often than his predecessors.
Ahmadinejad is well aware that if he succeeds in portraying himself as someone who – in opposition to the Supreme Leader’s will – wants to solve major issues between Iran and the US, that could partly compensate for his economic failures and gain him some new supporters.
For that plan to work, Ahmadinejad needs to survive through the next parliamentary election in March 2012 and the presidential election in June 2013, despite Khamenei’s determination to marginalize him.
Regardless of whether Khamenei was aware of the plot against al-Jubeir, its purpose was to delegitimize Ahmadinejad’s foreign-policy agenda.
So, if Khamenei really was unaware of the plot, it means that some elements in the government or the Revolutionary Guards are acting on their own, which is deeply troubling in its own right.
If there are elements powerful enough to plan such a plot –&#160;even an unsuccessful one – they could do it again in the future.
That raises an even more troubling question: If such “independent” decision-makers exist, are they able to affect Iran’s nuclear policy, too?&#160;
The Domestic Wars of Hosni Mubarak
The decision by Egyptian President Hosni Mubarak’s government to try two senior judges for blowing the whistle on vote rigging in last autumn’s parliamentary elections has rocked the country.
Massive crowds have gathered to support the judges – and have caught Mubarak’s regime completely unaware.
Mubarak’s government now seems to be backtracking as fast as it can.
Judge Mahmoud Mekki has been acquitted, and Judge Hisham al-Bastawisy, who suffered a heart attack the night before, has merely been reprimanded.
Yet Cairo remains restless, and the government fears another outpouring of support for democracy, as the judges have called for renewed nationwide demonstrations.
Egyptian judges have a long-standing tradition of discretion and propriety.
But they feel abused by government efforts to sugarcoat the manipulation of election after election by claiming that judges supervise the voting.
What makes their struggle loom so large for a normally quiescent Egyptian public is partly that nearly all 9,000 judges are standing fast in solidarity.
Their representative body, the Judges’ Club, has long pushed for a new law to restore judicial independence.
Now the judges are insisting on their independence by themselves.
The Mubarak regime is adamantly opposed, and resorts to extra-judicial means, such as emergency courts and national security and military courts, which do not observe international standards.
Contrary to his campaign promises during his run for a fifth term as President, Mubarak has requested (and his rubber-stamp parliament has granted) a two-year extension of the Emergency Law by which Egypt has been ruled throughout his presidency.
It is to this law, above all, that the judges and Egypt’s civil society object.
The Emergency Law has been in force since the assassination of President Anwar Sadat in October 1981, and Mubarak claims that he needs another extension to combat terrorism.
But, according to a recent human rights report, despite the Emergency Law, 89 people were killed and 236 wounded in terrorist attacks in Egypt during the previous 12 months.
In neighboring Israel, which is still in a struggle with the Palestinians, only 18 were killed and 25 wounded in similar attacks during the same period.
Yet Israelis do not live under an emergency law.
Consider, moreover, that at the height of the Arab-Israeli conflict in 1973, Egypt’s armed forces stood at one million troops.
Now only 350,000 serve in the military, while the internal security police recently hit the one-million mark.
Mubarak’s first internal war was with Islamic militants during his early years in power, but he now finds himself caught in three more domestic wars.
The battle with the judges has incited enough popular unrest to warrant Mubarak’s deployment of thousands of black-uniformed central security forces in the heart of Cairo.
This deployment, lasting three weeks so far, is already longer than the combined duration of the last two wars with Israel.
Another domestic war, with the Egyptian Bedouins of Sinai, broke out two years ago.
Taking their cue from their Palestinian neighbors, if not from al-Qaeda, alienated young Bedouins apparently decided to rebel against their treatment as third-class citizens.
All around them, but especially in the ebullient resorts of southern Sinai, billions are spent on roads, airports, and beaches; sizeable parcels of land are allocated generously to rich Egyptians from the Nile Valley and to foreigners, but not to Sinai natives.
Indeed, Sinai Bedouins have the right of use but not ownership of land, because a lethargic, occasionally corrupt bureaucracy still deems the Sinai a military zone and its natives’ loyalty questionable.
Two years ago, on the anniversary of the war of October 1973, young Sinai militants bombed the Taba Hilton.
Last July, on another national holiday, they hit three tourist spots not far from the Mubarak family compound in Sharm el-Sheikh.
These symbolic as well as lethal warnings to a family that has grown Pharaonic in scale, style, and power have gone unheeded.
The third recent domestic war, this one over Christian Coptic citizenship rights, has been brewing for years.
Copts are the original Egyptians, and they were the majority population until the tenth century.
As Egypt was Arabized and Islamized, the Copts became a minority in their original homeland.
In Mubarak’s Egypt, citizens’ legal equality, while stipulated in the constitution, is not respected or observed, especially with regard to the construction and protection of Coptic churches.
Last November, when Muslim zealots attacked a Coptic church in Alexandria, several Copts were injured.
Six months later, a fanatic targeted three churches during Sunday services, killing a few worshippers and injuring many.
Copts marched in the streets of Alexandria for the next three days, protesting the security authorities’ leniency toward the culprits, the scapegoating of their community, or even an official hand in the attacks to justify an extension of the Emergency Law.
Hosni Mubaraks’ four domestic wars are fuelled by Egypt’s excluded, who are increasingly in rebellion against a regime that has long outlived its legitimate mandate.
The battle with the judges may well prove to be Mubarak’s Achilles’ heel.
Justice is a central value for Egyptians, and its absence is at the core of all protests.
There can be no evidence more compelling than the unprecedented numbers of people who have rallied peacefully in solidarity with the judges.
America’s Labor Market by the Numbers
NEWPORT BEACH – Politicians and economists now join investors in a ritual that typically takes place on the first Friday of each month and has important consequences for global markets: anticipating, internalizing, and reacting to the monthly employment report released by the United States Bureau of Labor Statistics (BLS).
Over the last few years, the report has evolved in a significant way – not only providing an assessment of the economy’s past and current state, but, increasingly, containing insights into its future as well.
Think of the BLS’s employment report as a comprehensive monthly check-up for the American labor market.
Among its many interesting statistics, it tells you how many jobs are created and where; how earnings and hours worked are evolving; and the number, age, and education of those seeking employment.
Despite the data’s richness, only two indicators consistently attract widespread attention: net monthly job creation (which amounted to 169,000 in August) and the unemployment rate (7.3% in August, the lowest since December 2008).
Together they point to a gradual and steady improvement in overall labor-market conditions.
This is certainly good news.
It is not long ago that job creation was negative and the unemployment rate stood at 10%.
The problem is that the headline numbers shed only partial light on what may lie ahead.
The figure for monthly job creation, for example, is distorted by the growing importance of part-time employment, and it fails to convey the reality of stagnant earnings.
Meanwhile, the headline unemployment rate does not reflect the growing number of Americans who have left the work force – a phenomenon vividly reflected by the decline in the labor participation rate to just 63.2%, a 35-year low.
To get a real sense of the labor market’s health, we need to look elsewhere in the BLS’s report.
What these other numbers have to tell us – about both the present and the future – is far from reassuring.
Consider the statistics on the duration of unemployment.
After all, the longer one is unemployed, the harder it is to find a full-time job at a decent wage.
In August, the BLS classified 4.3 million Americans as long-term unemployed, or 37.9% of the total unemployed – a worrisome figure, given that the global financial crisis was five years ago.
And, remember, this number excludes all the discouraged Americans who are no longer looking for a job.
In fact, the more comprehensive employment/population ratio stands at only 58.6%.
The teenage-unemployment rate is another under-appreciated indicator that is at an alarming level.
At 22.7%, too many American teenagers, lacking steady work experience early in their professional careers, risk going from unemployed to unemployable.
Then there are the indicators that link educational attainment and employment status. Most notable here is the growing gap between those with a college degree (where the unemployment rate is only 3.5%) and those lacking a high school diploma (11.3%).
Rather than confirming the paradigm of gradual and steady improvement, these disaggregated numbers attest to a highly segmented, multi-speed labor market – one with features that could become more deeply embedded in the structure of the economy.
If current trends persist, the BLS’s report will continue to evolve from a snapshot of the past and present to a preview of the future.
Undoubtedly, the US labor market’s uneven recovery has much to do with the structural and policy gaps exposed by the 2008 global financial crisis and the recession that followed.
The economy is still struggling to provide a sufficient number of jobs for those who were previously employed in leverage-driven activities that are no longer sustainable (let alone desirable).
Moreover, US schools, particularly at the primary and secondary levels, continue to slip down the global scale, constraining Americans’ ability to benefit from globalization.
Meanwhile, existing and newly created jobs provide less of an earnings upside.
And political polarization narrows the scope for effective tactical and structural policy responses.
This combination of factors is particularly burdensome for the most vulnerable segments of the US population – particularly those with limited educational attainment, first-time labor-market entrants, and those who have been out of work for an extended period.
So while net job creation will continue and the unemployment rate will maintain its downward trajectory – both highly welcome – the labor market’s evolution risks fueling rather than countering already-significant income and wealth inequalities, as well as poverty.
Overburdened social support mechanisms would thus come under even greater pressure.
And all of this would amplify rather than attenuate political polarization, placing other urgent policy priorities at even greater risk.
If this interpretation is correct, the heightened attention given to the monthly BLS headline indicators needs to be accompanied by a broader analysis and a different mindset.
After all, the report is much more than a scorecard on America’s performance in confronting a persistent economic, political, and social challenge; it is also an urgent call for a more focused corrective effort involving both government and business.
A better mix of fiscal and monetary policies and sustained measures to enhance productivity and competitiveness remain necessary conditions for addressing America’s labor-market challenges.
But they are not sufficient.
Both the public and private sectors – individually and through scalable and durable partnerships – need to think much more seriously about labor retraining and retooling programs, enhanced labor mobility, vocational training, and internships.
President Barack Obama’s appointment of a “jobs czar” would also help to enhance the credibility, accountability, and coordination required to overcome today’s significant and rising employment challenges.
Yes, the headline numbers will continue to signal overall improvement in the labor market.
The urgent task now is to ensure that lasting progress is not undermined by the worrisome compositional trends that the BLS’s report highlights month after month.
The Dragon and the Bear
Second honeymoons rarely, if ever, recapture the zest of lost love.
Yet ever since the Soviet Union’s collapse in 1991, Russia and China have sought to rekindle the close relations that once supposedly existed between the USSR and Mao’s China before Khrushchev’s denunciation of Stalin in 1956.
But that renewed Sino-Russian marriage always smacked more of convenience – aimed as it was at checking American hegemony – than of true romance.
Now Russia’s invasion of Georgia has shattered even the illusion of attraction.
In 1969, the Chinese and Soviet armies exchanged fire across their disputed border.
Recently, the two countries signed an agreement that seemed to put an end to their long border dispute.
The agreement was a sort of follow-up to the visit to Beijing of Dmitry Medvedev, who made China one of his first official trips abroad after being elected Russia’s president.
During Vladimir Putin’s presidency, Chinese and Russian troops engaged in joint military maneuvers, and the two countries became dominant powers in the Shanghai Cooperation Organization (SCO), which, to some Western observers, looked like an effort to counterbalance NATO.
There were also years of “Russia in China” and “China in Russia” cultural exchanges, meant to underscore that the two countries were tied together not just by geopolitical pragmatism, but by genuine cultural/historical ties as well.
But the fact is that 17 years of high-level bilateral cooperation have produced little of substance.
Indeed, in the wake of the invasion of Georgia, China may be seriously rethinking its relations with Russia.
It may not yet be ready to embark on a full-fledged policy of “containment,” but in the wake of the dismemberment of Georgia – and with Russia claiming a zone of “privileged influence” throughout the former Soviet world – China clearly views Russia as an emerging strategic threat.
For example, China has refused to endorse Russia’s recognition of the independence of South Ossetia and Abkhazia, and has encouraged the SCO’s other members to do the same.
The reasons are not hard to find.
As a general principle of foreign policy, China believes that national borders are sacrosanct.
No power, not even the United Nations, should be allowed to change them without the consent of the country concerned.
More importantly, China views the break-up of the USSR as one of the greatest strategic gifts in its history.
Instead of confronting a (usually hostile) Russian/Soviet empire on its border, a vast swath of buffer states appeared after 1991.
Their continued independence is now deemed essential to China’s national security.
As a result, any more Russian efforts to establish even informal suzerainty over the Soviet successor states are, following the dismemberment of Georgia, likely to meet Chinese resistance.
The economic components of the Sino-Russian relationship – where real attachments are tested – are also dissatisfactory, at least from China’s point of view.
China’s major interest in Russia is oil and gas.
But, while Russia is firmly committed to being a major supplier of gas and oil to Europe, it is hesitant to play a similar role with China.
Moreover, Russia’s efforts to gain monopoly control of the gas pipeline networks across Eurasia pose a direct danger for China, because monopolists can not only gouge their consumers, but also shut off supplies for political purposes, as Russia has done repeatedly over the past two decades.
So China’s national security interest is to ensure that the gas-supplying nations of Central Asia have outlets to sell their gas that are not under Kremlin control.
Other than oil, gas, and other commodities, China has rather limited interests in Russia.
Russia has been China’s major supplier of weapons since the late 1990’s.
But, given the stagnant state of Russian science and technology, even the best Russian equipment nowadays seems out of date.
Indeed, although the war with Georgia demonstrated the revived combat spirit of the Russian army – at least when compared to its ineptness in the two Chechen wars of the 1990’s – it also exhibited the grave defects of Russian military technology.
Most of the arms used were yesterday’s weapons.
As China is now able to harness its own technological might to produce sophisticated weapons, Russia’s usefulness in this area is waning fast.
Nor do the Chinese have much interest in assuming de facto control of Asiatic Russia, despite shrieks from Russian strategic pundits that this is China’s real goal.
China might, indeed, have an interest in some border areas with fertile soil and moderate climate.
But it hardly wishes to colonize the frozen wastes of Siberia.
In fact, Siberia is not much different from China’s own almost empty mountain/desert borderlands, where even agriculture is a daunting task.
As for Russia’s Far East, the Chinese believe it will eventually fall to China anyway, so there is no need to hasten the process.
China is far more interested in focusing on the United States, its major trade partner and rival, and on South Asia and Iran, which supplies much of China’s oil and regards it as a more reliable ally than Russia.
Thus, the settling of the border dispute with Russia was not aimed so much at building a geopolitical marriage as securing each other’s rear, offering both sides a free hand to explore opportunities elsewhere.
What China wants and what it gets may be different things.
With its long borders with Russia, China knows it would have much to regret if a new, oil-fired Russian empire appeared on its doorstep.
The Dream of Space
More than 50 years ago, visionaries like the British science fiction writer Arthur C. Clarke and the German (and American) rocket engineer Wernher von Braun laid out a series of steps for the journey into space.
Clark presented his vision in a 1951 book The Exploration of Space and von Braun’s proposals appeared in a series of Collier’s magazine articles published between 1952 and 1954.
A few years later – indeed, 50 years ago this week – the Soviet Union launched the first earth orbiting satellite, inaugurating the space age.
After 50 years, many elements of the original vision have been achieved, some have failed, and there have been more than a few surprises.
Clarke and von Braun encountered a wall of skepticism when their proposals first appeared.
The public viewed space travel as science fiction, a form of popular entertainment thought to have little chance of realization.
But Clarke, von Braun, and others persevered.
We would, they predicted, launch satellites, dispatch humans on orbital flights, assemble a large earth-orbiting space station, build reusable space shuttles, construct space telescopes, and send humans to the moon, Venus, and Mars.
Clarke explained how humans could build communication satellites.
Von Braun predicted that military forces would use orbiting platforms to observe troop movements and prevent surprise attacks.
All of these visions appeared before the first artificial satellite was launched.
In 1959, officials at the United States National Aeronautics and Space Administration (NASA) adopted most of these steps as part of their long-range plan.
The Soviet Union engaged the US in a race to the moon.
Government support for realizing the vision inspired the British film maker Stanley Kubrick to present the steps in his classic movie 2001: A Space Odyssey , released in 1968.
Clarke helped to write the screenplay.
The movie depicted a large space station, a winged space shuttle, a lunar base, and astronauts on a trip to the outer solar system.
True to the plan, humans built scientific satellites, communication satellites, reconnaissance or “spy” satellites, space stations, and reusable space shuttles.
They sent robotic spacecraft to the moon, Venus, Mars, and other planets, and humans to the moon.
In those respects, the original vision was achieved.
The human space effort, nonetheless, departed from the plan in unexpected ways.
The International Space Station, currently under construction, does not resemble the large, rotating wheels presented by Kubrick, Clarke, and von Braun.
Early planners viewed the space station as an assembly point for expeditions to the moon and planets.
Kubrick’s 900-foot wide wheel rotated in such a fashion as to produce a sense of gravity equal to that felt on the moon, to which many of the station’s fictional occupants were bound.
The real space station is a micro-gravity research laboratory with no capability to launch trips to the moon or beyond.
Larger capabilities were part of the original space station plan, approved in 1984, but subsequently disappeared as the cost of the facility grew.
Nor has the US space shuttle accomplished its goals.
Von Braun predicted that the US would need to launch 364 space shuttles in less than one year to prepare for the first lunar voyage.
When the space shuttle was approved in 1972, NASA officials predicted that they would launch one every week or two.
But the space shuttle has proven far more difficult and costly to operate than anticipated, flying only four times per year on average.
It has neither reduced the cost of space access nor made space flight safe and routine, as promised.
When the shuttle is retired around 2010, NASA will replace it with an old-fashioned spacecraft with no wings at all.
In contrast to human space flight, which has lagged behind expectations, the robotic space effort has exceeded them.
Von Braun endorsed an automated telescope in space, but thought that astronauts would need to visit the instrument to change the film.
Early plans for military reconnaissance stations assumed that soldiers would be stationed on board.
Clarke believed that astronauts would be dispatched to maintain and operate communication satellites.
Kubrick suggested that people – accompanied by a computer – would pilot spacecraft to explore planets like Jupiter and Saturn.
But none of this proved necessary, owing to remarkable advances in robotics, solid-state electronics, imaging, data collection, and communication.
Sensors that convert light waves into electronic signals eliminated the need for film on space telescopes, while computer chip technology (integrated circuits) allowed communication satellites to be operated without periodic human repair.
Robotic spacecraft are currently returning information from the outer regions of the solar system and roving the surface of Mars.
No military force has yet perceived the need to station soldiers on its various reconnaissance, communication, or navigation satellites.
Robotic technology is advancing more rapidly than the technologies supporting human space flight.
Around the world, clever scientists, engineers, and entrepreneurs are working to realize the remainder of the space exploration dream. Watch for humans to establish space tourism, return to the moon, and attempt an expedition to Mars.
Advances in robotics will continue to occur, with automated spacecraft that can dig and swim, and telescopes that can look for earth-like planets around nearby stars.
If the past is any guide, much of what seems like science fiction today will become reality tomorrow.
The Drug War\u0027s Failures
If we're lucky, our grandchildren will recall the global war on drugs of the late 20th and early 21st century as some bizarre mania to which only past generations could succumb.
The world's drug problems are far more severe today than they were a century ago.
Not that drug use has increased fantastically - after all, huge quantities of alcohol, opium and other drugs were consumed back then.
The real problem is that today's drug control policies foster more harm than good; indeed, they probably cause more overall harm than drug abuse itself.
Like alcohol Prohibition in the US during the 1920s and early 1930s, global drug prohibition has failed to reduce drug abuse even as it generates extraordinary levels of crime, violence, corruption and disease.
In 1998, the UN estimated the total value of the illicit drug trade at $400 billion, or 7% of global trade.
Critics say the figure is only half that - still a remarkable sum.
Colombia today is far worse than Chicago under Al Capone. So, too, are other Latin American, Caribbean and Asian countries.
Drug prohibition effectively imposes a tax on the global trade in illicit drugs that is enforced by governments and collected by those willing to violate the laws.
No other set of laws produces so much revenue for criminals, terrorists and corrupt officials.
No other laws generate so much corruption or such violence; and no other set of laws contributes so much to the spread of HIV/AIDS, hepatitis and other diseases.
The war on drugs persists in part because of two myths.
The first assumes that human beings are better off "drug free," and that all societies should strive to be drug free.
But few - if any - drug free societies ever existed.
The second myth presumes that prohibition reduces the harm associated with drugs.
Global markets in cannabis, opium and coca products are basically similar to other global commodities markets, yet global drug control policies operate on the assumption that the drug markets bear more in common with smallpox and other infectious diseases for which there is no demand.
Governments can act unilaterally and multilaterally to regulate commodity markets, but it is a mistake to perceive prohibition as the ultimate or maximum form of regulation.
Prohibition in fact represents the abdication of regulation.
Whatever is not suppressed is effectively unregulated, except by criminal organizations.
Around the world, drug law violations account for the largest share of the roughly 8 million people incarcerated in local jails and prisons.
In 1980, 50,000 people were incarcerated in the US for drug law violations.
Today, the total approaches half-a-million, with a few hundred thousand more locked up on other prohibition-related offenses.
That total represents almost 10% of all the world's inmates.
Throughout the developing world, poor peasants involved in producing opium, coca and cannabis are arrested, sometimes beaten, and often extorted by government agents enforcing drug laws.
In Bolivia and Peru, coca was integrated into society.
The same was true of opium in Asia. Prohibitions imposed by the US and other governments decimated traditions that often "domesticated" these drugs so as to reduce their harm, while simultaneously encouraging transitions to refined drugs like heroin and cocaine.
US law enforcement and intelligence agencies routinely provide information to other governments knowing that it will be used not just to arrest but to torture those they identify.
All these consequences of the drug war can be defined as human rights abuses.
But the core human rights issue is different - the notion that people should not be punished for what they put into their bodies.
That right of sovereignty over one's mind and body - which also incorporates the right not to be forced to take drugs against one's will - represents in some respects the most fundamental of all rights.
Heroin users are denied the most effective medication available to remedy their addiction, i.e., methadone.
People unable or unwilling to stop injecting drugs are denied access to sterile syringes, with devastating consequences.
Millions who smoke marijuana or consume other psychoactive drugs are harmed far more by state action than by drug use.
More and more voices are calling for a systematic review and revision of the international anti-drug conventions that sustain today's failed policies.
Some emphasize the anti-scientific and otherwise illegitimate basis for including cannabis and coca in the conventions.
Others point to the contradictions between anti-drug conventions and international human rights conventions.
Others note that the anti-drug conventions exacerbate the problems they seek to ameliorate.
A new global drug control regime is needed.
It must reject the foolish rhetoric of creating "a drug-free world" and acknowledge that the true challenge is learning to live with drugs so that they cause the least harm.
An effective strategy needs to establish realistic objectives and criteria for evaluating success or failure, and these criteria must focus on reducing the death, disease, crime and suffering associated with both drug use and drug policies.
It also must embrace the principle that people should not be punished for what they put into their bodies, but only for the harms they do others.
Those are the key elements of a more ethical and effective drug control regime than the one that haunts today's world.
The Duisenberg Legacy
The European Central Bank's first president, Wim Duisenberg, is leaving office with his head held high.
Under his leadership, Europe's first central bank has "grown up," going from infancy to eminence in little more than a mere five years.
The euro has been firmly established as a world-class currency.
Inflation in the euro-zone economy hovers around 2%--more or less in line with the definition of price stability favored by the ECB.
Inflationary expectations are stable, and the Governing Council runs smoothly. 
Not everyone in Europe is happy with an anti-inflation central bank.
